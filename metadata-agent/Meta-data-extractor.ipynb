{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,re\n",
    "pdf_file = \"/Users/arjunbhndary/SEI_accelerate/rrbZq8-2308.10462v3.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@misc{-1,\n",
      "  author = {Weyssow, Martin and Diro and Zhou, Xin and Kim, Kisub and Lo, David},\n",
      "  title = {Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models},\n",
      "  date = {2024-12-27},\n",
      "  year = {2024},\n",
      "  month = {12},\n",
      "  day = {27},\n",
      "  doi = {10.1145/nnnnnnn.nnnnnnn},\n",
      "  eprint = {arXiv:2308.10462v3[cs.SE]},\n",
      "  abstract = {Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.},\n",
      "keywords = {CCS Concepts:, Software and its engineering → Software creation and management; Software development techniques code generation, large language models, parameter-efficient fine-tuning, quantization, empirical study}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def processHeaderDocument(pdf_path, grobid_url=\"https://kermitt2-grobid.hf.space\"):\n",
    "    files = {\"input\": open(pdf_path, \"rb\")}\n",
    "    params = {\"consolidate\": \"2\"}\n",
    "    r = requests.post(\n",
    "        f\"{grobid_url}/api/processHeaderDocument\",\n",
    "        files=files,\n",
    "        params=params,\n",
    "        timeout=60\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.text  # TEI XML\n",
    "xml_metadata = processHeaderDocument(pdf_file)\n",
    "print(xml_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" \n",
      "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n",
      "xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\"\n",
      " xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
      "\t<teiHeader xml:lang=\"en\">\n",
      "\t\t<fileDesc>\n",
      "\t\t\t<titleStmt>\n",
      "\t\t\t\t<title level=\"a\" type=\"main\">Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</title>\n",
      "\t\t\t</titleStmt>\n",
      "\t\t\t<publicationStmt>\n",
      "\t\t\t\t<publisher/>\n",
      "\t\t\t\t<availability  status=\"unknown\">\n",
      "\t\t\t\t\t<licence/>\n",
      "\t\t\t\t</availability>\n",
      "\t\t\t\t<date type=\"published\" when=\"2024-12-27\">27 Dec 2024</date>\n",
      "\t\t\t</publicationStmt>\n",
      "\t\t\t<sourceDesc>\n",
      "\t\t\t\t<biblStruct>\n",
      "\t\t\t\t\t<analytic>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<persName><forename type=\"first\">Martin</forename><surname>Weyssow</surname></persName>\n",
      "\t\t\t\t\t\t\t<email>martin.weyssow@umontreal.ca</email>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<persName><surname>Diro</surname></persName>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<persName><forename type=\"first\">Xin</forename><surname>Zhou</surname></persName>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<persName><forename type=\"first\">Kisub</forename><surname>Kim</surname></persName>\n",
      "\t\t\t\t\t\t\t<email>kisubkim@gmail.com</email>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<persName><forename type=\"first\">David</forename><surname>Lo</surname></persName>\n",
      "\t\t\t\t\t\t\t<email>davidlo@smu.edu.sg</email>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff0\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">University of Montreal</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<country key=\"CA\">Canada</country>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff1\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">Singapore Management University</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<country key=\"SG\">Singapore</country>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff2\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">Singapore Management University</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<country key=\"SG\">Singapore</country>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff3\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"department\" key=\"dep1\">Singapore HOUARI SAHRAOUI</orgName>\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"department\" key=\"dep2\">DIRO</orgName>\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\" key=\"instit1\">Singapore Management University</orgName>\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\" key=\"instit2\">University of Montreal</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<country key=\"CA\">Canada</country>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff4\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"department\">DIRO</orgName>\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">University of Montreal</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<settlement>Xin Zhou</settlement>\n",
      "\t\t\t\t\t\t\t\t\t<country key=\"CA\">Canada</country>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff5\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">Singapore Management University</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<settlement>Singapore</settlement>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff6\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">Singapore Management University</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<settlement>Singapore</settlement>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff7\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">Singapore Management University</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<settlement>Singapore</settlement>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<author>\n",
      "\t\t\t\t\t\t\t<affiliation key=\"aff8\">\n",
      "\t\t\t\t\t\t\t\t<orgName type=\"institution\">University of Montreal</orgName>\n",
      "\t\t\t\t\t\t\t\t<address>\n",
      "\t\t\t\t\t\t\t\t\t<country key=\"CA\">Canada</country>\n",
      "\t\t\t\t\t\t\t\t</address>\n",
      "\t\t\t\t\t\t\t</affiliation>\n",
      "\t\t\t\t\t\t</author>\n",
      "\t\t\t\t\t\t<title level=\"a\" type=\"main\">Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</title>\n",
      "\t\t\t\t\t</analytic>\n",
      "\t\t\t\t\t<monogr>\n",
      "\t\t\t\t\t\t<imprint>\n",
      "\t\t\t\t\t\t\t<date type=\"published\" when=\"2024-12-27\">27 Dec 2024</date>\n",
      "\t\t\t\t\t\t</imprint>\n",
      "\t\t\t\t\t</monogr>\n",
      "\t\t\t\t\t<idno type=\"MD5\">ED08C5420997939C148460D2D743B3D0</idno>\n",
      "\t\t\t\t\t<idno type=\"DOI\">10.1145/nnnnnnn.nnnnnnn</idno>\n",
      "\t\t\t\t\t<idno type=\"arXiv\">arXiv:2308.10462v3[cs.SE]</idno>\n",
      "\t\t\t\t</biblStruct>\n",
      "\t\t\t</sourceDesc>\n",
      "\t\t</fileDesc>\n",
      "\t\t<encodingDesc>\n",
      "\t\t\t<appInfo>\n",
      "\t\t\t\t<application version=\"0.8.1\" ident=\"GROBID\" when=\"2025-08-17T09:27+0000\">\n",
      "\t\t\t\t\t<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
      "\t\t\t\t\t<ref target=\"https://github.com/kermitt2/grobid\"/>\n",
      "\t\t\t\t</application>\n",
      "\t\t\t</appInfo>\n",
      "\t\t</encodingDesc>\n",
      "\t\t<profileDesc>\n",
      "\t\t\t<textClass>\n",
      "\t\t\t\t<keywords>\n",
      "\t\t\t\t\t<term>CCS Concepts:</term>\n",
      "\t\t\t\t\t<term>Software and its engineering → Software creation and management; Software development techniques code generation</term>\n",
      "\t\t\t\t\t<term>large language models</term>\n",
      "\t\t\t\t\t<term>parameter-efficient fine-tuning</term>\n",
      "\t\t\t\t\t<term>quantization</term>\n",
      "\t\t\t\t\t<term>empirical study 29.47</term>\n",
      "\t\t\t\t</keywords>\n",
      "\t\t\t</textClass>\n",
      "\t\t\t<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.</p></div>\n",
      "\t\t\t</abstract>\n",
      "\t\t</profileDesc>\n",
      "\t</teiHeader>\n",
      "\t<text xml:lang=\"en\">\n",
      "\t\t<body>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>Large Language Models (LLMs) based on the Transformer architecture <ref type=\"bibr\" target=\"#b67\">[67]</ref>, demonstrate significant potential in diverse domains, including natural language processing (NLP) <ref type=\"bibr\" target=\"#b29\">[29,</ref><ref type=\"bibr\" target=\"#b44\">44,</ref><ref type=\"bibr\" target=\"#b76\">76]</ref>, computer vision <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b59\">59,</ref><ref type=\"bibr\" target=\"#b85\">85]</ref>, and software engineering <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b66\">66,</ref><ref type=\"bibr\" target=\"#b82\">82]</ref>. These models excel in generating high-quality content given natural language intents in zero-shot, i.e., without fine-tuning. This capability has sparked considerable interest in the software engineering field for automating code-related tasks such as program repair <ref type=\"bibr\" target=\"#b27\">[27,</ref><ref type=\"bibr\" target=\"#b80\">80,</ref><ref type=\"bibr\" target=\"#b81\">81]</ref> and code generation <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b9\">9,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>.</p><p>While the zero-shot capabilities of LLMs are impressive, their full potential often emerges through fine-tuning <ref type=\"bibr\" target=\"#b54\">[54,</ref><ref type=\"bibr\" target=\"#b75\">75]</ref>. Specifically, fine-tuning an LLM to task-specific data allows it to learn and encode knowledge of the potentially highly contextual data at hand and thus generate more meaningful content. However, this process comes at a significant computational cost. Full fine-tuning, where all the parameters of the LLMs are updated during training, demands remarkable computational resources, especially when the LLM contains billions of parameters <ref type=\"bibr\" target=\"#b62\">[62]</ref>. To mitigate this computational burden, prior studies in software engineering <ref type=\"bibr\" target=\"#b53\">[53,</ref><ref type=\"bibr\" target=\"#b80\">80,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref> have investigated prompt-engineering techniques such as In-Context Learning (ICL) <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b54\">54]</ref> and Retrieval-Augmented Generation (RAG) <ref type=\"bibr\" target=\"#b31\">[31]</ref>. ICL consists of providing prompt examples of the task to the LLM, guiding it to generate contextually appropriate content without any fine-tuning involved. These examples can be manually created or randomly selected from a relevant training dataset. This technique has already shown promising results for code-related tasks, including automated program repair <ref type=\"bibr\" target=\"#b80\">[80]</ref>, bug fixing <ref type=\"bibr\" target=\"#b53\">[53]</ref>, and code generation <ref type=\"bibr\" target=\"#b61\">[61,</ref><ref type=\"bibr\" target=\"#b73\">73,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref>. Expanding upon ICL, RAG offers a more robust and powerful alternative that incorporate a knowledge retrieval system at inference. Using RAG, a retrieval model fetches relevant information from an indexed corpus, such as code documentation or similar code snippets to the input problem. The retrieved information is then added to the input prompt to guide generation. Unlike ICL, which relies on preselected examples that may not always be tailored to the specific input, RAG dynamically adapts to each individual input problem, providing more relevant context. This technique has demonstrated significant improvements in software engineering tasks such as code generation and summarization <ref type=\"bibr\" target=\"#b39\">[39,</ref><ref type=\"bibr\" target=\"#b52\">52,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref>, code completion <ref type=\"bibr\" target=\"#b41\">[41]</ref>, and program repair <ref type=\"bibr\" target=\"#b70\">[70]</ref>.</p><p>Although ICL and RAG provide a viable alternative to full fine-tuning, it operates at inference time and does not involve learning task-specific parameters, which may prevent the LLM from capturing fine-grained information about the task and result in a loss of effectiveness. In this context, Parameter-Efficient Fine-Tuning (PEFT) techniques have emerged as promising solutions to render the fine-tuning cost at the lowest while allowing the model to learn task-specific parameters. Prior works <ref type=\"bibr\" target=\"#b10\">[10,</ref><ref type=\"bibr\" target=\"#b57\">57,</ref><ref type=\"bibr\" target=\"#b68\">68,</ref><ref type=\"bibr\" target=\"#b69\">69]</ref> in code intelligence have demonstrated the capability of PEFT techniques, and often shown their superiority over full fine-tuning across a wide range of tasks. However, these studies focus on small language models (SLMs) (&lt;0.25B parameters) such as CodeBERT <ref type=\"bibr\" target=\"#b16\">[16]</ref> and CodeT5 <ref type=\"bibr\" target=\"#b72\">[72]</ref> and overlooked the applicability of PEFT techniques to LLMs (≥1B parameters), leaving an important research gap. Given the growing ubiquity of LLMs, we believe addressing this gap is paramount in advancing the field of code intelligence and harnessing the full potential of LLMs. Furthermore, we identify an additional research opportunity in exploring the usage of PEFT techniques under limited resource scenarios, aiming to demonstrate the democratization of LLMs tuning through PEFT. Addressing these gaps will not only show how PEFT techniques can enhance the effectiveness of LLMs but also how they broaden the accessibility and utility of LLMs in scarce computation settings and alleviate the dependence of practitioners on large computational infrastructures.</p><p>In this paper, we present an empirical study on the usage of existing PEFT techniques with LLMs. We focus our study on code generation, which has been a pivotal area of research due to its transformative impact on automating software development <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b50\">50]</ref>. Our objective is twofold. First, we aim to assess the code generation capabilities of LLMs using existing PEFT techniques such as LoRA <ref type=\"bibr\" target=\"#b24\">[24]</ref> and QLoRA <ref type=\"bibr\" target=\"#b13\">[13]</ref> on datasets without test cases, including Conala <ref type=\"bibr\" target=\"#b91\">[91]</ref> and CodeAlpacaPy <ref type=\"bibr\" target=\"#b8\">[8]</ref>, as well as the APPS dataset <ref type=\"bibr\" target=\"#b22\">[22]</ref> with test cases. Second, we seek to compare the effectiveness of LLMs tuned with these PEFT techniques against SLMs, ICL, and RAG. Additionally, we conduct our comparative study with limited availability of computational resources to investigate the broad practicality of using PEFT techniques for LLMs. To achieve these objectives, we formulate four research questions that guide our study:</p><p>-RQ1: How do LLMs and SLMs perform using ICL on the Conala and CodeAlpacaPy datasets? -RQ2: How do LLMs and SLMs perform using PEFT techniques on the Conala and CodeAlpacaPy datasets? -RQ3: How does LoRA compare with ICL and RAG on the Conala and CodeAlpacaPy datasets? -RQ4: Can we enhance the effectiveness of LLMs for code generation in the APPS dataset using LoRA and QLoRA?</p><p>Altogether, answering these four research questions fulfills both objectives of this empirical study. Our first three RQs focus on evaluating SLMs and LLMs for code generation on the Conala and CodeAlpaca datasets. In RQ1, we illustrate the baseline effectiveness of SLMs and LLMs using ICL, which retrieves random examples from the training set to guide the model in generating code. By addressing RQ2, we gain a comprehensive understanding of how effective SLMs and LLMs are when using different PEFT techniques. In RQ3, we conduct a comparative study of the effectiveness of LoRA with ICL and RAG, a strong baseline that dynamically retrieves relevant examples by selecting those closest to the test instructions from the training set. Finally, to showcase the potential broader impact of PEFT, we study in RQ4 whether tuning LLMs using LoRA and QLoRA can improve their effectiveness on APPS, a challenging benchmark with test cases.</p><p>To address these RQs, we conduct experiments on three datasets, APPS <ref type=\"bibr\" target=\"#b22\">[22]</ref>, Conala <ref type=\"bibr\" target=\"#b86\">[86]</ref>, and CodeAlpacaPy specifically curated from CodeAlpaca <ref type=\"bibr\" target=\"#b8\">[8]</ref> for Python code generation. Conversely to evaluation datasets such as HumanEval <ref type=\"bibr\" target=\"#b9\">[9]</ref>, the APPS, Conala and CodeAlpaca datasets, widely used in prior code generation studies <ref type=\"bibr\" target=\"#b49\">[49,</ref><ref type=\"bibr\" target=\"#b71\">71,</ref><ref type=\"bibr\" target=\"#b73\">73,</ref><ref type=\"bibr\" target=\"#b73\">73,</ref><ref type=\"bibr\" target=\"#b88\">88,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref>, include sufficient training examples that can be employed for fine-tuning. For a comprehensive comparative analysis, we select four distinct model families: CodeT5+ <ref type=\"bibr\" target=\"#b71\">[71]</ref>, CodeGen <ref type=\"bibr\" target=\"#b48\">[48]</ref>, CodeGen2 <ref type=\"bibr\" target=\"#b47\">[47]</ref>, and CodeLlama <ref type=\"bibr\" target=\"#b56\">[56]</ref>, including eight large and three small variants. Note that we omitted closed-sourced LLMs such as Codex due to the inaccessibility of their parameters, which makes the study of any fine-tuning technique infeasible. Furthermore, our study incorporates six PEFT techniques: LoRA <ref type=\"bibr\" target=\"#b24\">[24]</ref>, IA3 <ref type=\"bibr\" target=\"#b37\">[37]</ref>, Prompt tuning <ref type=\"bibr\" target=\"#b30\">[30]</ref>, and Prefix tuning <ref type=\"bibr\" target=\"#b33\">[33]</ref>. In addition, we explore QLoRA <ref type=\"bibr\" target=\"#b13\">[13]</ref> with 8-bit and 4-bit quantization, which combines LoRA and model quantization. Unlike ICL and RAG, these techniques entail learning new parameters to tune the LLMs for the specific downstream task. Our main findings are the following: -ICL drastically improves the effectiveness of all models compared to a zero-shot prompt for code generation on Conala and CodeAlpacaPy. -Increasing the number of ICL examples does not always lead to improvement in effectiveness. Models achieve peak effectiveness with eight and four examples for Conala and CodeAlpacaPy, respectively. -LLMs fine-tuned with LoRA, IA3, and Prompt tuning, i.e., a few millions of parameters, consistently outperform SLMs fully fine-tuned with hundreds of millions of parameters. -Among PEFT techniques, LoRA achieves the highest effectiveness for the LLMs and SLMs.</p><p>-QLoRA considerably reduces memory usage, achieving up to a 2-fold decrease compared to LoRA while improving or preserving the models' effectiveness. Furthermore, QLoRA enables the fine-tuning of LLMs up to 34B parameters for less than 24GB of GPU memory. -LoRA significantly enhances the performance of all models compared to ICL and RAG for code generation on Conala and CodeAlpacaPy. -LoRA and QLoRA improve CodeLlama-7B-Instruct's effectiveness for code generation on the APPS dataset. Our study sheds light on the promising opportunities that PEFT techniques hold, warranting further exploration for their application in other code-related tasks and scenarios.</p><p>To summarize, our contributions are the following: -We conduct a comprehensive empirical study of six PEFT techniques, i.e., LoRA, IA3, Prompt tuning, Prefix tuning, QLoRA-8bit, and QLoRA-4bit, for Python code generation over a broad range of SLMs and LLMs. -A comprehensive comparison and analysis of PEFT techniques against ICL and RAG for LLMs on code generation. -We demonstrate the practicality of leveraging PEFT techniques to effectively fine-tune LLMs of code and reduce the computational burden associated with full fine-tuning, showcasing their potential broader applications in software engineering.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">BACKGROUND 2.1 In-Context Learning (ICL) and Retrieval-Augmented Generation (RAG)</head><p>As one of the specific types of LLM-related techniques, ICL has emerged as an effective technique <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b11\">11,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" target=\"#b45\">45,</ref><ref type=\"bibr\" target=\"#b51\">51]</ref>. ICL seeks to improve the abilities of LLMs by integrating context-specific information, in the form of an input prompt or instruction template, during the inference and thus without the need to perform gradient-based training. Therefore, by considering the context, the model becomes more capable of generating coherent and contextually relevant outputs. This contextual coherence of the LLM and not having to perform costly gradient-based training constitutes prime advantages of using ICL to specialize LLMs to a specific task or dataset. However, ICL also presents some inconveniences, including the need to design representative prompts <ref type=\"bibr\" target=\"#b37\">[37,</ref><ref type=\"bibr\" target=\"#b74\">74,</ref><ref type=\"bibr\" target=\"#b90\">90]</ref>. RAG is a more sophisticated approach to inject examples into input prompts at inference. Unlike ICL that select random examples, RAG relies on a retrieval model that dynamically retrieves examples from a dataset that are close to a query. In practice, the query can be formulated using information from the test example at test time, such as the coding problem for the case of code generation. Altogether, RAG allows injection more relevant information in the input prompt than ICL and has been succesfully applied to software engineering tasks, such as code generation <ref type=\"bibr\" target=\"#b52\">[52,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref>, code summarization <ref type=\"bibr\" target=\"#b39\">[39,</ref><ref type=\"bibr\" target=\"#b52\">52]</ref>, and code completion <ref type=\"bibr\" target=\"#b41\">[41]</ref>. Nonetheless, both ICL and RAG suffer from a few limitations. One concerns the introduction of extra input tokens in the prompt, which may be infeasible when the contextual information is too large. Another limitation is the reliance on the quality and relevance of the retrieved examples. In RAG, the retrieval model must accurately find examples that are genuinely similar or useful for the test query. If the retrieval mechanism fails to identify appropriate examples, it can inject irrelevant or misleading information into the prompt, ultimately degrading performance.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Parameter-Efficient Fine-Tuning (PEFT)</head><p>PEFT refer to the utilization of techniques that optimize the fine-tuning process of LLMs by selectively updating a subset of parameters instead of updating the entire model's parameters <ref type=\"bibr\" target=\"#b14\">[14]</ref>. Technically, PEFT techniques focus on learning a small number of parameters for the task at hand by designing additional layers <ref type=\"bibr\" target=\"#b23\">[23]</ref>, adding prepending additional tokens <ref type=\"bibr\" target=\"#b30\">[30,</ref><ref type=\"bibr\" target=\"#b33\">33]</ref>, decomposing weight gradients into specific matrices <ref type=\"bibr\" target=\"#b24\">[24]</ref>. One of the representative cutting-edge PEFT techniques is LOw-Rank Adaptation of LLMs (LoRA) <ref type=\"bibr\" target=\"#b24\">[24]</ref>. The technique consists of freezing the model weights and injecting low-rank trainable matrices into the attention layers of the Transformer architecture <ref type=\"bibr\" target=\"#b67\">[67]</ref>, thereby drastically reducing the number of trainable parameters. We employ LoRA as one of our PEFT technique since it has been widely used in NLP <ref type=\"bibr\" target=\"#b14\">[14,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" target=\"#b65\">65]</ref> and showed promising performance. We also employ IA3 which intends to improve upon LoRA and further reduces the amount of trainable parameters <ref type=\"bibr\" target=\"#b37\">[37]</ref>. In addition to LoRA and IA3, we also include Prompt tuning <ref type=\"bibr\" target=\"#b30\">[30]</ref> and Prefix tuning <ref type=\"bibr\" target=\"#b30\">[30]</ref> in our study. Prompt tuning involves the process of prepending virtual tokens to the input tokens of the LLM, whereas Prefix tuning inserts virtual tokens in all the layers of the target model and thus requires learning more parameters. These virtual tokens are differentiable, allowing them to be learned through backpropagation during fine-tuning, while the rest of the LLM remains frozen. Furthermore, QLoRA <ref type=\"bibr\" target=\"#b13\">[13]</ref> combines LoRA with model quantization, enabling the fine-tuning of LLMs with less GPU memory by reducing the precision of floating point data types within the model.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">APPLYING LLMS WITH LIMITED RESOURCES</head><p>In the era of LLMs, the availability of substantial computational resources plays a crucial role in harnessing their high capabilities. Unfortunately, many researchers and practitioners often find themselves constrained by the limited availability of high-end computing infrastructures.</p><p>For instance, a software engineer with access to only a single consumer GPU (e.g., 24GB of VRAM) may find full fine-tuning impractical due to the significant memory demands. The rapid increase in model size and the number of trainable parameters exacerbates this issue. Despite its effectiveness, full fine-tuning comes at a steep computational cost <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b51\">51]</ref>., underscoring a computation-effectiveness trade-off (see Table <ref type=\"table\" target=\"#tab_0\">1</ref>).</p><p>To address these limitations, alternative approaches like ICL and RAG have gained attention. ICL and RAG offer a low-computation option by eliminating the need for parameter updates. However, these techniques comes with its own set of challenges, including the selection of representative examples and sensitivity to prompt design <ref type=\"bibr\" target=\"#b37\">[37,</ref><ref type=\"bibr\" target=\"#b74\">74,</ref><ref type=\"bibr\" target=\"#b90\">90]</ref>. In practice, this can result in lower effectiveness compared to fine-tuning, particularly for highly contextual tasks prevalent in software engineering.  To overcome these limitations, we foresee the emergence of PEFT techniques as promising solutions, offering more computationally efficient and scalable approaches to fine-tuning LLMs. PEFT methods, such as LoRA and QLoRA, limit the number of parameters being updated, thus reducing memory consumption while maintaining effectiveness competitive with full fine-tuning. This makes PEFT particularly well-suited for practitioners with limited access to computational resources. As illustrated in Table <ref type=\"table\" target=\"#tab_0\">1</ref>, PEFT strike an optimal balance between computational cost and effectiveness. Furthermore, Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref> shows that by employing PEFT techniques like LoRA, practitioners can fine-tune models such as CodeLlama-7B without exceeding 19GB of GPU memory. For even larger models, such as CodeLlama-34B, QLoRA with quantization enables fine-tuning within the constraints of a 24GB VRAM GPU.</p><p>In conclusion, PEFT empower software engineers to overcome resource limitations, allowing for effective LLM fine-tuning in highly contextual tasks without relying on expensive computational infrastructures. This makes PEFT not only a practical but also an essential tool for democratizing access to LLM capabilities.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">METHODOLOGY</head><p>In this section, we present the experimental setup of our study. We conduct all the experiments under a resource-constrained scenario. Specifically, all the procedures, i.e., fine-tuning and inference, of the models are performed with access to a single 24GB GPU. The main objective of our study is to demonstrate whether the fine-tuning of LLMs through PEFT is feasible and desirable over previous approaches and smaller models in this context.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Research Questions</head><p>In this study, we focus on the following research questions:</p><p>-RQ1: How do LLMs and SLMs perform using ICL on the Conala and CodeAlpacaPy datasets?</p><p>We study the baseline effectiveness of LLMs (≥ 1B parameters) and SLMs (&lt; 1B parameters) for code generation using the zero-shot prompt and ICL, where 𝑛 randomly selected examples are added to the input prompt. We test each model with up to 16 ICL examples, due to our limited computation resources.</p><p>We study the effectiveness of a large spectrum of SLMs and LLMs for code generation on two datasets covering codes of various lengths. We select a wide range of models of various sizes, pre-trained on diverse codebases and with different learning objectives to study how these factors impact their effectiveness. -RQ2: How do LLMs and SLMs perform using PEFT techniques on the Conala and CodeAl-pacaPy datasets? In this RQ, we investigate whether PEFT techniques consistently outperform ICL for SLMs and LLMs. We compare the best-performing configurations of ICL in RQ1 with PEFT techniques, including LoRA, IA3, Prompt tuning, Prefix tuning. Furthermore, we also investigate the effect of quantization with QLoRA-8bit and QLoRA-4bit on our best-performing model and larger variants.</p><p>For SLMs, we also include a comparison with full-parameter fine-tuning, as commonly used in previous SE studies <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b72\">72,</ref><ref type=\"bibr\" target=\"#b77\">77,</ref><ref type=\"bibr\" target=\"#b92\">92]</ref>. We do not include full-parameter fine-tuning for LLMs, as it is not feasible within our computational budget. -RQ3: How does LoRA compare with ICL and RAG on the Conala and CodeAlpacaPy datasets?</p><p>In this RQ, we compare the effectiveness of our best-performing LLM fine-tuned using LoRA with RAG. Our RAG setup consists of retrieving up to 16 examples from the training set that are closely related to the input prompt, which is similar to other approaches previously proposed for various SE tasks <ref type=\"bibr\" target=\"#b39\">[39,</ref><ref type=\"bibr\" target=\"#b41\">41,</ref><ref type=\"bibr\" target=\"#b70\">70]</ref>. -RQ4: Can we enhance the effectiveness of LLMs for code generation in the APPS dataset using LoRA and QLoRA? Lastly, we explore whether LLM fine-tuned using LoRA and QLoRA show improvement in functional correctness in the APPS dataset. We fine-tune our best-performing LLM using LoRA and QLoRA on the training set of APPS, and report the average of test cases passed as well as the Pass@𝑘 on APPS' test set for introductory, interview, and competition-level coding problems.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Datasets and Task</head><p>Throughout our study, we compare all the studied models on a Python code generation task. This task has gained significant attention in recent years <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b10\">10,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b50\">50,</ref><ref type=\"bibr\" target=\"#b61\">61]</ref> with the emergence of LLMs and their capability to generate Python code in zero-shot, i.e., without further fine-tuning. In particular, evaluation datasets such as HumanEval <ref type=\"bibr\" target=\"#b9\">[9]</ref> have extensively been used to benchmark code generation approaches <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b9\">9,</ref><ref type=\"bibr\" target=\"#b82\">82]</ref>. While HumanEval is widely utilized, it lacks a training corpus to evaluate finetuning or PEFT approaches. As our study's focus is on specializing LLMs using PEFT techniques, we have opted not to utilize HumanEval. Instead, we choose to use three other widely-used code generation datasets: the Conala <ref type=\"bibr\" target=\"#b87\">[87]</ref>, CodeAlpaca <ref type=\"bibr\" target=\"#b8\">[8]</ref>, and APPS <ref type=\"bibr\" target=\"#b22\">[22]</ref> datasets. All datasets provide an ample number of examples that can be employed for fine-tuning a model and have been used in prior code generation studies with LLMs <ref type=\"bibr\" target=\"#b71\">[71,</ref><ref type=\"bibr\" target=\"#b73\">73,</ref><ref type=\"bibr\" target=\"#b88\">88,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref>.</p><p>Conala dataset. We use a curated version of the Conala dataset <ref type=\"bibr\" target=\"#b91\">[91]</ref>. The dataset was crawled from StackOverflow and contains manually annotated pairs of code and natural language intent. Each natural language intent contains hints about the manipulated variables in the ground truth code, e.g., see the first example in Table <ref type=\"table\" target=\"#tab_2\">2</ref>, providing more context to the model for generating relevant code. In Figure <ref type=\"figure\" target=\"#fig_1\">2</ref>, we report the token length distributions of the three datasets. In Conala, most code solutions are short and one-liners, making it relatively easy for an LLM to generate exact match predictions. APPS dataset. The APPS dataset consists of 10,000 code generation problems, each paired with Python solutions. These problems are categorized into three difficulty levels: introductory, interview, and competition, with solutions varying from simple one-liners to complex algorithms. We can see in Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> and Table <ref type=\"table\" target=\"#tab_2\">2</ref> that APPS include more lengthy and complex examples than the two other datasets. On average, each problem is accompanied by 21.2 test cases, designed to evaluate the functional correctness of the generated code. The original dataset is split into 5,000 samples for training and 5,000 for testing. ### Instruction: You are given a string s = s1 s2 . . . sn of length n, which only contains digits 1, 2,..., 9. A substring s[l...r] of s is a string slsl+1sl+2 ...sr. A substring s[l...r] of s is called even if the number represented by it is even. Find the number of even substrings of s. Note, that even if some substrings are equal as strings, but have different l and r, they are counted as different substrings. The first line contains an integer n (1 ≤ n ≤ 65000) -the length of the string s. The second line contains a string s of length n. The string s consists only of digits 1, 2,..., 9. Print the number of even substrings of s. ### Response: Ground truth:</p><formula xml:id=\"formula_0\">n = int(input()) ans = 0 for i in range(n): for j in range(i, n): if int(s[i:j+1]) \\% 2 == 0: ans += 1 print(ans)</formula><p>In this study, we use 4,500 samples for training, 500 for validation, and 750 for testing, ensuring a balanced distribution of 250 test samples per difficulty level.</p><p>Task design. In Table <ref type=\"table\" target=\"#tab_2\">2</ref>, we illustrate an overview of the task design. The prompt is in the form of an instruction template, where \"### Instruction:\" and \"### Response:\" play the role of delimiting the instruction, i.e., natural language intent, and the answer, i.e., code generation. Note that this prompt design may not be optimal, but this kind of instruction template has shown to be effective in prior works <ref type=\"bibr\" target=\"#b36\">[36,</ref><ref type=\"bibr\" target=\"#b89\">89]</ref>. The code generated by the model is compared with the ground truth to assess the quality of the generation. During fine-tuning, we minimize a standard autoregressive cross-entropy loss function:</p><formula xml:id=\"formula_1\">L = - 𝑇 +1 ∑︁ 𝑖=1 𝑀 𝑖 • log 𝑃 (𝑥 𝑖 | 𝑥 &lt;𝑖 ) ,</formula><p>where:</p><formula xml:id=\"formula_2\">𝑀 𝑖 = 1 , if 𝑥 𝑖 ≠ -100 0 , otherwise.</formula><p>The model receives a concatenation of the prompt and the ground truth as input and predicts each token 𝑥 𝑖 in an autoregressive manner given the previous tokens 𝑥 &lt;𝑖 . Note that in the computation of the loss, we ignore the tokens from the instruction template to force the model to focus on generating code. We set the value of the instruction tokens to -100 and ignore them in the loss computation using the indicator function 𝑀 𝑖 . At inference, the model receives the prompt as input and attempts to generate the ground truth code by generating up to 10 code candidates.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">ICL and RAG</head><p>We conduct experiments using ICL and RAG on the Conala and CodeAlpacaPy datasets. For both techniques, we select the maximum number of samples that can fit into our GPU memory. For ICL, we use up to 16 examples for the Conala dataset and 8 examples for CodeAlpacaPy. These examples are randomly sampled from the corresponding training datasets and concatenated with the input prompt during inference. For RAG, we leverage GTE-small, a general-purpose, lightweight embedding model that outperforms many larger models, including OpenAI's proprietary embeddings <ref type=\"bibr\" target=\"#b34\">[34]</ref>. We generate embeddings for all instructions (excluding the code) in the training sets. At inference time, we retrieve up to 16 examples for Conala and 4 examples for CodeAlpacaPy, selecting those with instructions most similar to the test input. As with ICL, the retrieved examples are concatenated with the input problem to guide code generation.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Small and Large Language Models</head><p>In order to carry out a comprehensive analysis, we selected our SLMs and LLMs according to several criteria. First, we exclusively considered open-source models. We omitted closed-sourced LLMs such as Codex due to the inaccessibility of their parameters, which makes the study of any fine-tuning technique infeasible. All the studied models' checkpoints can be freely accessed, and have been pre-trained using open-source data. Secondly, we selected LLMs, which have been released within the past two years. Finally, to investigate the impact of scaling, we selected models with a diverse range of parameters. We consider models with less than 1B parameters as SLMs, and the others as LLMs. Note that we selected models that fit a single 24GB GPU for fine-tuning and inference without causing memory overflow. In total, we included 11 SLMs and LLMs from diverse families of models to conduct our experiments. -SLMs. We use CodeGen-350M-mono <ref type=\"bibr\" target=\"#b48\">[48]</ref>, CodeT5+-220M <ref type=\"bibr\" target=\"#b71\">[71]</ref>, and CodeT5+-770M <ref type=\"bibr\" target=\"#b71\">[71]</ref> as SLMs.</p><p>CodeGen-350M-mono is an autoregressive language model and a small version of CodeGen pretrained on various programming languages and further fine-tuned on Python data. CodeT5+-220M and CodeT5+-770M are encoder-decoder language models that improve upon CodeT5 by leveraging a two-staged pre-training phase on natural language and code data, and new learning objectives.</p><p>-CodeGen2 <ref type=\"bibr\" target=\"#b47\">[47]</ref> is a family of prefix-based language models which combines the learning schemes of a bi-directional encoder and a uni-directional decoder. CodeGen2 improves upon CodeGen <ref type=\"bibr\" target=\"#b48\">[48]</ref>, therefore we do not include the CodeGen family in our evaluation. CodeGen2 models were pretrained on a deduplicated version of TheStack <ref type=\"bibr\" target=\"#b28\">[28]</ref> spanning a wide range of languages. We employ CodeGen2-1B, CodeGen2-3.7B and CodeGen2-7B. -CodeLlama <ref type=\"bibr\" target=\"#b56\">[56]</ref> is a family of LLMs based on Llama 2 <ref type=\"bibr\" target=\"#b64\">[64]</ref>. Each model was initialized with Llama 2 and further pre-trained on code. CodeLlama comes in three different variants: CodeLlama specialized for code, CodeLlama-Instruct specialized for instruction-tuning and CodeLlama-Python specialized for Python. We employ CodeLlama-7B, CodeLlama-7B-Instruct and CodeLlama-7B-Python to initiate our experiments. In RQ4, we fine-tune CodeLlama-13B-Python and CodeLlama-34B-Python using QLoRA.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.5\">Metrics</head><p>We measure the effectiveness of the models through widely used metrics in prior code generation work.</p><p>For experiments on Conala and CodeAlpacaPy, we report the Exact Match (EM) and CodeBLEU <ref type=\"bibr\" target=\"#b55\">[55]</ref> metrics. Given a generated code and a ground truth, the EM returns 1 if both codes are identical, otherwise 0. To evaluate the effectiveness of the models on a list of 𝑘 ∈ [1, 10] candidates, we report the EM@𝑘, which computes the average correct predictions among a list of 𝑘 candidates. For our experiments on the APPS dataset, we report two metrics: the average number of test cases passed and Pass@𝑘. The average number of test cases passed evaluates how well the model performs by measuring the proportion of test cases that its generated code passes for each sample. In contrast, Pass@𝑘 is a more stringent metric that measures the percentage of problems for which at least one of the top 𝑘 generated code samples passes all test cases, reflecting the model's ability to produce fully correct solutions within 𝑘 attempts.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.6\">Implementation Details</head><p>For all our experiments, we used a single NVIDIA RTX A5000 24GB GPU. We study a total of seven tuning techniques: Full fine-tuning, ICL, LoRA <ref type=\"bibr\" target=\"#b24\">[24]</ref>, IA3 <ref type=\"bibr\" target=\"#b37\">[37]</ref>, Prompt tuning <ref type=\"bibr\" target=\"#b30\">[30]</ref>, Prefix tuning <ref type=\"bibr\" target=\"#b33\">[33]</ref>, and QLoRA <ref type=\"bibr\" target=\"#b13\">[13]</ref>. We implemented all the tuning techniques using HuggingFace <ref type=\"bibr\" target=\"#b79\">[79]</ref> and PEFT <ref type=\"bibr\" target=\"#b43\">[43]</ref> libraries.</p><p>We used full fine-tuning only for the SLMs, as tuning all the parameters of the LLMs is computationally intractable within a maximum GPU memory of 24GB. We set the learning rate to 5𝑒 -5. For LoRA and IA3, we applied the low-rank matrix decomposition on the attention layers of the models and set 𝑟 = 16 and 𝛼 = 32. For implementing QLoRA, we use 8-bit and 4-bit quantization <ref type=\"bibr\" target=\"#b12\">[12]</ref>. We set the learning rate to 3𝑒 -4 for LoRA, IA3 and QLoRA. For Prompt tuning and Prefix tuning, we prepended a set of 20 trainable continuous virtual tokens to each input sample of the models and applied learning rates of 3𝑒 -3 and 3𝑒 -2.</p><p>We used Adafactor <ref type=\"bibr\" target=\"#b60\">[60]</ref> optimizer with 16-bit float precision for all models. We fine-tuned the models for a maximum of five epochs and evaluated them every 0.2 * 𝑙𝑒𝑛(𝑡𝑟𝑎𝑖𝑛_𝑠𝑒𝑡) optimization steps. We fine-tune all models with a batch size of 8. We selected the checkpoint with the lowest evaluation loss for inference and found that beam search with a beam size of 10 yields the best effectiveness. Given the various token length distribution and complexity of the datasets, we generate codes with up to 64, 128, and 1024 tokens for Conala, CodeAlpacaPy, and APPS, respectively. We make our code publicly available: <ref type=\"url\" target=\"https://github.com/martin-wey/peft-llm-code\">https://github.com/martin-wey/peft-llm-code</ref>. We evaluate the models' effectiveness using EM@10 and compare them across these two datasets in Fig. <ref type=\"figure\" target=\"#fig_2\">3</ref>. Note that CodeGen2 architecture results in substantially more GPU memory usage than other models, which explains why we evaluate ICL with fewer examples than other models. First, we observe a substantial gap in EM@10 between the two datasets. This difference can be explained by the fact that the CodeAlpacaPy dataset contains much more challenging samples compared to the Conala dataset, as shown in Table <ref type=\"table\" target=\"#tab_2\">2</ref>.</p><p>Second, there is a notable gap in effectiveness between SLMs and LLMs, regardless of the number of examples provided. This observation highlights the advantages of large-scale pre-training and the use of larger models in this context.</p><p>For the Conala dataset, increasing the number of examples leads to higher EM@10 scores. However, when using more than eight examples, the effectiveness of the models begins to decline. For the CodeAlpacaPy dataset, a similar trend is observed, but the optimal number of examples is smaller. Most models achieve their best EM@10 scores when using three or four examples. This observation underscores the limitation of ICL, as adding more examples results in a degradation of the models' effectiveness.</p><p>Finally, CodeLlama models outperform all models across both datasets, achieving a peak EM@10 of 29.83 on Conala (CodeLlama-7B) and 11.94 on CodeAlpacaPy (CodeLlama-7B-Python). In contrast, smaller models, such as CodeGen2-3.7B achieves an EM@10 of 23.94 and 7.00 on Conala and CodeAl-pacaPy, respectively.</p><p>Answer to RQ1: ICL drastically improves the effectiveness of all models compared to zero-shot. Our best model, CodeLlama-7B, achieves EM@10 scores of 29.83 (7.73) and 11.62 (7.01) on Conala and CodeAlpacaPy with ICL (zero-shot), respectively.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">RQ2: Effectiveness of Models using PEFT Techniques</head><p>We report the detailed results of the effectiveness of the SLMs and LLMs on match-based code generation for both Conala and CodeAlpacaPy datasets in Table <ref type=\"table\" target=\"#tab_3\">3</ref>.</p><p>SLMs vs. LLMs. CodeGen-350M-mono with LoRA demonstrates the best effectiveness on average among small models, while CodeLlama-7B-Python with LoRA is the best LLM on average. Under the same 24GB GPU memory limitation, the best LLM surpasses the best small model by 39.8%, 41.7%, and 47.1% (72.3%, 48.8%, and 9.1%) in EM@1, EM@10, and CodeBLEU concerning the Conala (CodeAlpacaPy) dataset, respectively.</p><p>SLMs. Among the SLMs, CodeGen-350M-mono shows the highest effectiveness across all metrics on both datasets. Our results align with prior studies <ref type=\"bibr\" target=\"#b48\">[48,</ref><ref type=\"bibr\" target=\"#b73\">73,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref> that identified CodeGen-350M-mono as a robust SLM for Python code generation tasks. Interestingly, although it requires tuning approximately 1% of the total parameters of the model, LoRA appears as the best tuning technique, surpassing full fine-tuning by a considerable margin across nearly all configurations. For instance, the EM@10 score for CodeGen-350M-mono on the Conala dataset, with full fine-tuning, is 18.42, while it soars to 25.60 with LoRA.</p><p>LLMs. In Figure <ref type=\"figure\">4</ref>, we present a comparative analysis of the models' effectiveness when tuned using LoRA, focusing on CodeBLEU and EM@10 scores. Both plots clearly establish CodeLlama models as the best-performing LLMs in our study. Remarkably, CodeGen2-7B, despite sharing a similar number of parameters, lags behind all CodeLlama-7B variants. Unsurprisingly, harnessing larger models leads to better effectiveness. Given the low computational costs of PEFT techniques, leveraging smaller models in a context akin to ours seems counterproductive. Subsequently, in this paper, we demonstrate that even larger models can be fine-tuned through the combination of PEFT with quantization.</p><p>Best PEFT technique. Overall, LoRA emerges as the most effective PEFT technique among the studied ones. Although being presented as an incremental improvement over LoRA <ref type=\"bibr\" target=\"#b37\">[37]</ref>, IA3 often shows lower scores compared to LoRA. Prompt tuning appears as another viable tuning option, while further reducing the number of trainable parameters. However, Prefix tuning fails to effectively adapt the larger models to both datasets.</p><p>Our analysis reveals notably higher EM scores for the Conala dataset, which can be attributed to differences in task complexity between the two datasets (see Section 4.2). It is important to note that CodeBLEU scores on Conala are comparatively lower due to the metric's reliance on dataflow graph computations, which may not always be available for small code examples.  Effect of quantization with QLoRA. We explore the potential benefits of employing QLoRA <ref type=\"bibr\" target=\"#b13\">[13]</ref>, a computationally efficient technique that combines LoRA with 8-bit or 4-bit quantization for fine-tuning LLMs. In Figure <ref type=\"figure\">5</ref>, we display EM@10 scores for three CodeLlama model variants: CodeLlama-7B-Python, CodeLlama-13B-Python, and CodeLlama-34B-Python, alongside peak GPU memory consumption consistently below 24GB for each tuning configuration. The results underscore a significant improvement in the effectiveness of larger quantized models on Conala, with a more moderate impact on CodeAlpacaPy. For instance, CodeLlama-34B-Python, fine-tuned with QLoRA-4bit, achieves a substantial 12.2% increase in Conala's EM@10 score (40.70) compared to CodeLlama-7B-Python with LoRA <ref type=\"bibr\">(36.28)</ref>. Surprisingly, QLoRA also brings notable improvements over LoRA for CodeLlama-7B-Python on Conala, while achieving comparable results on CodeAlpacaPy. The application of quantization enables the utilization of larger models that can be accommodated within a single 24GB GPU. Specifically, for CodeLlama-7B-Python, QLoRA-4bit achieves a remarkable 2x reduction in peak memory usage while significantly improving the EM@10 score.</p><p>Answer to RQ2: LLMs with PEFT consistently and significantly outperform SLMs under the same GPU limit. Specifically, the best-performing LLM with PEFT surpasses the best small model by 39.8-72.3% in terms of EM@𝑘. Among different PEFT techniques, LoRA is the most effective. In addition, applying quantization with LoRA results in a drastic decrease in GPU usage while maintaining effectiveness on both datasets and accommodating the fine-tuning of larger models up to 34B parameters.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">RQ3: Comparative Analysis of LoRA, ICL, and RAG</head><p>In this RQ, we aim to investigate whether PEFT techniques consistently outperform the widely used ICL and RAG when applying LLMs in match-based code generation.</p><p>In Figure <ref type=\"figure\">6</ref>, we compare the effectiveness of the SLMs and LLMs using ICL and LoRA in terms of CodeBLEU and EM@10. In this figure, we report the highest metrics achieved over the different ICL</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>CL-7B</head><p>CL-13B CL-34B 30 configurations for each model. In Figure <ref type=\"figure\" target=\"#fig_6\">7</ref>, we explore the effectiveness of CodeLlama models using RAG, with up to 16 and 4 retrieved examples for Conala and CodeAlpacaPy, respectively. Similar to RQ1, we use fewer examples for CodeAlpacaPy to avoid out-of-memory errors. We compare the effectiveness of RAG with LoRA and the best EM@10 score achieved using ICL.</p><p>LoRA vs. ICL. As shown in Fig. <ref type=\"figure\">6</ref>, all models fine-tuned with LoRA demonstrate significantly higher EM@10 scores compared to ICL across both datasets. For example, CodeLlama-7B-Python with LoRA tuning achieves a 23.1% improvement in EM@10 on Conala (36.28 for LoRA vs. 29.47 for ICL). This pattern holds for CodeAlpacaPy, with even greater relative gains in EM@10. However, we observe some variation in CodeBLEU scores for most models on CodeAlpacaPy. For instance, CodeLlama-7B sees a CodeBLEU increase of 2.36 with LoRA. On CoNala, though, the impact of LoRA on CodeBLEU is less pronounced than that of ICL. These differences can be explained by the nature of the metrics: EM@10 is more conservative, requiring the generated solution to exactly match the ground truth, while CodeBLEU gives higher scores for solutions that are close but not exact. This distinction highlights how LoRA better adapts models to downstream datasets, particularly when precision is crucial.</p><p>RAG vs. ICL vs. LoRA. In comparing RAG, ICL, and LoRA on the CoNala dataset, RAG demonstrates higher effectiveness than ICL but falls short of LoRA's effectiveness across all three CodeLlama model variants. Notably, CodeLlama-7B achieves a maximum of 29.83 and 35.17 EM@10 with ICL and RAG, respectively, whereas the model tuned with LoRA reaches an EM@10 of 39.31.</p><p>For both Conala and CodeAlpacaPy datasets, the gains in EM@10 get thinner as we increase the number of examples using RAG. EM@10 saturates at around 8-16 examples for Conala and 3-4 examples for CodeAlpacaPy. Furthermore, we note that for the more challenging CodeAlpacaPy datasets, RAG yields lower EM@10 compared to randomly selected examples using ICL, highlighting RAG's limitations when problem complexity increases. LoRA, however, consistently outperforms both RAG and ICL on CodeAlpacaPy, highlighting its superior ability to adapt to more challenging datasets.</p><p>Answer to RQ3: LoRA is superior to ICL and RAG on Conala and CodeAlpacaPy datasets across the three CodeLlama-7B variants. Table <ref type=\"table\">4</ref>. [RQ4] -Effectiveness of CodeLlama-7B-Instruct on the APPs dataset in zero-shot and using LoRA, QLoRA-8bit, and QLoRA-4bit in terms of average passed tests (Avg) and Pass@k (P@k).</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Introductory</head><p>Interview Competition</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Model</head><p>Avg P@1 P@2 P@5 Avg P@1 P@2 P@5 Avg P@1 P@2 P@5 In this final RQ, we explore the broader applicability of LoRA and QLoRA, to enhance CodeLlama-7B-Instruct's effectiveness for execution-based code generation. The reason for choosing the instruct variant of CodeLlama-7B is because the model generally shows higher effectiveness than the other model variants on APPs in the seminal paper of CodeLlama <ref type=\"bibr\" target=\"#b56\">[56]</ref>. We do not compare LoRA and QLoRA with ICL and RAG for this dataset because they require increasing the prompt length beyond 2,048 tokens, which leads to out-of-memory errors. Our results, summarized in Table <ref type=\"table\">4</ref>, focus on the average number of test cases passed (Avg) and Pass@𝑘 for introductory, interview, and competition-level tasks.</p><p>For both introductory and interview-level code generation tasks, LoRA and QLoRA-8/4bit lead to significant improvements in the average number of passed test cases. Specifically, QLoRA-4bit results in a notable 52% increase in the average number of tests passed compared to the base model. In terms of Pass@𝑘 metrics, both LoRA and QLoRA-4bit demonstrate gains at the introductory level, with Pass@5 improving by +3.60% over the base model. However, these improvements are less substantial for interview and competition-level code generation, reflecting the greater complexity and challenge posed by these more advanced tasks.</p><p>Answer to RQ4: LoRA and QLoRA enhance CodeLlama-7B-Instruct's effectiveness on APPs, particularly at the introductory, with QLoRA-4bit boosting the average number of passed test cases by 52% and Pass@5 by 40%. However, improvements are less notable for interview and competition-level tasks.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">DISCUSSION</head><p>Our study explores PEFTs applied to code LLMs, elucidating the positive impact of these applications in efficiently tuning LLMs to task-specific datasets for code generation. In particular, our study illustrates the practicality of fine-tuning LLMs using PEFT, thereby alleviating the dependence of practitioners on large and expensive infrastructures. Our findings also pinpoint several promising areas for future exploration, including the investigation of efficient techniques across diverse fine-tuning settings, during inference, and for other SE tasks.</p><p>Efficient techniques for LLMs of code. Our work emphasizes efficient fine-tuning techniques, democratizing the tuning of LLMs to a broad audience. Nonetheless, our study did not include the exploration of efficient techniques for low-cost inference. While PEFT techniques require additional fine-tuning time compared to ICL and RAG, it is noteworthy that these techniques do not impose any supplementary time cost during inference. Nonetheless, we acknowledge the necessity of future investigations into techniques to reduce the time cost associated with LLMs during inference.</p><p>PEFT and ICL/RAG are non-exclusive techniques that can be used jointly. However, we decided not to include experiments on the application of ICL/RAG to LLMs fine-tuned using PEFT. In practice, increasing the number of ICL/RAG examples at inference entails increased computational overhead as the token length of the prompt expands. Consequently, we contend that employing ICL/RAG on a fine-tuned LLM might counterproductively escalate computational demands, outweighing potential benefits.</p><p>From a different angle, prior studies <ref type=\"bibr\" target=\"#b18\">[18,</ref><ref type=\"bibr\" target=\"#b78\">78,</ref><ref type=\"bibr\" target=\"#b83\">83]</ref> highlighted the need to consider pre-trained language models and LLMs of code in continual learning settings. In this paradigm, the model must dynamically adapt to new data over time while preserving performance on previously seen data. In the specific setting of continuously evolving LLMs, PEFT techniques potentially offer valuable benefits. Nonetheless, it is yet to be determined whether PEFT techniques can efficiently adapt LLMs under a continual learning setting for code-related tasks, without compromising the retention of past knowledge.</p><p>Effectiveness of QLoRA. Across all study datasets, we observed that QLoRA-4bit demonstrated competitive or comparable effectiveness to other PEFT methods. Notably, QLoRA-4bit outperformed LoRA and QLoRA-8bit on the Conala and APPs datasets. We hypothesize that this improvement stems from the regularization effect of reducing weight precision to 4 bits, which helps stabilize fine-tuning and mitigates overfitting. These findings highlight the potential for more efficient PEFT techniques, though further exploration is needed to fully understand their broader applicability.</p><p>New findings for PEFT in software engineering. Our findings in RQ1 reveal that PEFT methods outperform full fine-tuning for SLMs in code generation tasks. This stands in contrast to prior large-scale studies in NLP, such as Ding et al. <ref type=\"bibr\" target=\"#b14\">[14]</ref>, which demonstrated the superior effectiveness of full fine-tuning over techniques like LoRA, Prompt Tuning, and Prefix Tuning across a wide range of NLP tasks.</p><p>In the context of software engineering, while previous studies <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b40\">40]</ref> have shown that PEFT methods, like LoRA, can perform comparably to full fine-tuning for SLMs, our results go further. We show that all PEFT techniques studied in this paper significantly outperform full fine-tuning for SLMs like CodeGen-350M-mono and CodeT5+-770M on the Conala and CodeAlpacaPy datasets (see Table <ref type=\"table\" target=\"#tab_3\">3</ref>), highlighting the clear advantages of PEFT in these scenarios. However, due to resource constraints, we were unable to evaluate full fine-tuning for LLMs, leaving room for future studies to explore this further in the software engineering domain.</p><p>Additionally, our research uncovers new insights into the benefits of QLoRA and the comparative effectiveness of LoRA versus RAG for code generation tasks. First, in RQ3 and RQ4, we demonstrate that QLoRA offers comparable or even superior performance to LoRA while drastically cutting computational costs. Second, we reveal limitations of ICL and RAG, showing that LLM effectiveness tends to plateau as more examples are retrieved. In contrast, our study highlights the consistent advantages of PEFT techniques like LoRA and QLoRA in overcoming these limitations.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>SE tasks and multi-tasking.</head><p>To ensure a focused study, we avoided adding extra tasks and datasets, preventing an excessively broad set of analyses. Exploring PEFT techniques for LLMs across varied tasks and datasets is a promising direction for future research. In particular, Lorahub <ref type=\"bibr\" target=\"#b26\">[26]</ref>, a recently introduced framework for multi-task learning, demonstrates that a composition of LoRA modules trained on different tasks can generalize to new, unseen tasks while offering a strong performance-efficiency trade-off. We believe applying similar approaches in AI for SE holds great potential, particularly as the research field aims at automating a broad range of code-related tasks.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">THREATS TO VALIDITY</head><p>External validity. One main threat relates to the choice of our SLMs and LLMs. We mitigated this threat by carefully selecting a diverse set of models, as explained in Section 4.4. These models encompass various families of LLMs, trained on distinct pre-training data and learning objectives, and varying in size. Furthermore, we did not select larger model variants except when using QLoRA, as other PEFT techniques, ICL, and RAG limit the use of larger models within our resource constraints.</p><p>Another external threat to the validity is related to the quality and representativeness of the finetuning datasets. To alleviate this concern, we chose the Conala dataset, which contains high-quality examples mined from StackOverflow posts. Additionally, this dataset has been representatively used by multiple prior studies <ref type=\"bibr\" target=\"#b49\">[49,</ref><ref type=\"bibr\" target=\"#b73\">73,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref> on code generation tasks. Furthermore, the authors enriched each natural language intent with hints, enhancing the alignment of input prompts with possible human intents. To enrich our study, we included CodeAlpacaPy as a second dataset which encompasses lengthier examples, bringing another line of analysis. We did not include evaluation datasets such as HumanEval <ref type=\"bibr\" target=\"#b9\">[9]</ref> and MBPP <ref type=\"bibr\" target=\"#b3\">[3]</ref>, as they do not include training examples. However, to further expand our study, we explored the effectiveness of LoRA and QLoRA for execution-based code generation on the APPs dataset.</p><p>Finally, the monolingual aspect of our datasets constitutes another threat to external validity. We studied full fine-tuning, PEFT, ICL, and RAG for code generation of Python code snippets. However, we anticipate that PEFT is also applicable to other programming languages, considering the impressive generation capabilities of LLMs on a diverse range of programming languages <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b6\">6]</ref>.</p><p>Internal validity. The hyperparameter choices for the PEFT methods constitute the main threat to internal validity. For each PEFT technique, we used hyperparameters values which have been used in previous work on PEFT for code models as well as in the seminal papers that contributed the PEFT techniques. Additionally, since LoRA with 𝑟 = 16 and 𝛼 = 32 consistently outperforms all configurations of ICL and RAG across our top three models, conducting a detailed hyperparameter sensitivity analysis of LoRA could further solidify the advantage of PEFT over ICL and RAG. Future work could explore the sensitivity of key LoRA hyperparameters, such as rank 𝑟 and scaling factor 𝛼, across a broader range of software engineering tasks.</p><p>Construct validity. The choice of our evaluation metrics constitutes the main threat to construct validity. To mitigate this threat, we selected evaluation metrics widely used in prior works <ref type=\"bibr\" target=\"#b22\">[22,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b42\">42,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\" target=\"#b72\">72,</ref><ref type=\"bibr\" target=\"#b84\">84]</ref> on code generation. Furthermore, we evaluate each approach using EM@𝑘 on Conala and CodeAlpacaPy, which enriched our analysis by computing the exact match over different ranges of code candidates. Similarly, for APPs, we evaluate the base model and LoRA/QLoRA on Pass@𝑘 with up to 5 candidates. Finally, we did not use Pass@𝑘 metrics as the CoNaLa and CodeAlpacaPy datasets do not include unit tests. Enriching the datasets with unit tests constitutes an interesting area of future work.</p><p>In this section, we overview existing work on LLMs for code generation and contrast previous contributions on efficient model adaptation of code for downstream tasks with our study. Automated Code Generation. A significant portion of code generation techniques <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b4\">4,</ref><ref type=\"bibr\" target=\"#b21\">21,</ref><ref type=\"bibr\" target=\"#b63\">63,</ref><ref type=\"bibr\" target=\"#b72\">72]</ref> relies on deep-learning-based approaches. The latest trend in automated code generation revolves around leveraging LLMs like GPT models <ref type=\"bibr\" target=\"#b50\">[50]</ref> due to their remarkable breakthroughs in this domain. One notable example is Codex, developed by Chen et al. <ref type=\"bibr\" target=\"#b9\">[9]</ref>, which is a fine-tuned version of GPT-3. Other noteworthy models following the success of Codex include CodeGen <ref type=\"bibr\" target=\"#b48\">[48]</ref>, CodeGen2 <ref type=\"bibr\" target=\"#b47\">[47]</ref> and CodeLlama <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These LLMs effectively democratize the breakthrough performance achieved by Codex and bring it to a broader audience. However, the high computational costs associated with full finetuning for LLMs to achieve optimal performance are impractical for most researchers and practitioners. We believe that our study can shed light on more efficient and cost-effective approaches to fine-tuning these LLMs, mitigating the computational burdens associated with their adoption.</p><p>Efficient Adaptation of Models of Code. Efficient adaptation of models of code involves the utilization of techniques to efficiently adapt a model to a task-specific dataset (see Section 2). In this context, the term \"efficient\" refers to rendering the fine-tuning computation costs low, e.g, using LoRA, or utilizing parameter-free techniques such as prompting and ICL.</p><p>Most prior research has concentrated on employing ICL and prompting to adapt models to diverse code-related tasks. Gao et al. <ref type=\"bibr\" target=\"#b17\">[17]</ref> showcased the advantages of ICL in tasks like bug fixing, code summarization, and program synthesis. They highlighted that the model's performance on downstream tasks is influenced by multiple factors, including the selection, quantity, and order of prompt examples. Other studies <ref type=\"bibr\" target=\"#b53\">[53,</ref><ref type=\"bibr\" target=\"#b80\">80]</ref> also demonstrated that pre-trained language models and LLMs like Codex can effectively handle bug fixing and automated program repair using ICL. Moreover, Geng et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref> demonstrated the capability of Codex to generate multi-intent comment generation to describe the functionality of a method or its implementation details, for instance. The selection of relevant prompts for a task with ICL is crucial to ensure the good performance of an LLM. Prior works <ref type=\"bibr\" target=\"#b46\">[46,</ref><ref type=\"bibr\" target=\"#b91\">91]</ref> designed selection techniques to retrieve highly relevant prompt examples tailored to downstream tasks, outperforming random selection methods. Lastly, recent research <ref type=\"bibr\" target=\"#b61\">[61]</ref> highlighted the advantages of retrieving prompt examples at the repository level, providing LLMs with valuable contextual information in the prompts. In this study, we leveraged ICL without the intention of fully exploring its potential. Instead, we opted for a simple implementation of ICL by selecting random few-shot examples using different seeds. Expanding this study to incorporate more ICL approaches would enhance the comparison with PEFT techniques for code.</p><p>Regarding PEFT techniques, prior research in code intelligence has focused on Prompt tuning <ref type=\"bibr\" target=\"#b30\">[30]</ref>, Prefix-tuning <ref type=\"bibr\" target=\"#b33\">[33]</ref> and Adapters <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b23\">23,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" target=\"#b57\">57,</ref><ref type=\"bibr\" target=\"#b58\">58]</ref>. Wang et al. <ref type=\"bibr\" target=\"#b68\">[68]</ref> initiated the usage of Prompt tuning for code-related tasks and demonstrated its superiority over full fine-tuning of CodeT5 and CodeBERT in defect prediction, code summarization, and code translation. Goel et al. <ref type=\"bibr\" target=\"#b20\">[20]</ref> explored the use of programming-language-specific adapters for knowledge transfer in pre-trained language models, demonstrating that tuning BERT with these adapters surpass CodeBERT on cloze test and code clone detection. Choi et al. <ref type=\"bibr\" target=\"#b10\">[10]</ref> designed a code-specific Prefix tuning approach within a sequence-tosequence architecture for generation tasks. Our study differs from these three previous works as they focus on SLMs, whereas we propose the first comprehensive study of PEFT techniques with LLMs for code generation. Moreover, our study includes LoRA, IA3, and QLoRA, which none of the previous work in code intelligence considered for efficiently tuning LLMs of code. Wang et al. <ref type=\"bibr\" target=\"#b69\">[69]</ref> showcased the superiority of utilizing Adapters for fine-tuning pre-trained language models over full fine-tuning. Recent work have contributed empirical studies for various software engineering tasks, including code change <ref type=\"bibr\" target=\"#b40\">[40]</ref>, code summarization <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b57\">57]</ref>, defect prediction <ref type=\"bibr\" target=\"#b38\">[38]</ref>, and code clone detection <ref type=\"bibr\" target=\"#b57\">[57]</ref>, using Adapter tuning and LoRA for SLMs. Our research diverges from these prior work, as we concentrate on LLMs. Although we did not incorporate Adapters in our investigation, we believe that LoRA, IA3, Prompt tuning, Prefix tuning, and QLoRA provide a sufficiently thorough analysis of PEFT techniques. We recognize the value of exploring additional PEFT techniques for various code intelligence tasks in the future.</p></div>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"9\">CONCLUSION AND FUTURE WORK</head><p>This study establishes the effectiveness of PEFT techniques in fine-tuning LLMs for code generation. Our comparative analysis across various parameter-efficient techniques, including LoRA, IA3, Prompt tuning, Prefix tuning, and QLoRA, reveals the superiority of PEFT over full fine-tuning for SLMs and ICL and RAG for LLMs. Furthermore, our study illustrates the practicality of PEFT under a limited resources scenario, effectively mitigating the reliance on large and expensive computational infrastructures. To the best of our knowledge, this study is among the first comprehensive exploration of PEFT techniques for LLMs in software engineering, suggesting a promising avenue for future research. We anticipate our findings will inspire further investigation into the application of PEFT techniques in software engineering, with potentially far-reaching impacts. Our future work will extend the study to alternative software engineering tasks such as automated code review and comment generation. Finally, we aim to validate further relevance of PEFT techniques under multi-tasking and continual learning settings for automated software engineering.</p></div><figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_0\"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Peak GPU memory consumption during models fine-tuning using full fine-tuning (ft), LoRA, and QLoRA.</figDesc></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_1\"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Token length distribution of the Conala, CodeAlpacaPy, and APPS datasets.</figDesc></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_2\"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. [RQ1] -Effectiveness of the models using ICL with various number of random examples on the Conala and CodeAlpacaPy datasets.</figDesc></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_3\"><head></head><label></label><figDesc>: Baseline Effectiveness of Models Using Zero-Shot and ICL We start by investigating the baseline effectiveness of all SLMs and LLMs for match-based code generation. Specifically, we use zero-shot and ICL approaches with up to 16 retrieved random examples for the Conala dataset and eight for the CodeAlpacaPy dataset. The reason behind utilizing fewer examples for CodeAlpacaPy is because considering 16 examples results in out-of-memory errors under our setup.</figDesc></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_4\"><head></head><label></label><figDesc>Fig.4. [RQ2] -Effectiveness of the models fine-tuned using LoRA for both datasets in terms of EM@10 and CodeBLEU.</figDesc></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_5\"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. [RQ2] -Effectiveness and GPU usage of 7B, 13B, and 34B CodeLlama-Python (CL) LLMs fine-tuned using LoRA and QLoRA with 8-bit and 4-bit quantization.</figDesc></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_6\"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. [RQ3] -Comparison of the effectiveness of RAG with various number of retrieved examples against ICL and LoRA on the Conala (top) and CodeAlpacaPy (bottom) datasets. The ICL scores depict the highest scores achieved for each model in RQ1.</figDesc></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_0\"><head>Table 1 .</head><label>1</label><figDesc>Computation-effectiveness trade-off for each model tuning technique.</figDesc><table><row><cell>Technique</cell><cell cols=\"2\">Computation costs</cell><cell cols=\"2\">Effectiveness</cell></row><row><cell>Full fine-tuning</cell><cell></cell><cell>high [X]</cell><cell cols=\"2\">high [✓]</cell></row><row><cell>ICL and RAG</cell><cell></cell><cell>low [✓]</cell><cell cols=\"2\">low [X]</cell></row><row><cell>PEFT</cell><cell></cell><cell>low [✓]</cell><cell cols=\"2\">high [✓]</cell></row><row><cell>CodeT5+-220M-ft</cell><cell>3.54</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CodeGen-350M-mono-ft</cell><cell></cell><cell>5.84</cell><cell></cell><cell></cell></row><row><cell>CodeT5+-770M-ft</cell><cell></cell><cell>8.16</cell><cell></cell><cell></cell></row><row><cell>CodeLlama-7B-QLoRA-4bit</cell><cell></cell><cell>9.16</cell><cell></cell><cell></cell></row><row><cell>CodeGen2-1B-lora</cell><cell></cell><cell>9.8</cell><cell></cell><cell></cell></row><row><cell>CodeGen2-3.7B-lora</cell><cell></cell><cell></cell><cell>14.08</cell><cell></cell></row><row><cell>CodeLlama-13B-QLoRA-4bit</cell><cell></cell><cell></cell><cell>15.01</cell><cell></cell></row><row><cell>CodeLlama-7B-lora</cell><cell></cell><cell></cell><cell></cell><cell>19.06</cell></row><row><cell>CodeGen2-7B-lora</cell><cell></cell><cell></cell><cell></cell><cell>20.29</cell></row><row><cell>CodeLlama-34B-QLoRA-4bit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23.59</cell></row><row><cell>0</cell><cell>5</cell><cell cols=\"2\">10 Peak memory consumption (GB) 15</cell><cell>20</cell><cell>24</cell></row></table></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_1\"><head></head><label></label><figDesc>We filter out code samples that cannot be statically parsed to ensure the dataset encompasses only syntactically valid Python codes. As illustrated in the bottom example of Table2and in Figure2, CodeAlpacaPy contains lengthier and more complex examples than Conala, allowing for a more comprehensive evaluation of PEFT for code generation. The dataset contains 2,192/314/628 samples as the training/validation/test sets, respectively.</figDesc><table /><note><p><p><p><p>In this curated version of the dataset, the authors ensured that each sample in the validation and test sets contained at least one Python function that does not appear in the training set. Additionally, they ensured that examples crawled from the same StackOverflow post appear in different sets. Thus, we can guarantee that each natural intent in the test does not appear in the training set. The dataset contains 2,135/201/543 samples as the training/validation/test sets, respectively.</p>CodeAlpacaPy dataset. We construct a curated Python version of the CodeAlpaca</p><ref type=\"bibr\" target=\"#b8\">[8]</ref> </p>dataset by specifically selecting the Python data samples within the CodeAlpaca dataset.</p></note></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_2\"><head>Table 2 .</head><label>2</label><figDesc>Overview of the code generation task, with three examples taken from the Conala, CodeAlpacaPy, and APPS datasets.</figDesc><table><row><cell>Conala</cell></row></table></figure>\n",
      "<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_3\"><head>Table 3 .</head><label>3</label><figDesc>[RQ2] -Comparison of the SLMs and LLMs using various tuning techniques ( blue : best-performing tuning method per model, orange : best overall performing model).Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models • 15</figDesc><table><row><cell>Conala</cell><cell>CodeAlpacaPy</cell></row></table><note><p>, Vol. 1, No. 1, Article . Publication date: December 2024.</p></note></figure>\n",
      "\t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" xml:id=\"foot_0\"><p>, Vol. 1, No. 1, Article . Publication date: December 2024.</p></note>\n",
      "\t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" xml:id=\"foot_1\"><p>, Vol. 1, No. 1, Article . Publication date: December 2024. Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models • 17</p></note>\n",
      "\t\t</body>\n",
      "\t\t<back>\n",
      "\t\t\t<div type=\"references\">\n",
      "\n",
      "\t\t\t\t<listBibl>\n",
      "\n",
      "<biblStruct xml:id=\"b0\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">CodeT5+-220M CodeT5+-770M CodeGen-350M-mono CodeGen2-1B CodeGen2-.7B CodeGen2-7B CodeLlama-7B CodeLlama-7B-Instruct CodeLlama-7B-Python REFERENCES</title>\n",
      "\t\t<imprint/>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b1\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Structural language models of code</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Uri</forename><surname>Alon</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Roy</forename><surname>Sadaka</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Eran</forename><surname>Yahav</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">International conference on machine learning</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>PMLR</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"245\" to=\"256\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b2\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Multi-lingual evaluation of code generation models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ben</forename><surname>Athiwaratkun</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Krishna</forename><surname>Sanjay</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zijian</forename><surname>Gouda</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiaopeng</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yuchen</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ming</forename><surname>Tian</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Tan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Uddin</forename><surname>Wasi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shiqi</forename><surname>Ahmad</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qing</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mingyue</forename><surname>Sun</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Shang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2210.14868</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b3\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Program synthesis with large language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jacob</forename><surname>Austin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Augustus</forename><surname>Odena</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Maxwell</forename><surname>Nye</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Maarten</forename><surname>Bosma</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Henryk</forename><surname>Michalewski</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">David</forename><surname>Dohan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ellen</forename><surname>Jiang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Carrie</forename><surname>Cai</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Terry</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Quoc</forename><surname>Le</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2108.07732</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b4\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Deepcoder: Learning to write programs</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Matej</forename><surname>Balog</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Alexander</forename><forename type=\"middle\">L</forename><surname>Gaunt</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Marc</forename><surname>Brockschmidt</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sebastian</forename><surname>Nowozin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Tarlow</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:1611.01989</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2016\">2016. 2016</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b5\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Language models are few-shot learners</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tom</forename><surname>Brown</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Mann</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Nick</forename><surname>Ryder</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Melanie</forename><surname>Subbiah</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jared</forename><forename type=\"middle\">D</forename><surname>Kaplan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Prafulla</forename><surname>Dhariwal</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Arvind</forename><surname>Neelakantan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pranav</forename><surname>Shyam</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Girish</forename><surname>Sastry</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Amanda</forename><surname>Askell</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Advances in neural information processing systems</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">33</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"1877\" to=\"1901\" />\n",
      "\t\t\t<date type=\"published\" when=\"2020\">2020. 2020</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b6\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Federico</forename><surname>Cassano</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">John</forename><surname>Gouwar</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Nguyen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sydney</forename><surname>Nguyen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Luna</forename><surname>Phipps-Costin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Donald</forename><surname>Pinckney</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ming-Ho</forename><surname>Yee</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yangtian</forename><surname>Zi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Carolyn</forename><forename type=\"middle\">Jane</forename><surname>Anderson</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Molly</forename><forename type=\"middle\">Q</forename><surname>Feldman</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">IEEE Transactions on Software Engineering</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b7\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Prompt-RSVQA: Prompting visual context to a language model for remote sensing visual question answering</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Christel</forename><surname>Chappuis</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Valérie</forename><surname>Zermatten</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sylvain</forename><surname>Lobry</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bertrand</forename><forename type=\"middle\">Le</forename><surname>Saux</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Devis</forename><surname>Tuia</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Models • 23 Conference on Computer Vision and Pattern Recognition</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022-12\">2022. December 2024</date>\n",
      "\t\t\t<biblScope unit=\"volume\">1</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"1372\" to=\"1381\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note>Proceedings of the IEEE/CVF</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b8\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Code Alpaca: An Instruction-following LLaMA model for code generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sahil</forename><surname>Chaudhary</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<ptr target=\"https://github.com/sahil280114/codealpaca\" />\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b9\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Evaluating large language models trained on code</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mark</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jerry</forename><surname>Tworek</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Heewoo</forename><surname>Jun</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qiming</forename><surname>Yuan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Harri</forename><surname>Edwards</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yuri</forename><surname>Burda</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Nicholas</forename><surname>Joseph</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Greg</forename><surname>Brockman</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2107.03374</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b10\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yunseok</forename><surname>Choi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jee-Hyong</forename><surname>Lee</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Findings of the Association for Computational Linguistics: ACL 2023</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"5282\" to=\"5297\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b11\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Palm: Scaling language modeling with pathways</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Aakanksha</forename><surname>Chowdhery</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sharan</forename><surname>Narang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Maarten</forename><surname>Bosma</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Gaurav</forename><surname>Mishra</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Adam</forename><surname>Roberts</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Paul</forename><surname>Barham</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hyung</forename><forename type=\"middle\">Won</forename><surname>Chung</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Charles</forename><surname>Sutton</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sebastian</forename><surname>Gehrmann</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2204.02311</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b12\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tim</forename><surname>Dettmers</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Younes</forename><surname>Belkada</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2208.07339</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b13\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Qlora: Efficient finetuning of quantized llms</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tim</forename><surname>Dettmers</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Artidoro</forename><surname>Pagnoni</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ari</forename><surname>Holtzman</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2305.14314</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b14\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ning</forename><surname>Ding</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yujia</forename><surname>Qin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Guang</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fuchao</forename><surname>Wei</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zonghan</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yusheng</forename><surname>Su</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shengding</forename><surname>Hu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yulin</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chi-Min</forename><surname>Chan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Weize</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2203.06904</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b15\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Parameter-efficient fine-tuning of large-scale pre-trained language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ning</forename><surname>Ding</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yujia</forename><surname>Qin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Guang</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fuchao</forename><surname>Wei</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zonghan</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yusheng</forename><surname>Su</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shengding</forename><surname>Hu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yulin</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chi-Min</forename><surname>Chan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Weize</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Nature Machine Intelligence</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">5</biblScope>\n",
      "\t\t\t<biblScope unit=\"issue\">3</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"220\" to=\"235\" />\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b16\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Codebert: A pre-trained model for programming and natural languages</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhangyin</forename><surname>Feng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daya</forename><surname>Guo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Duyu</forename><surname>Tang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Nan</forename><surname>Duan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiaocheng</forename><surname>Feng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ming</forename><surname>Gong</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Linjun</forename><surname>Shou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bing</forename><surname>Qin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ting</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daxin</forename><surname>Jiang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2002.08155</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2020\">2020. 2020</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b17\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuzheng</forename><surname>Gao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xin-Cheng</forename><surname>Wen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Cuiyun</forename><surname>Gao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Wenxuan</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Michael R Lyu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2304.07575</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b18\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuzheng</forename><surname>Gao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hongyu</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Cuiyun</forename><surname>Gao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chaozheng</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2302.03482</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b19\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mingyang</forename><surname>Geng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shangwen</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dezun</forename><surname>Dong</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Haotian</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ge</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhi</forename><surname>Jin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiaoguang</forename><surname>Mao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiangke</forename><surname>Liao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2024\">2024. 2024</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b20\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">On the cross-modal transfer from natural language to code through adapter modules</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Divyam</forename><surname>Goel</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ramansh</forename><surname>Grover</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fatemeh</forename><forename type=\"middle\">H</forename><surname>Fard</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension</title>\n",
      "\t\t<meeting>the 30th IEEE/ACM International Conference on Program Comprehension</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"71\" to=\"81\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b21\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Retrieval-based neural code generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Anugrah</forename><surname>Shirley</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Raphael</forename><surname>Hayati</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pravalika</forename><surname>Olivier</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pengcheng</forename><surname>Avvaru</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Anthony</forename><surname>Yin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Graham</forename><surname>Tomasic</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Neubig</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:1808.10025</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2018\">2018. 2018</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b22\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Measuring Coding Challenge Competence With APPS</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dan</forename><surname>Hendrycks</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Steven</forename><surname>Basart</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Saurav</forename><surname>Kadavath</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mantas</forename><surname>Mazeika</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Akul</forename><surname>Arora</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ethan</forename><surname>Guo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Collin</forename><surname>Burns</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Samir</forename><surname>Puranik</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Horace</forename><surname>He</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dawn</forename><surname>Song</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jacob</forename><surname>Steinhardt</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t\t<publisher>NeurIPS</publisher>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b23\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Parameter-efficient transfer learning for NLP</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Neil</forename><surname>Houlsby</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Andrei</forename><surname>Giurgiu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Stanislaw</forename><surname>Jastrzebski</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bruna</forename><surname>Morrone</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Quentin</forename><surname>De Laroussilhe</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Andrea</forename><surname>Gesmundo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mona</forename><surname>Attariyan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sylvain</forename><surname>Gelly</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">International Conference on Machine Learning</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>PMLR</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2019\">2019. 2790-2799</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b24\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Lora: Low-rank adaptation of large language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">J</forename><surname>Edward</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yelong</forename><surname>Hu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Phillip</forename><surname>Shen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zeyuan</forename><surname>Wallis</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shean</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lu</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Weizhu</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2106.09685</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b25\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhiqiang</forename><surname>Hu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yihuai</forename><surname>Lan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lei</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Wanyu</forename><surname>Xu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ee-Peng</forename><surname>Lim</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Roy</forename></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ka-Wei</forename><surname>Lee</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lidong</forename><surname>Bing</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Soujanya</forename><surname>Poria</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2304.01933</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b26\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Lorahub: Efficient cross-task generalization via dynamic lora composition</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chengsong</forename><surname>Huang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qian</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bill</forename><surname>Yuchen Lin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tianyu</forename><surname>Pang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chao</forename><surname>Du</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Min</forename><surname>Lin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2307.13269</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b27\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Repair is nearly generation: Multilingual program repair with llms</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Harshit</forename><surname>Joshi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">José Cambronero</forename><surname>Sanchez</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sumit</forename><surname>Gulwani</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Gust</forename><surname>Vu Le</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ivan</forename><surname>Verbruggen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Radiček</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the AAAI Conference on Artificial Intelligence</title>\n",
      "\t\t<meeting>the AAAI Conference on Artificial Intelligence</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t\t<biblScope unit=\"volume\">37</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"5131\" to=\"5140\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b28\">\n",
      "\t<monogr>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Denis</forename><surname>Kocetkov</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Raymond</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Loubna</forename><surname>Ben Allal</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jia</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chenghao</forename><surname>Mou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yacine</forename><surname>Jernite</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Margaret</forename><surname>Mitchell</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sean</forename><surname>Hughes</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dzmitry</forename><surname>Bahdanau</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Leandro</forename><surname>Von Werra</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Harm</forename><surname>De Vries</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<title level=\"m\">The Stack: 3 TB of permissively licensed source code</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">Preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b29\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Large language models are zero-shot reasoners</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Takeshi</forename><surname>Kojima</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shane</forename><surname>Shixiang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Machel</forename><surname>Gu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yutaka</forename><surname>Reid</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yusuke</forename><surname>Matsuo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Iwasawa</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Advances in neural information processing systems</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">35</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"22199\" to=\"22213\" />\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b30\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">The Power of Scale for Parameter-Efficient Prompt Tuning</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Brian</forename><surname>Lester</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Rami</forename><surname>Al-Rfou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Noah</forename><surname>Constant</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>\n",
      "\t\t<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"3045\" to=\"3059\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b31\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Patrick</forename><surname>Lewis</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ethan</forename><surname>Perez</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Aleksandra</forename><surname>Piktus</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fabio</forename><surname>Petroni</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Vladimir</forename><surname>Karpukhin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Naman</forename><surname>Goyal</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Heinrich</forename><surname>Küttler</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Wen-Tau</forename><surname>Yih</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tim</forename><surname>Rocktäschel</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Advances in Neural Information Processing Systems</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">33</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"9459\" to=\"9474\" />\n",
      "\t\t\t<date type=\"published\" when=\"2020\">2020. 2020</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b32\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Skcoder: A sketch-based approach for automatic code generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jia</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yongmin</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ge</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhi</forename><surname>Jin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yiyang</forename><surname>Hao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xing</forename><surname>Hu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2302.06144</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b33\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Prefix-tuning: Optimizing continuous prompts for generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lisa</forename><surname>Xiang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Percy</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Liang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2101.00190</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b34\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Towards General Text Embeddings with Multi-stage Contrastive Learning</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zehan</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xin</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yanzhao</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dingkun</forename><surname>Long</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pengjun</forename><surname>Xie</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Meishan</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2308.03281</idno>\n",
      "\t\t<ptr target=\"https://arxiv.org/abs/2308.03281\" />\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b35\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Holistic evaluation of language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Rishi</forename><surname>Bommasani</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tony</forename><surname>Lee</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dimitris</forename><surname>Tsipras</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dilara</forename><surname>Soylu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Michihiro</forename><surname>Yasunaga</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yian</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Deepak</forename><surname>Narayanan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yuhuai</forename><surname>Wu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ananya</forename><surname>Kumar</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2211.09110</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b36\">\n",
      "\t<monogr>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">M</forename><surname>Liang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Y</forename><surname>Yuksekgonul</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">E</forename><surname>Mao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Wu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Zou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2304.02819</idno>\n",
      "\t\t<title level=\"m\">GPT detectors are biased against non-native English writers</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b37\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Haokun</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Derek</forename><surname>Tam</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mohammed</forename><surname>Muqeeth</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jay</forename><surname>Mohta</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tenghao</forename><surname>Huang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mohit</forename><surname>Bansal</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Colin</forename><forename type=\"middle\">A</forename><surname>Raffel</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Advances in Neural Information Processing Systems</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">35</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"1950\" to=\"1965\" />\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b38\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jiaxing</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chaofeng</forename><surname>Sha</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xin</forename><surname>Peng</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">38th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>IEEE</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"397\" to=\"408\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b39\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Retrieval-augmented generation for code summarization via hybrid gnn</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shangqing</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yu</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiaofei</forename><surname>Xie</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jingkai</forename><surname>Siow</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yang</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2006.05405</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2020\">2020. 2020</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b40\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Delving into Parameter-Efficient Fine-Tuning in Code Change Learning: An Empirical Study</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuo</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jacky</forename><surname>Keung</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhen</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fang</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qilin</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yihan</forename><surname>Liao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2402.06247</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2024\">2024. 2024</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b41\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Reacc: A retrievalaugmented code completion framework</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuai</forename><surname>Lu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Nan</forename><surname>Duan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hojae</forename><surname>Han</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daya</forename><surname>Guo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Seung-Won</forename><surname>Hwang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Alexey</forename><surname>Svyatkovskiy</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2203.07722</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b42\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Codexglue: A machine learning benchmark dataset for code understanding and generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuai</forename><surname>Lu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daya</forename><surname>Guo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuo</forename><surname>Ren</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Junjie</forename><surname>Huang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Alexey</forename><surname>Svyatkovskiy</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ambrosio</forename><surname>Blanco</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Colin</forename><surname>Clement</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dawn</forename><surname>Drain</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daxin</forename><surname>Jiang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Duyu</forename><surname>Tang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2102.04664</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b43\">\n",
      "\t<monogr>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sourab</forename><surname>Mangrulkar</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sylvain</forename><surname>Gugger</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Younes</forename><surname>Belkada</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sayak</forename><surname>Paul</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Bossan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<ptr target=\"https://github.com/huggingface/peft\" />\n",
      "\t\t<title level=\"m\">PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b44\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Recent advances in natural language processing via large pre-trained language models: A survey</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bonan</forename><surname>Min</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hayley</forename><surname>Ross</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Elior</forename><surname>Sulem</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Amir</forename><surname>Pouran</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ben</forename><surname>Veyseh</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Thien</forename><surname>Huu Nguyen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Oscar</forename><surname>Sainz</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Eneko</forename><surname>Agirre</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ilana</forename><surname>Heintz</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dan</forename><surname>Roth</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Comput. Surveys</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b45\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Metaicl: Learning to learn in context</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sewon</forename><surname>Min</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hannaneh</forename><surname>Hajishirzi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2110.15943</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b46\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Retrieval-based prompt selection for code-related few-shot learning</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Noor</forename><surname>Nashid</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mifta</forename><surname>Sintaha</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ali</forename><surname>Mesbah</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 45th International Conference on Software Engineering (ICSE&apos;23)</title>\n",
      "\t\t<meeting>the 45th International Conference on Software Engineering (ICSE&apos;23)</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b47\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Codegen2: Lessons for training llms on programming and natural languages</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Erik</forename><surname>Nijkamp</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hiroaki</forename><surname>Hayashi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Silvio</forename><surname>Savarese</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yingbo</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2305.02309</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b48\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Erik</forename><surname>Nijkamp</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bo</forename><surname>Pang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hiroaki</forename><surname>Hayashi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lifu</forename><surname>Tu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Huan</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yingbo</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Silvio</forename><surname>Savarese</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2203.13474[cs.LG</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023-12\">2023. December 2024</date>\n",
      "\t\t\t<biblScope unit=\"volume\">1</biblScope>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note>Publication date</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b49\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Code generation from natural language with less prior knowledge and more monolingual data</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sajad</forename><surname>Norouzi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Keyi</forename><surname>Tang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yanshuai</forename><surname>Cao</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>\n",
      "\t\t<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n",
      "\t\t\t<biblScope unit=\"volume\">2</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"776\" to=\"785\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note>Short Papers</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b50\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">GPT-4 technical report</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Openai</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno>arXiv</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"2303\" to=\"08774\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b51\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Training language models to follow instructions with human feedback</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Long</forename><surname>Ouyang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xu</forename><surname>Jiang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Diogo</forename><surname>Almeida</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Carroll</forename><surname>Wainwright</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pamela</forename><surname>Mishkin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chong</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sandhini</forename><surname>Agarwal</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Katarina</forename><surname>Slama</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Alex</forename><surname>Ray</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Advances in Neural Information Processing Systems</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">35</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"27730\" to=\"27744\" />\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b52\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Retrieval augmented code generation and summarization</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Md Rizwan Parvez</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Uddin</forename><surname>Wasi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Saikat</forename><surname>Ahmad</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Baishakhi</forename><surname>Chakraborty</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Kai-Wei</forename><surname>Ray</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Chang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2108.11601</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b53\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Can OpenAI&apos;s codex fix bugs? an evaluation on QuixBugs</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Aron</forename><surname>Julian</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hlib</forename><surname>Prenner</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Romain</forename><surname>Babii</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Robbes</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the Third International Workshop on Automated Program Repair</title>\n",
      "\t\t<meeting>the Third International Workshop on Automated Program Repair</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"69\" to=\"75\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b54\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Language models are unsupervised multitask learners</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">David</forename><surname>Luan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">OpenAI blog</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">1</biblScope>\n",
      "\t\t\t<biblScope unit=\"issue\">8</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\">9</biblScope>\n",
      "\t\t\t<date type=\"published\" when=\"2019\">2019. 2019</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b55\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Codebleu: a method for automatic evaluation of code synthesis</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daya</forename><surname>Shuo Ren</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuai</forename><surname>Guo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Long</forename><surname>Lu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shujie</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Duyu</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Neel</forename><surname>Tang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ming</forename><surname>Sundaresan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ambrosio</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuai</forename><surname>Blanco</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Ma</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2009.10297</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2020\">2020. 2020</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b56\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Code llama: Open foundation models for code</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jonas</forename><surname>Baptiste Roziere</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fabian</forename><surname>Gehring</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sten</forename><surname>Gloeckle</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Itai</forename><surname>Sootla</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Gat</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ellen</forename><surname>Xiaoqing</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yossi</forename><surname>Tan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jingyu</forename><surname>Adi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tal</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jérémy</forename><surname>Remez</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Rapin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2308.12950</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b57\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Utilization of pre-trained language models for adapter-based knowledge transfer in software engineering</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Iman</forename><surname>Saberi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fatemeh</forename><surname>Fard</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Fuxiang</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Empirical Software Engineering</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">29</biblScope>\n",
      "\t\t\t<biblScope unit=\"issue\">4</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\">94</biblScope>\n",
      "\t\t\t<date type=\"published\" when=\"2024\">2024. 2024</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b58\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Iman</forename><surname>Saberi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">H</forename><surname>Fatemeh</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Fard</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2303.06233</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b59\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Prompting large language models with answer heuristics for knowledge-based visual question answering</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhenwei</forename><surname>Shao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhou</forename><surname>Yu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Meng</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jun</forename><surname>Yu</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
      "\t\t<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"14974\" to=\"14983\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b60\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Adafactor: Adaptive learning rates with sublinear memory cost</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mitchell</forename><surname>Stern</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">International Conference on Machine Learning</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>PMLR</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2018\">2018</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"4596\" to=\"4604\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b61\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Repository-level prompt generation for large language models of code</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Disha</forename><surname>Shrivastava</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hugo</forename><surname>Larochelle</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Tarlow</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">International Conference on Machine Learning</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>PMLR</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"31693\" to=\"31715\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b62\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Energy and policy considerations for deep learning in NLP</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Emma</forename><surname>Strubell</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ananya</forename><surname>Ganesh</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Andrew</forename><surname>Mccallum</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:1906.02243</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2019\">2019. 2019</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b63\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Treegen: A tree-based transformer architecture for code generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zeyu</forename><surname>Sun</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qihao</forename><surname>Zhu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yingfei</forename><surname>Xiong</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yican</forename><surname>Sun</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lili</forename><surname>Mou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lu</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the AAAI Conference on Artificial Intelligence</title>\n",
      "\t\t<meeting>the AAAI Conference on Artificial Intelligence</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n",
      "\t\t\t<biblScope unit=\"volume\">34</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"8984\" to=\"8991\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b64\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Llama: Open and efficient foundation language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hugo</forename><surname>Touvron</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Thibaut</forename><surname>Lavril</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Gautier</forename><surname>Izacard</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xavier</forename><surname>Martinet</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Marie-Anne</forename><surname>Lachaux</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Timothée</forename><surname>Lacroix</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Baptiste</forename><surname>Rozière</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Naman</forename><surname>Goyal</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Eric</forename><surname>Hambro</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Faisal</forename><surname>Azhar</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2302.13971</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b65\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Efficient methods for natural language processing: A survey</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Marcos</forename><surname>Treviso</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ji-Ung</forename><surname>Lee</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tianchu</forename><surname>Ji</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Betty</forename><surname>Van Aken</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qingqing</forename><surname>Cao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Manuel R Ciosici</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Kenneth</forename><surname>Hassid</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sara</forename><surname>Heafield</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Colin</forename><surname>Hooker</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Raffel</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Transactions of the Association for Computational Linguistics</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">11</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"826\" to=\"860\" />\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b66\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Priyan</forename><surname>Vaithilingam</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tianyi</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Elena</forename><forename type=\"middle\">L</forename><surname>Glassman</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Chi conference on human factors in computing systems extended abstracts</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"1\" to=\"7\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b67\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Attention is all you need</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Łukasz</forename><surname>Kaiser</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Illia</forename><surname>Polosukhin</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Advances in neural information processing systems</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">30</biblScope>\n",
      "\t\t\t<date type=\"published\" when=\"2017\">2017. 2017</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b68\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">No more finetuning? an experimental evaluation of prompt tuning in code intelligence</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chaozheng</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yuanhang</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Cuiyun</forename><surname>Gao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yun</forename><surname>Peng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hongyu</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Michael R Lyu</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>\n",
      "\t\t<meeting>the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"382\" to=\"394\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b69\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Deze</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Boxing</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shanshan</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Wei</forename><surname>Luo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shaoliang</forename><surname>Peng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Wei</forename><surname>Dong</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiangke</forename><surname>Liao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2303.15822</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b70\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Weishi</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yue</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shafiq</forename><surname>Joty</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Steven Ch</forename><surname>Hoi</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>\n",
      "\t\t<meeting>the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"146\" to=\"158\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b71\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Codet5+: Open code large language models for code understanding and generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yue</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hung</forename><surname>Le</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">D</forename><forename type=\"middle\">Q</forename><surname>Nghi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Junnan</forename><surname>Bui</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Steven Ch</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Hoi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2305.07922</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b72\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Codet5: Identifier-aware unified pre-trained encoderdecoder models for code understanding and generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yue</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Weishi</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shafiq</forename><surname>Joty</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Steven Ch</forename><surname>Hoi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2109.00859</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b73\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Execution-Based Evaluation for Open-Domain Code Generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhiruo</forename><surname>Wang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuyan</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Fried</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Graham</forename><surname>Neubig</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2212.10481</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b74\">\n",
      "\t<monogr>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Albert</forename><surname>Webson</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ellie</forename><surname>Pavlick</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2109.01247</idno>\n",
      "\t\t<title level=\"m\">Do prompt-based models really understand the meaning of their prompts?</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b75\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Finetuned language models are zero-shot learners</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jason</forename><surname>Wei</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Maarten</forename><surname>Bosma</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Y</forename><surname>Vincent</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Kelvin</forename><surname>Zhao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Adams</forename><forename type=\"middle\">Wei</forename><surname>Guu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Brian</forename><surname>Yu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Nan</forename><surname>Lester</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Andrew</forename><forename type=\"middle\">M</forename><surname>Du</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Quoc V</forename><surname>Dai</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Le</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2109.01652</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021. 2021</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b76\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Emergent abilities of large language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Jason</forename><surname>Wei</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yi</forename><surname>Tay</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Rishi</forename><surname>Bommasani</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Colin</forename><surname>Raffel</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Barret</forename><surname>Zoph</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sebastian</forename><surname>Borgeaud</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dani</forename><surname>Yogatama</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Maarten</forename><surname>Bosma</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Denny</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Donald</forename><surname>Metzler</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2206.07682</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022. 2022</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b77\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Better modeling the programming world with code concept graphs-augmented multi-modal learning</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Martin</forename><surname>Weyssow</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Houari</forename><surname>Sahraoui</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bang</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results</title>\n",
      "\t\t<meeting>the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"21\" to=\"25\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b78\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Martin</forename><surname>Weyssow</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xin</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Kisub</forename><surname>Kim</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">David</forename><surname>Lo</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Houari</forename><surname>Sahraoui</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2305.04106</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b79\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Clement</forename><surname>Delangue</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Anthony</forename><surname>Moi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pierric</forename><surname>Cistac</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tim</forename><surname>Rault</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Rémi</forename><surname>Louf</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Morgan</forename><surname>Funtowicz</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:1910.03771</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2019\">2019. 2019</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b80\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Automated program repair in the era of large pre-trained language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chunqiu</forename><surname>Steven Xia</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yuxiang</forename><surname>Wei</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lingming</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 45th International Conference on Software Engineering (ICSE 2023)</title>\n",
      "\t\t<meeting>the 45th International Conference on Software Engineering (ICSE 2023)</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>Association for Computing Machinery</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b81\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Less training, more repairing please: revisiting automated program repair via zero-shot learning</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chunqiu</forename><surname>Steven</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xia</forename></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Lingming</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>\n",
      "\t\t<meeting>the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"959\" to=\"971\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b82\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">A Systematic Evaluation of Large Language Models of Code (MAPS 2022)</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">F</forename><surname>Frank</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Uri</forename><surname>Xu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Graham</forename><surname>Alon</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Vincent</forename><surname>Neubig</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><surname>Josua Hellendoorn</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"DOI\">10.1145/3520312.3534862</idno>\n",
      "\t\t<ptr target=\"https://doi.org/10.1145/3520312.3534862\" />\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n",
      "\t\t\t<publisher>Association for Computing Machinery</publisher>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"1\" to=\"10\" />\n",
      "\t\t\t<pubPlace>New York, NY, USA</pubPlace>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b83\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Exploring Continual Learning for Code Generation Models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Prateek</forename><surname>Yadav</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qing</forename><surname>Sun</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hantian</forename><surname>Ding</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiaopeng</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dejiao</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ming</forename><surname>Tan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiaofei</forename><surname>Ma</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Parminder</forename><surname>Bhatia</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Ramesh</forename><surname>Nallapati</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Murali</forename><surname>Krishna Ramanathan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2307.02435</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b84\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">ExploitGen: Templateaugmented exploit code generation based on CodeBERT</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Guang</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yu</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiang</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xiangyu</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Tingting</forename><surname>Han</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Taolue</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"j\">Journal of Systems and Software</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<biblScope unit=\"volume\">197</biblScope>\n",
      "\t\t\t<biblScope unit=\"page\">111577</biblScope>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b85\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Language in a bottle: Language model guided concept bottlenecks for interpretable image classification</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yue</forename><surname>Yang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Artemis</forename><surname>Panagopoulou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shenghao</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Jin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Chris</forename><surname>Callison-Burch</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mark</forename><surname>Yatskar</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
      "\t\t<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"19187\" to=\"19197\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b86\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pengcheng</forename><surname>Yin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bowen</forename><surname>Deng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Edgar</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bogdan</forename><surname>Vasilescu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Graham</forename><surname>Neubig</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"DOI\">10.1145/3196398.3196408</idno>\n",
      "\t\t<ptr target=\"https://doi.org/10.1145/3196398.3196408\" />\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">Proceedings of the 15th International Conference on Mining Software Repositories</title>\n",
      "\t\t<meeting>the 15th International Conference on Mining Software Repositories<address><addrLine>Gothenburg, Sweden; New York, NY, USA</addrLine></address></meeting>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>Association for Computing Machinery</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2018\">2018</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"476\" to=\"486\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b87\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Learning to mine aligned code and natural language pairs from stack overflow</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Pengcheng</forename><surname>Yin</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bowen</forename><surname>Deng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Edgar</forename><surname>Chen</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bogdan</forename><surname>Vasilescu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Graham</forename><surname>Neubig</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">IEEE/ACM 15th international conference on mining software repositories (MSR)</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>IEEE</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2018\">2018. 2018</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"476\" to=\"486\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b88\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Evaluating instruction-tuned large language models on code comprehension and generation</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhiqiang</forename><surname>Yuan</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Junwei</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Qiancheng</forename><surname>Zi</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Mingwei</forename><surname>Liu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xin</forename><surname>Peng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yiling</forename><surname>Lou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2308.01240</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023-12\">2023. 2023. December 2024</date>\n",
      "\t\t\t<biblScope unit=\"volume\">1</biblScope>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "\t<note>Publication date</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b89\">\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\" type=\"main\">Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Bowen</forename><surname>Zhang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xianghua</forename><surname>Fu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Daijun</forename><surname>Ding</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Hu</forename><surname>Huang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Yangyang</forename><surname>Li</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Liwen</forename><surname>Jing</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<idno type=\"arXiv\">arXiv:2304.03087</idno>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023. 2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "\t<note type=\"report_type\">arXiv preprint</note>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b90\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Calibrate before use: Improving few-shot performance of language models</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zihao</forename><surname>Zhao</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Eric</forename><surname>Wallace</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shi</forename><surname>Feng</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Dan</forename><surname>Klein</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Sameer</forename><surname>Singh</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">International Conference on Machine Learning</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>PMLR</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"12697\" to=\"12706\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b91\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Docprompting: Generating code by retrieving the docs</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Shuyan</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Uri</forename><surname>Alon</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Frank</forename><forename type=\"middle\">F</forename><surname>Xu</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Zhengbao</forename><surname>Jiang</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Graham</forename><surname>Neubig</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">The Eleventh International Conference on Learning Representations</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<date type=\"published\" when=\"2023\">2023</date>\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "<biblStruct xml:id=\"b92\">\n",
      "\t<analytic>\n",
      "\t\t<title level=\"a\" type=\"main\">Assessing generalizability of codebert</title>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Xin</forename><surname>Zhou</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">Donggyun</forename><surname>Han</surname></persName>\n",
      "\t\t</author>\n",
      "\t\t<author>\n",
      "\t\t\t<persName><forename type=\"first\">David</forename><surname>Lo</surname></persName>\n",
      "\t\t</author>\n",
      "\t</analytic>\n",
      "\t<monogr>\n",
      "\t\t<title level=\"m\">2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)</title>\n",
      "\t\t<imprint>\n",
      "\t\t\t<publisher>IEEE</publisher>\n",
      "\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n",
      "\t\t\t<biblScope unit=\"page\" from=\"425\" to=\"436\" />\n",
      "\t\t</imprint>\n",
      "\t</monogr>\n",
      "</biblStruct>\n",
      "\n",
      "\t\t\t\t</listBibl>\n",
      "\t\t\t</div>\n",
      "\t\t</back>\n",
      "\t</text>\n",
      "</TEI>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def processFulltextDocument(pdf_path, grobid_url=\"https://kermitt2-grobid.hf.space\"):\n",
    "    files = {\"input\": open(pdf_path, \"rb\")}\n",
    "    params = {\"consolidate\": \"1\"}\n",
    "    r = requests.post(\n",
    "        f\"{grobid_url}/api/processFulltextDocument\",\n",
    "        files=files,\n",
    "        params=params,\n",
    "        timeout=60\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.text  # TEI XML\n",
    "xml_metadata = processFulltextDocument(pdf_file)\n",
    "print(xml_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      " <ns0:teiHeader xmlns:ns0=\"http://www.tei-c.org/ns/1.0\" xml:lang=\"en\">\n",
      "\t\t<ns0:fileDesc>\n",
      "\t\t\t<ns0:titleStmt>\n",
      "\t\t\t\t<ns0:title level=\"a\" type=\"main\">Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</ns0:title>\n",
      "\t\t\t</ns0:titleStmt>\n",
      "\t\t\t<ns0:publicationStmt>\n",
      "\t\t\t\t<ns0:publisher />\n",
      "\t\t\t\t<ns0:availability status=\"unknown\">\n",
      "\t\t\t\t\t<ns0:licence />\n",
      "\t\t\t\t</ns0:availability>\n",
      "\t\t\t\t<ns0:date type=\"published\" when=\"2024-12-27\">27 Dec 2024</ns0:date>\n",
      "\t\t\t</ns0:publicationStmt>\n",
      "\t\t\t<ns0:sourceDesc>\n",
      "\t\t\t\t<ns0:biblStruct>\n",
      "\t\t\t\t\t<ns0:analytic>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">Martin</ns0:forename><ns0:surname>Weyssow</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t\t<ns0:email>martin.weyssow@umontreal.ca</ns0:email>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:surname>Diro</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">Kisub</ns0:forename><ns0:surname>Kim</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t\t<ns0:email>kisubkim@gmail.com</ns0:email>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">David</ns0:forename><ns0:surname>Lo</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t\t<ns0:email>davidlo@smu.edu.sg</ns0:email>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff0\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff1\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"SG\">Singapore</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff2\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"SG\">Singapore</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff3\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"department\" key=\"dep1\">Singapore HOUARI SAHRAOUI</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"department\" key=\"dep2\">DIRO</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\" key=\"instit1\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\" key=\"instit2\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff4\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"department\">DIRO</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Xin Zhou</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff5\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Singapore</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff6\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Singapore</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff7\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Singapore</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff8\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:title level=\"a\" type=\"main\">Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</ns0:title>\n",
      "\t\t\t\t\t</ns0:analytic>\n",
      "\t\t\t\t\t<ns0:monogr>\n",
      "\t\t\t\t\t\t<ns0:imprint>\n",
      "\t\t\t\t\t\t\t<ns0:date type=\"published\" when=\"2024-12-27\">27 Dec 2024</ns0:date>\n",
      "\t\t\t\t\t\t</ns0:imprint>\n",
      "\t\t\t\t\t</ns0:monogr>\n",
      "\t\t\t\t\t<ns0:idno type=\"MD5\">ED08C5420997939C148460D2D743B3D0</ns0:idno>\n",
      "\t\t\t\t\t<ns0:idno type=\"DOI\">10.1145/nnnnnnn.nnnnnnn</ns0:idno>\n",
      "\t\t\t\t\t<ns0:idno type=\"arXiv\">arXiv:2308.10462v3[cs.SE]</ns0:idno>\n",
      "\t\t\t\t</ns0:biblStruct>\n",
      "\t\t\t</ns0:sourceDesc>\n",
      "\t\t</ns0:fileDesc>\n",
      "\t\t<ns0:encodingDesc>\n",
      "\t\t\t<ns0:appInfo>\n",
      "\t\t\t\t<ns0:application version=\"0.8.1\" ident=\"GROBID\" when=\"2025-08-17T09:27+0000\">\n",
      "\t\t\t\t\t<ns0:desc>GROBID - A machine learning software for extracting information from scholarly documents</ns0:desc>\n",
      "\t\t\t\t\t<ns0:ref target=\"https://github.com/kermitt2/grobid\" />\n",
      "\t\t\t\t</ns0:application>\n",
      "\t\t\t</ns0:appInfo>\n",
      "\t\t</ns0:encodingDesc>\n",
      "\t\t<ns0:profileDesc>\n",
      "\t\t\t<ns0:textClass>\n",
      "\t\t\t\t<ns0:keywords>\n",
      "\t\t\t\t\t<ns0:term>CCS Concepts:</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>Software and its engineering → Software creation and management; Software development techniques code generation</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>large language models</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>parameter-efficient fine-tuning</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>quantization</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>empirical study 29.47</ns0:term>\n",
      "\t\t\t\t</ns0:keywords>\n",
      "\t\t\t</ns0:textClass>\n",
      "\t\t\t<ns0:abstract>\n",
      "<ns0:div><ns0:p>Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.</ns0:p></ns0:div>\n",
      "\t\t\t</ns0:abstract>\n",
      "\t\t</ns0:profileDesc>\n",
      "\t</ns0:teiHeader>\n",
      "\t \n",
      "\n",
      "Body:\n",
      " <ns0:body xmlns:ns0=\"http://www.tei-c.org/ns/1.0\">\n",
      "<ns0:div><ns0:head n=\"1\">INTRODUCTION</ns0:head><ns0:p>Large Language Models (LLMs) based on the Transformer architecture <ns0:ref type=\"bibr\" target=\"#b67\">[67]</ns0:ref>, demonstrate significant potential in diverse domains, including natural language processing (NLP) <ns0:ref type=\"bibr\" target=\"#b29\">[29,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b44\">44,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b76\">76]</ns0:ref>, computer vision <ns0:ref type=\"bibr\" target=\"#b7\">[7,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b59\">59,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b85\">85]</ns0:ref>, and software engineering <ns0:ref type=\"bibr\" target=\"#b9\">[9,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b66\">66,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b82\">82]</ns0:ref>. These models excel in generating high-quality content given natural language intents in zero-shot, i.e., without fine-tuning. This capability has sparked considerable interest in the software engineering field for automating code-related tasks such as program repair <ns0:ref type=\"bibr\" target=\"#b27\">[27,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b80\">80,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b81\">81]</ns0:ref> and code generation <ns0:ref type=\"bibr\" target=\"#b3\">[3,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b9\">9,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b48\">48]</ns0:ref>.</ns0:p><ns0:p>While the zero-shot capabilities of LLMs are impressive, their full potential often emerges through fine-tuning <ns0:ref type=\"bibr\" target=\"#b54\">[54,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b75\">75]</ns0:ref>. Specifically, fine-tuning an LLM to task-specific data allows it to learn and encode knowledge of the potentially highly contextual data at hand and thus generate more meaningful content. However, this process comes at a significant computational cost. Full fine-tuning, where all the parameters of the LLMs are updated during training, demands remarkable computational resources, especially when the LLM contains billions of parameters <ns0:ref type=\"bibr\" target=\"#b62\">[62]</ns0:ref>. To mitigate this computational burden, prior studies in software engineering <ns0:ref type=\"bibr\" target=\"#b53\">[53,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b80\">80,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref> have investigated prompt-engineering techniques such as In-Context Learning (ICL) <ns0:ref type=\"bibr\" target=\"#b5\">[5,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b54\">54]</ns0:ref> and Retrieval-Augmented Generation (RAG) <ns0:ref type=\"bibr\" target=\"#b31\">[31]</ns0:ref>. ICL consists of providing prompt examples of the task to the LLM, guiding it to generate contextually appropriate content without any fine-tuning involved. These examples can be manually created or randomly selected from a relevant training dataset. This technique has already shown promising results for code-related tasks, including automated program repair <ns0:ref type=\"bibr\" target=\"#b80\">[80]</ns0:ref>, bug fixing <ns0:ref type=\"bibr\" target=\"#b53\">[53]</ns0:ref>, and code generation <ns0:ref type=\"bibr\" target=\"#b61\">[61,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b73\">73,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref>. Expanding upon ICL, RAG offers a more robust and powerful alternative that incorporate a knowledge retrieval system at inference. Using RAG, a retrieval model fetches relevant information from an indexed corpus, such as code documentation or similar code snippets to the input problem. The retrieved information is then added to the input prompt to guide generation. Unlike ICL, which relies on preselected examples that may not always be tailored to the specific input, RAG dynamically adapts to each individual input problem, providing more relevant context. This technique has demonstrated significant improvements in software engineering tasks such as code generation and summarization <ns0:ref type=\"bibr\" target=\"#b39\">[39,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b52\">52,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref>, code completion <ns0:ref type=\"bibr\" target=\"#b41\">[41]</ns0:ref>, and program repair <ns0:ref type=\"bibr\" target=\"#b70\">[70]</ns0:ref>.</ns0:p><ns0:p>Although ICL and RAG provide a viable alternative to full fine-tuning, it operates at inference time and does not involve learning task-specific parameters, which may prevent the LLM from capturing fine-grained information about the task and result in a loss of effectiveness. In this context, Parameter-Efficient Fine-Tuning (PEFT) techniques have emerged as promising solutions to render the fine-tuning cost at the lowest while allowing the model to learn task-specific parameters. Prior works <ns0:ref type=\"bibr\" target=\"#b10\">[10,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b57\">57,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b68\">68,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b69\">69]</ns0:ref> in code intelligence have demonstrated the capability of PEFT techniques, and often shown their superiority over full fine-tuning across a wide range of tasks. However, these studies focus on small language models (SLMs) (&lt;0.25B parameters) such as CodeBERT <ns0:ref type=\"bibr\" target=\"#b16\">[16]</ns0:ref> and CodeT5 <ns0:ref type=\"bibr\" target=\"#b72\">[72]</ns0:ref> and overlooked the applicability of PEFT techniques to LLMs (≥1B parameters), leaving an important research gap. Given the growing ubiquity of LLMs, we believe addressing this gap is paramount in advancing the field of code intelligence and harnessing the full potential of LLMs. Furthermore, we identify an additional research opportunity in exploring the usage of PEFT techniques under limited resource scenarios, aiming to demonstrate the democratization of LLMs tuning through PEFT. Addressing these gaps will not only show how PEFT techniques can enhance the effectiveness of LLMs but also how they broaden the accessibility and utility of LLMs in scarce computation settings and alleviate the dependence of practitioners on large computational infrastructures.</ns0:p><ns0:p>In this paper, we present an empirical study on the usage of existing PEFT techniques with LLMs. We focus our study on code generation, which has been a pivotal area of research due to its transformative impact on automating software development <ns0:ref type=\"bibr\" target=\"#b9\">[9,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b48\">48,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b50\">50]</ns0:ref>. Our objective is twofold. First, we aim to assess the code generation capabilities of LLMs using existing PEFT techniques such as LoRA <ns0:ref type=\"bibr\" target=\"#b24\">[24]</ns0:ref> and QLoRA <ns0:ref type=\"bibr\" target=\"#b13\">[13]</ns0:ref> on datasets without test cases, including Conala <ns0:ref type=\"bibr\" target=\"#b91\">[91]</ns0:ref> and CodeAlpacaPy <ns0:ref type=\"bibr\" target=\"#b8\">[8]</ns0:ref>, as well as the APPS dataset <ns0:ref type=\"bibr\" target=\"#b22\">[22]</ns0:ref> with test cases. Second, we seek to compare the effectiveness of LLMs tuned with these PEFT techniques against SLMs, ICL, and RAG. Additionally, we conduct our comparative study with limited availability of computational resources to investigate the broad practicality of using PEFT techniques for LLMs. To achieve these objectives, we formulate four research questions that guide our study:</ns0:p><ns0:p>-RQ1: How do LLMs and SLMs perform using ICL on the Conala and CodeAlpacaPy datasets? -RQ2: How do LLMs and SLMs perform using PEFT techniques on the Conala and CodeAlpacaPy datasets? -RQ3: How does LoRA compare with ICL and RAG on the Conala and CodeAlpacaPy datasets? -RQ4: Can we enhance the effectiveness of LLMs for code generation in the APPS dataset using LoRA and QLoRA?</ns0:p><ns0:p>Altogether, answering these four research questions fulfills both objectives of this empirical study. Our first three RQs focus on evaluating SLMs and LLMs for code generation on the Conala and CodeAlpaca datasets. In RQ1, we illustrate the baseline effectiveness of SLMs and LLMs using ICL, which retrieves random examples from the training set to guide the model in generating code. By addressing RQ2, we gain a comprehensive understanding of how effective SLMs and LLMs are when using different PEFT techniques. In RQ3, we conduct a comparative study of the effectiveness of LoRA with ICL and RAG, a strong baseline that dynamically retrieves relevant examples by selecting those closest to the test instructions from the training set. Finally, to showcase the potential broader impact of PEFT, we study in RQ4 whether tuning LLMs using LoRA and QLoRA can improve their effectiveness on APPS, a challenging benchmark with test cases.</ns0:p><ns0:p>To address these RQs, we conduct experiments on three datasets, APPS <ns0:ref type=\"bibr\" target=\"#b22\">[22]</ns0:ref>, Conala <ns0:ref type=\"bibr\" target=\"#b86\">[86]</ns0:ref>, and CodeAlpacaPy specifically curated from CodeAlpaca <ns0:ref type=\"bibr\" target=\"#b8\">[8]</ns0:ref> for Python code generation. Conversely to evaluation datasets such as HumanEval <ns0:ref type=\"bibr\" target=\"#b9\">[9]</ns0:ref>, the APPS, Conala and CodeAlpaca datasets, widely used in prior code generation studies <ns0:ref type=\"bibr\" target=\"#b49\">[49,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b71\">71,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b73\">73,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b73\">73,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b88\">88,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref>, include sufficient training examples that can be employed for fine-tuning. For a comprehensive comparative analysis, we select four distinct model families: CodeT5+ <ns0:ref type=\"bibr\" target=\"#b71\">[71]</ns0:ref>, CodeGen <ns0:ref type=\"bibr\" target=\"#b48\">[48]</ns0:ref>, CodeGen2 <ns0:ref type=\"bibr\" target=\"#b47\">[47]</ns0:ref>, and CodeLlama <ns0:ref type=\"bibr\" target=\"#b56\">[56]</ns0:ref>, including eight large and three small variants. Note that we omitted closed-sourced LLMs such as Codex due to the inaccessibility of their parameters, which makes the study of any fine-tuning technique infeasible. Furthermore, our study incorporates six PEFT techniques: LoRA <ns0:ref type=\"bibr\" target=\"#b24\">[24]</ns0:ref>, IA3 <ns0:ref type=\"bibr\" target=\"#b37\">[37]</ns0:ref>, Prompt tuning <ns0:ref type=\"bibr\" target=\"#b30\">[30]</ns0:ref>, and Prefix tuning <ns0:ref type=\"bibr\" target=\"#b33\">[33]</ns0:ref>. In addition, we explore QLoRA <ns0:ref type=\"bibr\" target=\"#b13\">[13]</ns0:ref> with 8-bit and 4-bit quantization, which combines LoRA and model quantization. Unlike ICL and RAG, these techniques entail learning new parameters to tune the LLMs for the specific downstream task. Our main findings are the following: -ICL drastically improves the effectiveness of all models compared to a zero-shot prompt for code generation on Conala and CodeAlpacaPy. -Increasing the number of ICL examples does not always lead to improvement in effectiveness. Models achieve peak effectiveness with eight and four examples for Conala and CodeAlpacaPy, respectively. -LLMs fine-tuned with LoRA, IA3, and Prompt tuning, i.e., a few millions of parameters, consistently outperform SLMs fully fine-tuned with hundreds of millions of parameters. -Among PEFT techniques, LoRA achieves the highest effectiveness for the LLMs and SLMs.</ns0:p><ns0:p>-QLoRA considerably reduces memory usage, achieving up to a 2-fold decrease compared to LoRA while improving or preserving the models' effectiveness. Furthermore, QLoRA enables the fine-tuning of LLMs up to 34B parameters for less than 24GB of GPU memory. -LoRA significantly enhances the performance of all models compared to ICL and RAG for code generation on Conala and CodeAlpacaPy. -LoRA and QLoRA improve CodeLlama-7B-Instruct's effectiveness for code generation on the APPS dataset. Our study sheds light on the promising opportunities that PEFT techniques hold, warranting further exploration for their application in other code-related tasks and scenarios.</ns0:p><ns0:p>To summarize, our contributions are the following: -We conduct a comprehensive empirical study of six PEFT techniques, i.e., LoRA, IA3, Prompt tuning, Prefix tuning, QLoRA-8bit, and QLoRA-4bit, for Python code generation over a broad range of SLMs and LLMs. -A comprehensive comparison and analysis of PEFT techniques against ICL and RAG for LLMs on code generation. -We demonstrate the practicality of leveraging PEFT techniques to effectively fine-tune LLMs of code and reduce the computational burden associated with full fine-tuning, showcasing their potential broader applications in software engineering.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"2\">BACKGROUND 2.1 In-Context Learning (ICL) and Retrieval-Augmented Generation (RAG)</ns0:head><ns0:p>As one of the specific types of LLM-related techniques, ICL has emerged as an effective technique <ns0:ref type=\"bibr\" target=\"#b5\">[5,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b11\">11,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b35\">35,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b45\">45,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b51\">51]</ns0:ref>. ICL seeks to improve the abilities of LLMs by integrating context-specific information, in the form of an input prompt or instruction template, during the inference and thus without the need to perform gradient-based training. Therefore, by considering the context, the model becomes more capable of generating coherent and contextually relevant outputs. This contextual coherence of the LLM and not having to perform costly gradient-based training constitutes prime advantages of using ICL to specialize LLMs to a specific task or dataset. However, ICL also presents some inconveniences, including the need to design representative prompts <ns0:ref type=\"bibr\" target=\"#b37\">[37,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b74\">74,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b90\">90]</ns0:ref>. RAG is a more sophisticated approach to inject examples into input prompts at inference. Unlike ICL that select random examples, RAG relies on a retrieval model that dynamically retrieves examples from a dataset that are close to a query. In practice, the query can be formulated using information from the test example at test time, such as the coding problem for the case of code generation. Altogether, RAG allows injection more relevant information in the input prompt than ICL and has been succesfully applied to software engineering tasks, such as code generation <ns0:ref type=\"bibr\" target=\"#b52\">[52,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref>, code summarization <ns0:ref type=\"bibr\" target=\"#b39\">[39,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b52\">52]</ns0:ref>, and code completion <ns0:ref type=\"bibr\" target=\"#b41\">[41]</ns0:ref>. Nonetheless, both ICL and RAG suffer from a few limitations. One concerns the introduction of extra input tokens in the prompt, which may be infeasible when the contextual information is too large. Another limitation is the reliance on the quality and relevance of the retrieved examples. In RAG, the retrieval model must accurately find examples that are genuinely similar or useful for the test query. If the retrieval mechanism fails to identify appropriate examples, it can inject irrelevant or misleading information into the prompt, ultimately degrading performance.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"2.2\">Parameter-Efficient Fine-Tuning (PEFT)</ns0:head><ns0:p>PEFT refer to the utilization of techniques that optimize the fine-tuning process of LLMs by selectively updating a subset of parameters instead of updating the entire model's parameters <ns0:ref type=\"bibr\" target=\"#b14\">[14]</ns0:ref>. Technically, PEFT techniques focus on learning a small number of parameters for the task at hand by designing additional layers <ns0:ref type=\"bibr\" target=\"#b23\">[23]</ns0:ref>, adding prepending additional tokens <ns0:ref type=\"bibr\" target=\"#b30\">[30,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b33\">33]</ns0:ref>, decomposing weight gradients into specific matrices <ns0:ref type=\"bibr\" target=\"#b24\">[24]</ns0:ref>. One of the representative cutting-edge PEFT techniques is LOw-Rank Adaptation of LLMs (LoRA) <ns0:ref type=\"bibr\" target=\"#b24\">[24]</ns0:ref>. The technique consists of freezing the model weights and injecting low-rank trainable matrices into the attention layers of the Transformer architecture <ns0:ref type=\"bibr\" target=\"#b67\">[67]</ns0:ref>, thereby drastically reducing the number of trainable parameters. We employ LoRA as one of our PEFT technique since it has been widely used in NLP <ns0:ref type=\"bibr\" target=\"#b14\">[14,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b37\">37,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b65\">65]</ns0:ref> and showed promising performance. We also employ IA3 which intends to improve upon LoRA and further reduces the amount of trainable parameters <ns0:ref type=\"bibr\" target=\"#b37\">[37]</ns0:ref>. In addition to LoRA and IA3, we also include Prompt tuning <ns0:ref type=\"bibr\" target=\"#b30\">[30]</ns0:ref> and Prefix tuning <ns0:ref type=\"bibr\" target=\"#b30\">[30]</ns0:ref> in our study. Prompt tuning involves the process of prepending virtual tokens to the input tokens of the LLM, whereas Prefix tuning inserts virtual tokens in all the layers of the target model and thus requires learning more parameters. These virtual tokens are differentiable, allowing them to be learned through backpropagation during fine-tuning, while the rest of the LLM remains frozen. Furthermore, QLoRA <ns0:ref type=\"bibr\" target=\"#b13\">[13]</ns0:ref> combines LoRA with model quantization, enabling the fine-tuning of LLMs with less GPU memory by reducing the precision of floating point data types within the model.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"3\">APPLYING LLMS WITH LIMITED RESOURCES</ns0:head><ns0:p>In the era of LLMs, the availability of substantial computational resources plays a crucial role in harnessing their high capabilities. Unfortunately, many researchers and practitioners often find themselves constrained by the limited availability of high-end computing infrastructures.</ns0:p><ns0:p>For instance, a software engineer with access to only a single consumer GPU (e.g., 24GB of VRAM) may find full fine-tuning impractical due to the significant memory demands. The rapid increase in model size and the number of trainable parameters exacerbates this issue. Despite its effectiveness, full fine-tuning comes at a steep computational cost <ns0:ref type=\"bibr\" target=\"#b3\">[3,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b15\">15,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b51\">51]</ns0:ref>., underscoring a computation-effectiveness trade-off (see Table <ns0:ref type=\"table\" target=\"#tab_0\">1</ns0:ref>).</ns0:p><ns0:p>To address these limitations, alternative approaches like ICL and RAG have gained attention. ICL and RAG offer a low-computation option by eliminating the need for parameter updates. However, these techniques comes with its own set of challenges, including the selection of representative examples and sensitivity to prompt design <ns0:ref type=\"bibr\" target=\"#b37\">[37,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b74\">74,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b90\">90]</ns0:ref>. In practice, this can result in lower effectiveness compared to fine-tuning, particularly for highly contextual tasks prevalent in software engineering.  To overcome these limitations, we foresee the emergence of PEFT techniques as promising solutions, offering more computationally efficient and scalable approaches to fine-tuning LLMs. PEFT methods, such as LoRA and QLoRA, limit the number of parameters being updated, thus reducing memory consumption while maintaining effectiveness competitive with full fine-tuning. This makes PEFT particularly well-suited for practitioners with limited access to computational resources. As illustrated in Table <ns0:ref type=\"table\" target=\"#tab_0\">1</ns0:ref>, PEFT strike an optimal balance between computational cost and effectiveness. Furthermore, Fig. <ns0:ref type=\"figure\" target=\"#fig_0\">1</ns0:ref> shows that by employing PEFT techniques like LoRA, practitioners can fine-tune models such as CodeLlama-7B without exceeding 19GB of GPU memory. For even larger models, such as CodeLlama-34B, QLoRA with quantization enables fine-tuning within the constraints of a 24GB VRAM GPU.</ns0:p><ns0:p>In conclusion, PEFT empower software engineers to overcome resource limitations, allowing for effective LLM fine-tuning in highly contextual tasks without relying on expensive computational infrastructures. This makes PEFT not only a practical but also an essential tool for democratizing access to LLM capabilities.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"4\">METHODOLOGY</ns0:head><ns0:p>In this section, we present the experimental setup of our study. We conduct all the experiments under a resource-constrained scenario. Specifically, all the procedures, i.e., fine-tuning and inference, of the models are performed with access to a single 24GB GPU. The main objective of our study is to demonstrate whether the fine-tuning of LLMs through PEFT is feasible and desirable over previous approaches and smaller models in this context.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"4.1\">Research Questions</ns0:head><ns0:p>In this study, we focus on the following research questions:</ns0:p><ns0:p>-RQ1: How do LLMs and SLMs perform using ICL on the Conala and CodeAlpacaPy datasets?</ns0:p><ns0:p>We study the baseline effectiveness of LLMs (≥ 1B parameters) and SLMs (&lt; 1B parameters) for code generation using the zero-shot prompt and ICL, where 𝑛 randomly selected examples are added to the input prompt. We test each model with up to 16 ICL examples, due to our limited computation resources.</ns0:p><ns0:p>We study the effectiveness of a large spectrum of SLMs and LLMs for code generation on two datasets covering codes of various lengths. We select a wide range of models of various sizes, pre-trained on diverse codebases and with different learning objectives to study how these factors impact their effectiveness. -RQ2: How do LLMs and SLMs perform using PEFT techniques on the Conala and CodeAl-pacaPy datasets? In this RQ, we investigate whether PEFT techniques consistently outperform ICL for SLMs and LLMs. We compare the best-performing configurations of ICL in RQ1 with PEFT techniques, including LoRA, IA3, Prompt tuning, Prefix tuning. Furthermore, we also investigate the effect of quantization with QLoRA-8bit and QLoRA-4bit on our best-performing model and larger variants.</ns0:p><ns0:p>For SLMs, we also include a comparison with full-parameter fine-tuning, as commonly used in previous SE studies <ns0:ref type=\"bibr\" target=\"#b16\">[16,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b72\">72,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b77\">77,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b92\">92]</ns0:ref>. We do not include full-parameter fine-tuning for LLMs, as it is not feasible within our computational budget. -RQ3: How does LoRA compare with ICL and RAG on the Conala and CodeAlpacaPy datasets?</ns0:p><ns0:p>In this RQ, we compare the effectiveness of our best-performing LLM fine-tuned using LoRA with RAG. Our RAG setup consists of retrieving up to 16 examples from the training set that are closely related to the input prompt, which is similar to other approaches previously proposed for various SE tasks <ns0:ref type=\"bibr\" target=\"#b39\">[39,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b41\">41,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b70\">70]</ns0:ref>. -RQ4: Can we enhance the effectiveness of LLMs for code generation in the APPS dataset using LoRA and QLoRA? Lastly, we explore whether LLM fine-tuned using LoRA and QLoRA show improvement in functional correctness in the APPS dataset. We fine-tune our best-performing LLM using LoRA and QLoRA on the training set of APPS, and report the average of test cases passed as well as the Pass@𝑘 on APPS' test set for introductory, interview, and competition-level coding problems.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"4.2\">Datasets and Task</ns0:head><ns0:p>Throughout our study, we compare all the studied models on a Python code generation task. This task has gained significant attention in recent years <ns0:ref type=\"bibr\" target=\"#b9\">[9,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b10\">10,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b48\">48,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b50\">50,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b61\">61]</ns0:ref> with the emergence of LLMs and their capability to generate Python code in zero-shot, i.e., without further fine-tuning. In particular, evaluation datasets such as HumanEval <ns0:ref type=\"bibr\" target=\"#b9\">[9]</ns0:ref> have extensively been used to benchmark code generation approaches <ns0:ref type=\"bibr\" target=\"#b3\">[3,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b9\">9,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b82\">82]</ns0:ref>. While HumanEval is widely utilized, it lacks a training corpus to evaluate finetuning or PEFT approaches. As our study's focus is on specializing LLMs using PEFT techniques, we have opted not to utilize HumanEval. Instead, we choose to use three other widely-used code generation datasets: the Conala <ns0:ref type=\"bibr\" target=\"#b87\">[87]</ns0:ref>, CodeAlpaca <ns0:ref type=\"bibr\" target=\"#b8\">[8]</ns0:ref>, and APPS <ns0:ref type=\"bibr\" target=\"#b22\">[22]</ns0:ref> datasets. All datasets provide an ample number of examples that can be employed for fine-tuning a model and have been used in prior code generation studies with LLMs <ns0:ref type=\"bibr\" target=\"#b71\">[71,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b73\">73,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b88\">88,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref>.</ns0:p><ns0:p>Conala dataset. We use a curated version of the Conala dataset <ns0:ref type=\"bibr\" target=\"#b91\">[91]</ns0:ref>. The dataset was crawled from StackOverflow and contains manually annotated pairs of code and natural language intent. Each natural language intent contains hints about the manipulated variables in the ground truth code, e.g., see the first example in Table <ns0:ref type=\"table\" target=\"#tab_2\">2</ns0:ref>, providing more context to the model for generating relevant code. In Figure <ns0:ref type=\"figure\" target=\"#fig_1\">2</ns0:ref>, we report the token length distributions of the three datasets. In Conala, most code solutions are short and one-liners, making it relatively easy for an LLM to generate exact match predictions. APPS dataset. The APPS dataset consists of 10,000 code generation problems, each paired with Python solutions. These problems are categorized into three difficulty levels: introductory, interview, and competition, with solutions varying from simple one-liners to complex algorithms. We can see in Fig. <ns0:ref type=\"figure\" target=\"#fig_1\">2</ns0:ref> and Table <ns0:ref type=\"table\" target=\"#tab_2\">2</ns0:ref> that APPS include more lengthy and complex examples than the two other datasets. On average, each problem is accompanied by 21.2 test cases, designed to evaluate the functional correctness of the generated code. The original dataset is split into 5,000 samples for training and 5,000 for testing. ### Instruction: You are given a string s = s1 s2 . . . sn of length n, which only contains digits 1, 2,..., 9. A substring s[l...r] of s is a string slsl+1sl+2 ...sr. A substring s[l...r] of s is called even if the number represented by it is even. Find the number of even substrings of s. Note, that even if some substrings are equal as strings, but have different l and r, they are counted as different substrings. The first line contains an integer n (1 ≤ n ≤ 65000) -the length of the string s. The second line contains a string s of length n. The string s consists only of digits 1, 2,..., 9. Print the number of even substrings of s. ### Response: Ground truth:</ns0:p><ns0:formula xml:id=\"formula_0\">n = int(input()) ans = 0 for i in range(n): for j in range(i, n): if int(s[i:j+1]) \\% 2 == 0: ans += 1 print(ans)</ns0:formula><ns0:p>In this study, we use 4,500 samples for training, 500 for validation, and 750 for testing, ensuring a balanced distribution of 250 test samples per difficulty level.</ns0:p><ns0:p>Task design. In Table <ns0:ref type=\"table\" target=\"#tab_2\">2</ns0:ref>, we illustrate an overview of the task design. The prompt is in the form of an instruction template, where \"### Instruction:\" and \"### Response:\" play the role of delimiting the instruction, i.e., natural language intent, and the answer, i.e., code generation. Note that this prompt design may not be optimal, but this kind of instruction template has shown to be effective in prior works <ns0:ref type=\"bibr\" target=\"#b36\">[36,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b89\">89]</ns0:ref>. The code generated by the model is compared with the ground truth to assess the quality of the generation. During fine-tuning, we minimize a standard autoregressive cross-entropy loss function:</ns0:p><ns0:formula xml:id=\"formula_1\">L = - 𝑇 +1 ∑︁ 𝑖=1 𝑀 𝑖 • log 𝑃 (𝑥 𝑖 | 𝑥 &lt;𝑖 ) ,</ns0:formula><ns0:p>where:</ns0:p><ns0:formula xml:id=\"formula_2\">𝑀 𝑖 = 1 , if 𝑥 𝑖 ≠ -100 0 , otherwise.</ns0:formula><ns0:p>The model receives a concatenation of the prompt and the ground truth as input and predicts each token 𝑥 𝑖 in an autoregressive manner given the previous tokens 𝑥 &lt;𝑖 . Note that in the computation of the loss, we ignore the tokens from the instruction template to force the model to focus on generating code. We set the value of the instruction tokens to -100 and ignore them in the loss computation using the indicator function 𝑀 𝑖 . At inference, the model receives the prompt as input and attempts to generate the ground truth code by generating up to 10 code candidates.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"4.3\">ICL and RAG</ns0:head><ns0:p>We conduct experiments using ICL and RAG on the Conala and CodeAlpacaPy datasets. For both techniques, we select the maximum number of samples that can fit into our GPU memory. For ICL, we use up to 16 examples for the Conala dataset and 8 examples for CodeAlpacaPy. These examples are randomly sampled from the corresponding training datasets and concatenated with the input prompt during inference. For RAG, we leverage GTE-small, a general-purpose, lightweight embedding model that outperforms many larger models, including OpenAI's proprietary embeddings <ns0:ref type=\"bibr\" target=\"#b34\">[34]</ns0:ref>. We generate embeddings for all instructions (excluding the code) in the training sets. At inference time, we retrieve up to 16 examples for Conala and 4 examples for CodeAlpacaPy, selecting those with instructions most similar to the test input. As with ICL, the retrieved examples are concatenated with the input problem to guide code generation.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"4.4\">Small and Large Language Models</ns0:head><ns0:p>In order to carry out a comprehensive analysis, we selected our SLMs and LLMs according to several criteria. First, we exclusively considered open-source models. We omitted closed-sourced LLMs such as Codex due to the inaccessibility of their parameters, which makes the study of any fine-tuning technique infeasible. All the studied models' checkpoints can be freely accessed, and have been pre-trained using open-source data. Secondly, we selected LLMs, which have been released within the past two years. Finally, to investigate the impact of scaling, we selected models with a diverse range of parameters. We consider models with less than 1B parameters as SLMs, and the others as LLMs. Note that we selected models that fit a single 24GB GPU for fine-tuning and inference without causing memory overflow. In total, we included 11 SLMs and LLMs from diverse families of models to conduct our experiments. -SLMs. We use CodeGen-350M-mono <ns0:ref type=\"bibr\" target=\"#b48\">[48]</ns0:ref>, CodeT5+-220M <ns0:ref type=\"bibr\" target=\"#b71\">[71]</ns0:ref>, and CodeT5+-770M <ns0:ref type=\"bibr\" target=\"#b71\">[71]</ns0:ref> as SLMs.</ns0:p><ns0:p>CodeGen-350M-mono is an autoregressive language model and a small version of CodeGen pretrained on various programming languages and further fine-tuned on Python data. CodeT5+-220M and CodeT5+-770M are encoder-decoder language models that improve upon CodeT5 by leveraging a two-staged pre-training phase on natural language and code data, and new learning objectives.</ns0:p><ns0:p>-CodeGen2 <ns0:ref type=\"bibr\" target=\"#b47\">[47]</ns0:ref> is a family of prefix-based language models which combines the learning schemes of a bi-directional encoder and a uni-directional decoder. CodeGen2 improves upon CodeGen <ns0:ref type=\"bibr\" target=\"#b48\">[48]</ns0:ref>, therefore we do not include the CodeGen family in our evaluation. CodeGen2 models were pretrained on a deduplicated version of TheStack <ns0:ref type=\"bibr\" target=\"#b28\">[28]</ns0:ref> spanning a wide range of languages. We employ CodeGen2-1B, CodeGen2-3.7B and CodeGen2-7B. -CodeLlama <ns0:ref type=\"bibr\" target=\"#b56\">[56]</ns0:ref> is a family of LLMs based on Llama 2 <ns0:ref type=\"bibr\" target=\"#b64\">[64]</ns0:ref>. Each model was initialized with Llama 2 and further pre-trained on code. CodeLlama comes in three different variants: CodeLlama specialized for code, CodeLlama-Instruct specialized for instruction-tuning and CodeLlama-Python specialized for Python. We employ CodeLlama-7B, CodeLlama-7B-Instruct and CodeLlama-7B-Python to initiate our experiments. In RQ4, we fine-tune CodeLlama-13B-Python and CodeLlama-34B-Python using QLoRA.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"4.5\">Metrics</ns0:head><ns0:p>We measure the effectiveness of the models through widely used metrics in prior code generation work.</ns0:p><ns0:p>For experiments on Conala and CodeAlpacaPy, we report the Exact Match (EM) and CodeBLEU <ns0:ref type=\"bibr\" target=\"#b55\">[55]</ns0:ref> metrics. Given a generated code and a ground truth, the EM returns 1 if both codes are identical, otherwise 0. To evaluate the effectiveness of the models on a list of 𝑘 ∈ [1, 10] candidates, we report the EM@𝑘, which computes the average correct predictions among a list of 𝑘 candidates. For our experiments on the APPS dataset, we report two metrics: the average number of test cases passed and Pass@𝑘. The average number of test cases passed evaluates how well the model performs by measuring the proportion of test cases that its generated code passes for each sample. In contrast, Pass@𝑘 is a more stringent metric that measures the percentage of problems for which at least one of the top 𝑘 generated code samples passes all test cases, reflecting the model's ability to produce fully correct solutions within 𝑘 attempts.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"4.6\">Implementation Details</ns0:head><ns0:p>For all our experiments, we used a single NVIDIA RTX A5000 24GB GPU. We study a total of seven tuning techniques: Full fine-tuning, ICL, LoRA <ns0:ref type=\"bibr\" target=\"#b24\">[24]</ns0:ref>, IA3 <ns0:ref type=\"bibr\" target=\"#b37\">[37]</ns0:ref>, Prompt tuning <ns0:ref type=\"bibr\" target=\"#b30\">[30]</ns0:ref>, Prefix tuning <ns0:ref type=\"bibr\" target=\"#b33\">[33]</ns0:ref>, and QLoRA <ns0:ref type=\"bibr\" target=\"#b13\">[13]</ns0:ref>. We implemented all the tuning techniques using HuggingFace <ns0:ref type=\"bibr\" target=\"#b79\">[79]</ns0:ref> and PEFT <ns0:ref type=\"bibr\" target=\"#b43\">[43]</ns0:ref> libraries.</ns0:p><ns0:p>We used full fine-tuning only for the SLMs, as tuning all the parameters of the LLMs is computationally intractable within a maximum GPU memory of 24GB. We set the learning rate to 5𝑒 -5. For LoRA and IA3, we applied the low-rank matrix decomposition on the attention layers of the models and set 𝑟 = 16 and 𝛼 = 32. For implementing QLoRA, we use 8-bit and 4-bit quantization <ns0:ref type=\"bibr\" target=\"#b12\">[12]</ns0:ref>. We set the learning rate to 3𝑒 -4 for LoRA, IA3 and QLoRA. For Prompt tuning and Prefix tuning, we prepended a set of 20 trainable continuous virtual tokens to each input sample of the models and applied learning rates of 3𝑒 -3 and 3𝑒 -2.</ns0:p><ns0:p>We used Adafactor <ns0:ref type=\"bibr\" target=\"#b60\">[60]</ns0:ref> optimizer with 16-bit float precision for all models. We fine-tuned the models for a maximum of five epochs and evaluated them every 0.2 * 𝑙𝑒𝑛(𝑡𝑟𝑎𝑖𝑛_𝑠𝑒𝑡) optimization steps. We fine-tune all models with a batch size of 8. We selected the checkpoint with the lowest evaluation loss for inference and found that beam search with a beam size of 10 yields the best effectiveness. Given the various token length distribution and complexity of the datasets, we generate codes with up to 64, 128, and 1024 tokens for Conala, CodeAlpacaPy, and APPS, respectively. We make our code publicly available: <ns0:ref type=\"url\" target=\"https://github.com/martin-wey/peft-llm-code\">https://github.com/martin-wey/peft-llm-code</ns0:ref>. We evaluate the models' effectiveness using EM@10 and compare them across these two datasets in Fig. <ns0:ref type=\"figure\" target=\"#fig_2\">3</ns0:ref>. Note that CodeGen2 architecture results in substantially more GPU memory usage than other models, which explains why we evaluate ICL with fewer examples than other models. First, we observe a substantial gap in EM@10 between the two datasets. This difference can be explained by the fact that the CodeAlpacaPy dataset contains much more challenging samples compared to the Conala dataset, as shown in Table <ns0:ref type=\"table\" target=\"#tab_2\">2</ns0:ref>.</ns0:p><ns0:p>Second, there is a notable gap in effectiveness between SLMs and LLMs, regardless of the number of examples provided. This observation highlights the advantages of large-scale pre-training and the use of larger models in this context.</ns0:p><ns0:p>For the Conala dataset, increasing the number of examples leads to higher EM@10 scores. However, when using more than eight examples, the effectiveness of the models begins to decline. For the CodeAlpacaPy dataset, a similar trend is observed, but the optimal number of examples is smaller. Most models achieve their best EM@10 scores when using three or four examples. This observation underscores the limitation of ICL, as adding more examples results in a degradation of the models' effectiveness.</ns0:p><ns0:p>Finally, CodeLlama models outperform all models across both datasets, achieving a peak EM@10 of 29.83 on Conala (CodeLlama-7B) and 11.94 on CodeAlpacaPy (CodeLlama-7B-Python). In contrast, smaller models, such as CodeGen2-3.7B achieves an EM@10 of 23.94 and 7.00 on Conala and CodeAl-pacaPy, respectively.</ns0:p><ns0:p>Answer to RQ1: ICL drastically improves the effectiveness of all models compared to zero-shot. Our best model, CodeLlama-7B, achieves EM@10 scores of 29.83 (7.73) and 11.62 (7.01) on Conala and CodeAlpacaPy with ICL (zero-shot), respectively.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"5.2\">RQ2: Effectiveness of Models using PEFT Techniques</ns0:head><ns0:p>We report the detailed results of the effectiveness of the SLMs and LLMs on match-based code generation for both Conala and CodeAlpacaPy datasets in Table <ns0:ref type=\"table\" target=\"#tab_3\">3</ns0:ref>.</ns0:p><ns0:p>SLMs vs. LLMs. CodeGen-350M-mono with LoRA demonstrates the best effectiveness on average among small models, while CodeLlama-7B-Python with LoRA is the best LLM on average. Under the same 24GB GPU memory limitation, the best LLM surpasses the best small model by 39.8%, 41.7%, and 47.1% (72.3%, 48.8%, and 9.1%) in EM@1, EM@10, and CodeBLEU concerning the Conala (CodeAlpacaPy) dataset, respectively.</ns0:p><ns0:p>SLMs. Among the SLMs, CodeGen-350M-mono shows the highest effectiveness across all metrics on both datasets. Our results align with prior studies <ns0:ref type=\"bibr\" target=\"#b48\">[48,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b73\">73,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref> that identified CodeGen-350M-mono as a robust SLM for Python code generation tasks. Interestingly, although it requires tuning approximately 1% of the total parameters of the model, LoRA appears as the best tuning technique, surpassing full fine-tuning by a considerable margin across nearly all configurations. For instance, the EM@10 score for CodeGen-350M-mono on the Conala dataset, with full fine-tuning, is 18.42, while it soars to 25.60 with LoRA.</ns0:p><ns0:p>LLMs. In Figure <ns0:ref type=\"figure\">4</ns0:ref>, we present a comparative analysis of the models' effectiveness when tuned using LoRA, focusing on CodeBLEU and EM@10 scores. Both plots clearly establish CodeLlama models as the best-performing LLMs in our study. Remarkably, CodeGen2-7B, despite sharing a similar number of parameters, lags behind all CodeLlama-7B variants. Unsurprisingly, harnessing larger models leads to better effectiveness. Given the low computational costs of PEFT techniques, leveraging smaller models in a context akin to ours seems counterproductive. Subsequently, in this paper, we demonstrate that even larger models can be fine-tuned through the combination of PEFT with quantization.</ns0:p><ns0:p>Best PEFT technique. Overall, LoRA emerges as the most effective PEFT technique among the studied ones. Although being presented as an incremental improvement over LoRA <ns0:ref type=\"bibr\" target=\"#b37\">[37]</ns0:ref>, IA3 often shows lower scores compared to LoRA. Prompt tuning appears as another viable tuning option, while further reducing the number of trainable parameters. However, Prefix tuning fails to effectively adapt the larger models to both datasets.</ns0:p><ns0:p>Our analysis reveals notably higher EM scores for the Conala dataset, which can be attributed to differences in task complexity between the two datasets (see Section 4.2). It is important to note that CodeBLEU scores on Conala are comparatively lower due to the metric's reliance on dataflow graph computations, which may not always be available for small code examples.  Effect of quantization with QLoRA. We explore the potential benefits of employing QLoRA <ns0:ref type=\"bibr\" target=\"#b13\">[13]</ns0:ref>, a computationally efficient technique that combines LoRA with 8-bit or 4-bit quantization for fine-tuning LLMs. In Figure <ns0:ref type=\"figure\">5</ns0:ref>, we display EM@10 scores for three CodeLlama model variants: CodeLlama-7B-Python, CodeLlama-13B-Python, and CodeLlama-34B-Python, alongside peak GPU memory consumption consistently below 24GB for each tuning configuration. The results underscore a significant improvement in the effectiveness of larger quantized models on Conala, with a more moderate impact on CodeAlpacaPy. For instance, CodeLlama-34B-Python, fine-tuned with QLoRA-4bit, achieves a substantial 12.2% increase in Conala's EM@10 score (40.70) compared to CodeLlama-7B-Python with LoRA <ns0:ref type=\"bibr\">(36.28)</ns0:ref>. Surprisingly, QLoRA also brings notable improvements over LoRA for CodeLlama-7B-Python on Conala, while achieving comparable results on CodeAlpacaPy. The application of quantization enables the utilization of larger models that can be accommodated within a single 24GB GPU. Specifically, for CodeLlama-7B-Python, QLoRA-4bit achieves a remarkable 2x reduction in peak memory usage while significantly improving the EM@10 score.</ns0:p><ns0:p>Answer to RQ2: LLMs with PEFT consistently and significantly outperform SLMs under the same GPU limit. Specifically, the best-performing LLM with PEFT surpasses the best small model by 39.8-72.3% in terms of EM@𝑘. Among different PEFT techniques, LoRA is the most effective. In addition, applying quantization with LoRA results in a drastic decrease in GPU usage while maintaining effectiveness on both datasets and accommodating the fine-tuning of larger models up to 34B parameters.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"5.3\">RQ3: Comparative Analysis of LoRA, ICL, and RAG</ns0:head><ns0:p>In this RQ, we aim to investigate whether PEFT techniques consistently outperform the widely used ICL and RAG when applying LLMs in match-based code generation.</ns0:p><ns0:p>In Figure <ns0:ref type=\"figure\">6</ns0:ref>, we compare the effectiveness of the SLMs and LLMs using ICL and LoRA in terms of CodeBLEU and EM@10. In this figure, we report the highest metrics achieved over the different ICL</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head>CL-7B</ns0:head><ns0:p>CL-13B CL-34B 30 configurations for each model. In Figure <ns0:ref type=\"figure\" target=\"#fig_6\">7</ns0:ref>, we explore the effectiveness of CodeLlama models using RAG, with up to 16 and 4 retrieved examples for Conala and CodeAlpacaPy, respectively. Similar to RQ1, we use fewer examples for CodeAlpacaPy to avoid out-of-memory errors. We compare the effectiveness of RAG with LoRA and the best EM@10 score achieved using ICL.</ns0:p><ns0:p>LoRA vs. ICL. As shown in Fig. <ns0:ref type=\"figure\">6</ns0:ref>, all models fine-tuned with LoRA demonstrate significantly higher EM@10 scores compared to ICL across both datasets. For example, CodeLlama-7B-Python with LoRA tuning achieves a 23.1% improvement in EM@10 on Conala (36.28 for LoRA vs. 29.47 for ICL). This pattern holds for CodeAlpacaPy, with even greater relative gains in EM@10. However, we observe some variation in CodeBLEU scores for most models on CodeAlpacaPy. For instance, CodeLlama-7B sees a CodeBLEU increase of 2.36 with LoRA. On CoNala, though, the impact of LoRA on CodeBLEU is less pronounced than that of ICL. These differences can be explained by the nature of the metrics: EM@10 is more conservative, requiring the generated solution to exactly match the ground truth, while CodeBLEU gives higher scores for solutions that are close but not exact. This distinction highlights how LoRA better adapts models to downstream datasets, particularly when precision is crucial.</ns0:p><ns0:p>RAG vs. ICL vs. LoRA. In comparing RAG, ICL, and LoRA on the CoNala dataset, RAG demonstrates higher effectiveness than ICL but falls short of LoRA's effectiveness across all three CodeLlama model variants. Notably, CodeLlama-7B achieves a maximum of 29.83 and 35.17 EM@10 with ICL and RAG, respectively, whereas the model tuned with LoRA reaches an EM@10 of 39.31.</ns0:p><ns0:p>For both Conala and CodeAlpacaPy datasets, the gains in EM@10 get thinner as we increase the number of examples using RAG. EM@10 saturates at around 8-16 examples for Conala and 3-4 examples for CodeAlpacaPy. Furthermore, we note that for the more challenging CodeAlpacaPy datasets, RAG yields lower EM@10 compared to randomly selected examples using ICL, highlighting RAG's limitations when problem complexity increases. LoRA, however, consistently outperforms both RAG and ICL on CodeAlpacaPy, highlighting its superior ability to adapt to more challenging datasets.</ns0:p><ns0:p>Answer to RQ3: LoRA is superior to ICL and RAG on Conala and CodeAlpacaPy datasets across the three CodeLlama-7B variants. Table <ns0:ref type=\"table\">4</ns0:ref>. [RQ4] -Effectiveness of CodeLlama-7B-Instruct on the APPs dataset in zero-shot and using LoRA, QLoRA-8bit, and QLoRA-4bit in terms of average passed tests (Avg) and Pass@k (P@k).</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head>Introductory</ns0:head><ns0:p>Interview Competition</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head>Model</ns0:head><ns0:p>Avg P@1 P@2 P@5 Avg P@1 P@2 P@5 Avg P@1 P@2 P@5 In this final RQ, we explore the broader applicability of LoRA and QLoRA, to enhance CodeLlama-7B-Instruct's effectiveness for execution-based code generation. The reason for choosing the instruct variant of CodeLlama-7B is because the model generally shows higher effectiveness than the other model variants on APPs in the seminal paper of CodeLlama <ns0:ref type=\"bibr\" target=\"#b56\">[56]</ns0:ref>. We do not compare LoRA and QLoRA with ICL and RAG for this dataset because they require increasing the prompt length beyond 2,048 tokens, which leads to out-of-memory errors. Our results, summarized in Table <ns0:ref type=\"table\">4</ns0:ref>, focus on the average number of test cases passed (Avg) and Pass@𝑘 for introductory, interview, and competition-level tasks.</ns0:p><ns0:p>For both introductory and interview-level code generation tasks, LoRA and QLoRA-8/4bit lead to significant improvements in the average number of passed test cases. Specifically, QLoRA-4bit results in a notable 52% increase in the average number of tests passed compared to the base model. In terms of Pass@𝑘 metrics, both LoRA and QLoRA-4bit demonstrate gains at the introductory level, with Pass@5 improving by +3.60% over the base model. However, these improvements are less substantial for interview and competition-level code generation, reflecting the greater complexity and challenge posed by these more advanced tasks.</ns0:p><ns0:p>Answer to RQ4: LoRA and QLoRA enhance CodeLlama-7B-Instruct's effectiveness on APPs, particularly at the introductory, with QLoRA-4bit boosting the average number of passed test cases by 52% and Pass@5 by 40%. However, improvements are less notable for interview and competition-level tasks.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"6\">DISCUSSION</ns0:head><ns0:p>Our study explores PEFTs applied to code LLMs, elucidating the positive impact of these applications in efficiently tuning LLMs to task-specific datasets for code generation. In particular, our study illustrates the practicality of fine-tuning LLMs using PEFT, thereby alleviating the dependence of practitioners on large and expensive infrastructures. Our findings also pinpoint several promising areas for future exploration, including the investigation of efficient techniques across diverse fine-tuning settings, during inference, and for other SE tasks.</ns0:p><ns0:p>Efficient techniques for LLMs of code. Our work emphasizes efficient fine-tuning techniques, democratizing the tuning of LLMs to a broad audience. Nonetheless, our study did not include the exploration of efficient techniques for low-cost inference. While PEFT techniques require additional fine-tuning time compared to ICL and RAG, it is noteworthy that these techniques do not impose any supplementary time cost during inference. Nonetheless, we acknowledge the necessity of future investigations into techniques to reduce the time cost associated with LLMs during inference.</ns0:p><ns0:p>PEFT and ICL/RAG are non-exclusive techniques that can be used jointly. However, we decided not to include experiments on the application of ICL/RAG to LLMs fine-tuned using PEFT. In practice, increasing the number of ICL/RAG examples at inference entails increased computational overhead as the token length of the prompt expands. Consequently, we contend that employing ICL/RAG on a fine-tuned LLM might counterproductively escalate computational demands, outweighing potential benefits.</ns0:p><ns0:p>From a different angle, prior studies <ns0:ref type=\"bibr\" target=\"#b18\">[18,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b78\">78,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b83\">83]</ns0:ref> highlighted the need to consider pre-trained language models and LLMs of code in continual learning settings. In this paradigm, the model must dynamically adapt to new data over time while preserving performance on previously seen data. In the specific setting of continuously evolving LLMs, PEFT techniques potentially offer valuable benefits. Nonetheless, it is yet to be determined whether PEFT techniques can efficiently adapt LLMs under a continual learning setting for code-related tasks, without compromising the retention of past knowledge.</ns0:p><ns0:p>Effectiveness of QLoRA. Across all study datasets, we observed that QLoRA-4bit demonstrated competitive or comparable effectiveness to other PEFT methods. Notably, QLoRA-4bit outperformed LoRA and QLoRA-8bit on the Conala and APPs datasets. We hypothesize that this improvement stems from the regularization effect of reducing weight precision to 4 bits, which helps stabilize fine-tuning and mitigates overfitting. These findings highlight the potential for more efficient PEFT techniques, though further exploration is needed to fully understand their broader applicability.</ns0:p><ns0:p>New findings for PEFT in software engineering. Our findings in RQ1 reveal that PEFT methods outperform full fine-tuning for SLMs in code generation tasks. This stands in contrast to prior large-scale studies in NLP, such as Ding et al. <ns0:ref type=\"bibr\" target=\"#b14\">[14]</ns0:ref>, which demonstrated the superior effectiveness of full fine-tuning over techniques like LoRA, Prompt Tuning, and Prefix Tuning across a wide range of NLP tasks.</ns0:p><ns0:p>In the context of software engineering, while previous studies <ns0:ref type=\"bibr\" target=\"#b38\">[38,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b40\">40]</ns0:ref> have shown that PEFT methods, like LoRA, can perform comparably to full fine-tuning for SLMs, our results go further. We show that all PEFT techniques studied in this paper significantly outperform full fine-tuning for SLMs like CodeGen-350M-mono and CodeT5+-770M on the Conala and CodeAlpacaPy datasets (see Table <ns0:ref type=\"table\" target=\"#tab_3\">3</ns0:ref>), highlighting the clear advantages of PEFT in these scenarios. However, due to resource constraints, we were unable to evaluate full fine-tuning for LLMs, leaving room for future studies to explore this further in the software engineering domain.</ns0:p><ns0:p>Additionally, our research uncovers new insights into the benefits of QLoRA and the comparative effectiveness of LoRA versus RAG for code generation tasks. First, in RQ3 and RQ4, we demonstrate that QLoRA offers comparable or even superior performance to LoRA while drastically cutting computational costs. Second, we reveal limitations of ICL and RAG, showing that LLM effectiveness tends to plateau as more examples are retrieved. In contrast, our study highlights the consistent advantages of PEFT techniques like LoRA and QLoRA in overcoming these limitations.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head>SE tasks and multi-tasking.</ns0:head><ns0:p>To ensure a focused study, we avoided adding extra tasks and datasets, preventing an excessively broad set of analyses. Exploring PEFT techniques for LLMs across varied tasks and datasets is a promising direction for future research. In particular, Lorahub <ns0:ref type=\"bibr\" target=\"#b26\">[26]</ns0:ref>, a recently introduced framework for multi-task learning, demonstrates that a composition of LoRA modules trained on different tasks can generalize to new, unseen tasks while offering a strong performance-efficiency trade-off. We believe applying similar approaches in AI for SE holds great potential, particularly as the research field aims at automating a broad range of code-related tasks.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"7\">THREATS TO VALIDITY</ns0:head><ns0:p>External validity. One main threat relates to the choice of our SLMs and LLMs. We mitigated this threat by carefully selecting a diverse set of models, as explained in Section 4.4. These models encompass various families of LLMs, trained on distinct pre-training data and learning objectives, and varying in size. Furthermore, we did not select larger model variants except when using QLoRA, as other PEFT techniques, ICL, and RAG limit the use of larger models within our resource constraints.</ns0:p><ns0:p>Another external threat to the validity is related to the quality and representativeness of the finetuning datasets. To alleviate this concern, we chose the Conala dataset, which contains high-quality examples mined from StackOverflow posts. Additionally, this dataset has been representatively used by multiple prior studies <ns0:ref type=\"bibr\" target=\"#b49\">[49,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b73\">73,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref> on code generation tasks. Furthermore, the authors enriched each natural language intent with hints, enhancing the alignment of input prompts with possible human intents. To enrich our study, we included CodeAlpacaPy as a second dataset which encompasses lengthier examples, bringing another line of analysis. We did not include evaluation datasets such as HumanEval <ns0:ref type=\"bibr\" target=\"#b9\">[9]</ns0:ref> and MBPP <ns0:ref type=\"bibr\" target=\"#b3\">[3]</ns0:ref>, as they do not include training examples. However, to further expand our study, we explored the effectiveness of LoRA and QLoRA for execution-based code generation on the APPs dataset.</ns0:p><ns0:p>Finally, the monolingual aspect of our datasets constitutes another threat to external validity. We studied full fine-tuning, PEFT, ICL, and RAG for code generation of Python code snippets. However, we anticipate that PEFT is also applicable to other programming languages, considering the impressive generation capabilities of LLMs on a diverse range of programming languages <ns0:ref type=\"bibr\" target=\"#b2\">[2,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b6\">6]</ns0:ref>.</ns0:p><ns0:p>Internal validity. The hyperparameter choices for the PEFT methods constitute the main threat to internal validity. For each PEFT technique, we used hyperparameters values which have been used in previous work on PEFT for code models as well as in the seminal papers that contributed the PEFT techniques. Additionally, since LoRA with 𝑟 = 16 and 𝛼 = 32 consistently outperforms all configurations of ICL and RAG across our top three models, conducting a detailed hyperparameter sensitivity analysis of LoRA could further solidify the advantage of PEFT over ICL and RAG. Future work could explore the sensitivity of key LoRA hyperparameters, such as rank 𝑟 and scaling factor 𝛼, across a broader range of software engineering tasks.</ns0:p><ns0:p>Construct validity. The choice of our evaluation metrics constitutes the main threat to construct validity. To mitigate this threat, we selected evaluation metrics widely used in prior works <ns0:ref type=\"bibr\" target=\"#b22\">[22,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b32\">32,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b42\">42,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b56\">56,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b72\">72,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b84\">84]</ns0:ref> on code generation. Furthermore, we evaluate each approach using EM@𝑘 on Conala and CodeAlpacaPy, which enriched our analysis by computing the exact match over different ranges of code candidates. Similarly, for APPs, we evaluate the base model and LoRA/QLoRA on Pass@𝑘 with up to 5 candidates. Finally, we did not use Pass@𝑘 metrics as the CoNaLa and CodeAlpacaPy datasets do not include unit tests. Enriching the datasets with unit tests constitutes an interesting area of future work.</ns0:p><ns0:p>In this section, we overview existing work on LLMs for code generation and contrast previous contributions on efficient model adaptation of code for downstream tasks with our study. Automated Code Generation. A significant portion of code generation techniques <ns0:ref type=\"bibr\" target=\"#b1\">[1,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b4\">4,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b21\">21,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b63\">63,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b72\">72]</ns0:ref> relies on deep-learning-based approaches. The latest trend in automated code generation revolves around leveraging LLMs like GPT models <ns0:ref type=\"bibr\" target=\"#b50\">[50]</ns0:ref> due to their remarkable breakthroughs in this domain. One notable example is Codex, developed by Chen et al. <ns0:ref type=\"bibr\" target=\"#b9\">[9]</ns0:ref>, which is a fine-tuned version of GPT-3. Other noteworthy models following the success of Codex include CodeGen <ns0:ref type=\"bibr\" target=\"#b48\">[48]</ns0:ref>, CodeGen2 <ns0:ref type=\"bibr\" target=\"#b47\">[47]</ns0:ref> and CodeLlama <ns0:ref type=\"bibr\" target=\"#b56\">[56]</ns0:ref>. These LLMs effectively democratize the breakthrough performance achieved by Codex and bring it to a broader audience. However, the high computational costs associated with full finetuning for LLMs to achieve optimal performance are impractical for most researchers and practitioners. We believe that our study can shed light on more efficient and cost-effective approaches to fine-tuning these LLMs, mitigating the computational burdens associated with their adoption.</ns0:p><ns0:p>Efficient Adaptation of Models of Code. Efficient adaptation of models of code involves the utilization of techniques to efficiently adapt a model to a task-specific dataset (see Section 2). In this context, the term \"efficient\" refers to rendering the fine-tuning computation costs low, e.g, using LoRA, or utilizing parameter-free techniques such as prompting and ICL.</ns0:p><ns0:p>Most prior research has concentrated on employing ICL and prompting to adapt models to diverse code-related tasks. Gao et al. <ns0:ref type=\"bibr\" target=\"#b17\">[17]</ns0:ref> showcased the advantages of ICL in tasks like bug fixing, code summarization, and program synthesis. They highlighted that the model's performance on downstream tasks is influenced by multiple factors, including the selection, quantity, and order of prompt examples. Other studies <ns0:ref type=\"bibr\" target=\"#b53\">[53,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b80\">80]</ns0:ref> also demonstrated that pre-trained language models and LLMs like Codex can effectively handle bug fixing and automated program repair using ICL. Moreover, Geng et al. <ns0:ref type=\"bibr\" target=\"#b19\">[19]</ns0:ref> demonstrated the capability of Codex to generate multi-intent comment generation to describe the functionality of a method or its implementation details, for instance. The selection of relevant prompts for a task with ICL is crucial to ensure the good performance of an LLM. Prior works <ns0:ref type=\"bibr\" target=\"#b46\">[46,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b91\">91]</ns0:ref> designed selection techniques to retrieve highly relevant prompt examples tailored to downstream tasks, outperforming random selection methods. Lastly, recent research <ns0:ref type=\"bibr\" target=\"#b61\">[61]</ns0:ref> highlighted the advantages of retrieving prompt examples at the repository level, providing LLMs with valuable contextual information in the prompts. In this study, we leveraged ICL without the intention of fully exploring its potential. Instead, we opted for a simple implementation of ICL by selecting random few-shot examples using different seeds. Expanding this study to incorporate more ICL approaches would enhance the comparison with PEFT techniques for code.</ns0:p><ns0:p>Regarding PEFT techniques, prior research in code intelligence has focused on Prompt tuning <ns0:ref type=\"bibr\" target=\"#b30\">[30]</ns0:ref>, Prefix-tuning <ns0:ref type=\"bibr\" target=\"#b33\">[33]</ns0:ref> and Adapters <ns0:ref type=\"bibr\" target=\"#b20\">[20,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b23\">23,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b25\">25,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b57\">57,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b58\">58]</ns0:ref>. Wang et al. <ns0:ref type=\"bibr\" target=\"#b68\">[68]</ns0:ref> initiated the usage of Prompt tuning for code-related tasks and demonstrated its superiority over full fine-tuning of CodeT5 and CodeBERT in defect prediction, code summarization, and code translation. Goel et al. <ns0:ref type=\"bibr\" target=\"#b20\">[20]</ns0:ref> explored the use of programming-language-specific adapters for knowledge transfer in pre-trained language models, demonstrating that tuning BERT with these adapters surpass CodeBERT on cloze test and code clone detection. Choi et al. <ns0:ref type=\"bibr\" target=\"#b10\">[10]</ns0:ref> designed a code-specific Prefix tuning approach within a sequence-tosequence architecture for generation tasks. Our study differs from these three previous works as they focus on SLMs, whereas we propose the first comprehensive study of PEFT techniques with LLMs for code generation. Moreover, our study includes LoRA, IA3, and QLoRA, which none of the previous work in code intelligence considered for efficiently tuning LLMs of code. Wang et al. <ns0:ref type=\"bibr\" target=\"#b69\">[69]</ns0:ref> showcased the superiority of utilizing Adapters for fine-tuning pre-trained language models over full fine-tuning. Recent work have contributed empirical studies for various software engineering tasks, including code change <ns0:ref type=\"bibr\" target=\"#b40\">[40]</ns0:ref>, code summarization <ns0:ref type=\"bibr\" target=\"#b38\">[38,</ns0:ref><ns0:ref type=\"bibr\" target=\"#b57\">57]</ns0:ref>, defect prediction <ns0:ref type=\"bibr\" target=\"#b38\">[38]</ns0:ref>, and code clone detection <ns0:ref type=\"bibr\" target=\"#b57\">[57]</ns0:ref>, using Adapter tuning and LoRA for SLMs. Our research diverges from these prior work, as we concentrate on LLMs. Although we did not incorporate Adapters in our investigation, we believe that LoRA, IA3, Prompt tuning, Prefix tuning, and QLoRA provide a sufficiently thorough analysis of PEFT techniques. We recognize the value of exploring additional PEFT techniques for various code intelligence tasks in the future.</ns0:p></ns0:div>\n",
      "<ns0:div><ns0:head n=\"9\">CONCLUSION AND FUTURE WORK</ns0:head><ns0:p>This study establishes the effectiveness of PEFT techniques in fine-tuning LLMs for code generation. Our comparative analysis across various parameter-efficient techniques, including LoRA, IA3, Prompt tuning, Prefix tuning, and QLoRA, reveals the superiority of PEFT over full fine-tuning for SLMs and ICL and RAG for LLMs. Furthermore, our study illustrates the practicality of PEFT under a limited resources scenario, effectively mitigating the reliance on large and expensive computational infrastructures. To the best of our knowledge, this study is among the first comprehensive exploration of PEFT techniques for LLMs in software engineering, suggesting a promising avenue for future research. We anticipate our findings will inspire further investigation into the application of PEFT techniques in software engineering, with potentially far-reaching impacts. Our future work will extend the study to alternative software engineering tasks such as automated code review and comment generation. Finally, we aim to validate further relevance of PEFT techniques under multi-tasking and continual learning settings for automated software engineering.</ns0:p></ns0:div><ns0:figure xml:id=\"fig_0\"><ns0:head>Fig. 1 .</ns0:head><ns0:label>1</ns0:label><ns0:figDesc>Fig.1. Peak GPU memory consumption during models fine-tuning using full fine-tuning (ft), LoRA, and QLoRA.</ns0:figDesc></ns0:figure>\n",
      "<ns0:figure xml:id=\"fig_1\"><ns0:head>Fig. 2 .</ns0:head><ns0:label>2</ns0:label><ns0:figDesc>Fig. 2. Token length distribution of the Conala, CodeAlpacaPy, and APPS datasets.</ns0:figDesc></ns0:figure>\n",
      "<ns0:figure xml:id=\"fig_2\"><ns0:head>Fig. 3 .</ns0:head><ns0:label>3</ns0:label><ns0:figDesc>Fig. 3. [RQ1] -Effectiveness of the models using ICL with various number of random examples on the Conala and CodeAlpacaPy datasets.</ns0:figDesc></ns0:figure>\n",
      "<ns0:figure xml:id=\"fig_3\"><ns0:head /><ns0:label /><ns0:figDesc>: Baseline Effectiveness of Models Using Zero-Shot and ICL We start by investigating the baseline effectiveness of all SLMs and LLMs for match-based code generation. Specifically, we use zero-shot and ICL approaches with up to 16 retrieved random examples for the Conala dataset and eight for the CodeAlpacaPy dataset. The reason behind utilizing fewer examples for CodeAlpacaPy is because considering 16 examples results in out-of-memory errors under our setup.</ns0:figDesc></ns0:figure>\n",
      "<ns0:figure xml:id=\"fig_4\"><ns0:head /><ns0:label /><ns0:figDesc>Fig.4. [RQ2] -Effectiveness of the models fine-tuned using LoRA for both datasets in terms of EM@10 and CodeBLEU.</ns0:figDesc></ns0:figure>\n",
      "<ns0:figure xml:id=\"fig_5\"><ns0:head>Fig. 5 .Fig. 6 .</ns0:head><ns0:label>56</ns0:label><ns0:figDesc>Fig. 5. [RQ2] -Effectiveness and GPU usage of 7B, 13B, and 34B CodeLlama-Python (CL) LLMs fine-tuned using LoRA and QLoRA with 8-bit and 4-bit quantization.</ns0:figDesc></ns0:figure>\n",
      "<ns0:figure xml:id=\"fig_6\"><ns0:head>Fig. 7 .</ns0:head><ns0:label>7</ns0:label><ns0:figDesc>Fig. 7. [RQ3] -Comparison of the effectiveness of RAG with various number of retrieved examples against ICL and LoRA on the Conala (top) and CodeAlpacaPy (bottom) datasets. The ICL scores depict the highest scores achieved for each model in RQ1.</ns0:figDesc></ns0:figure>\n",
      "<ns0:figure type=\"table\" xml:id=\"tab_0\"><ns0:head>Table 1 .</ns0:head><ns0:label>1</ns0:label><ns0:figDesc>Computation-effectiveness trade-off for each model tuning technique.</ns0:figDesc><ns0:table><ns0:row><ns0:cell>Technique</ns0:cell><ns0:cell cols=\"2\">Computation costs</ns0:cell><ns0:cell cols=\"2\">Effectiveness</ns0:cell></ns0:row><ns0:row><ns0:cell>Full fine-tuning</ns0:cell><ns0:cell /><ns0:cell>high [X]</ns0:cell><ns0:cell cols=\"2\">high [✓]</ns0:cell></ns0:row><ns0:row><ns0:cell>ICL and RAG</ns0:cell><ns0:cell /><ns0:cell>low [✓]</ns0:cell><ns0:cell cols=\"2\">low [X]</ns0:cell></ns0:row><ns0:row><ns0:cell>PEFT</ns0:cell><ns0:cell /><ns0:cell>low [✓]</ns0:cell><ns0:cell cols=\"2\">high [✓]</ns0:cell></ns0:row><ns0:row><ns0:cell>CodeT5+-220M-ft</ns0:cell><ns0:cell>3.54</ns0:cell><ns0:cell /><ns0:cell /><ns0:cell /></ns0:row><ns0:row><ns0:cell>CodeGen-350M-mono-ft</ns0:cell><ns0:cell /><ns0:cell>5.84</ns0:cell><ns0:cell /><ns0:cell /></ns0:row><ns0:row><ns0:cell>CodeT5+-770M-ft</ns0:cell><ns0:cell /><ns0:cell>8.16</ns0:cell><ns0:cell /><ns0:cell /></ns0:row><ns0:row><ns0:cell>CodeLlama-7B-QLoRA-4bit</ns0:cell><ns0:cell /><ns0:cell>9.16</ns0:cell><ns0:cell /><ns0:cell /></ns0:row><ns0:row><ns0:cell>CodeGen2-1B-lora</ns0:cell><ns0:cell /><ns0:cell>9.8</ns0:cell><ns0:cell /><ns0:cell /></ns0:row><ns0:row><ns0:cell>CodeGen2-3.7B-lora</ns0:cell><ns0:cell /><ns0:cell /><ns0:cell>14.08</ns0:cell><ns0:cell /></ns0:row><ns0:row><ns0:cell>CodeLlama-13B-QLoRA-4bit</ns0:cell><ns0:cell /><ns0:cell /><ns0:cell>15.01</ns0:cell><ns0:cell /></ns0:row><ns0:row><ns0:cell>CodeLlama-7B-lora</ns0:cell><ns0:cell /><ns0:cell /><ns0:cell /><ns0:cell>19.06</ns0:cell></ns0:row><ns0:row><ns0:cell>CodeGen2-7B-lora</ns0:cell><ns0:cell /><ns0:cell /><ns0:cell /><ns0:cell>20.29</ns0:cell></ns0:row><ns0:row><ns0:cell>CodeLlama-34B-QLoRA-4bit</ns0:cell><ns0:cell /><ns0:cell /><ns0:cell /><ns0:cell /><ns0:cell>23.59</ns0:cell></ns0:row><ns0:row><ns0:cell>0</ns0:cell><ns0:cell>5</ns0:cell><ns0:cell cols=\"2\">10 Peak memory consumption (GB) 15</ns0:cell><ns0:cell>20</ns0:cell><ns0:cell>24</ns0:cell></ns0:row></ns0:table></ns0:figure>\n",
      "<ns0:figure type=\"table\" xml:id=\"tab_1\"><ns0:head /><ns0:label /><ns0:figDesc>We filter out code samples that cannot be statically parsed to ensure the dataset encompasses only syntactically valid Python codes. As illustrated in the bottom example of Table2and in Figure2, CodeAlpacaPy contains lengthier and more complex examples than Conala, allowing for a more comprehensive evaluation of PEFT for code generation. The dataset contains 2,192/314/628 samples as the training/validation/test sets, respectively.</ns0:figDesc><ns0:table /><ns0:note><ns0:p><ns0:p><ns0:p><ns0:p>In this curated version of the dataset, the authors ensured that each sample in the validation and test sets contained at least one Python function that does not appear in the training set. Additionally, they ensured that examples crawled from the same StackOverflow post appear in different sets. Thus, we can guarantee that each natural intent in the test does not appear in the training set. The dataset contains 2,135/201/543 samples as the training/validation/test sets, respectively.</ns0:p>CodeAlpacaPy dataset. We construct a curated Python version of the CodeAlpaca</ns0:p><ns0:ref type=\"bibr\" target=\"#b8\">[8]</ns0:ref> </ns0:p>dataset by specifically selecting the Python data samples within the CodeAlpaca dataset.</ns0:p></ns0:note></ns0:figure>\n",
      "<ns0:figure type=\"table\" xml:id=\"tab_2\"><ns0:head>Table 2 .</ns0:head><ns0:label>2</ns0:label><ns0:figDesc>Overview of the code generation task, with three examples taken from the Conala, CodeAlpacaPy, and APPS datasets.</ns0:figDesc><ns0:table><ns0:row><ns0:cell>Conala</ns0:cell></ns0:row></ns0:table></ns0:figure>\n",
      "<ns0:figure type=\"table\" xml:id=\"tab_3\"><ns0:head>Table 3 .</ns0:head><ns0:label>3</ns0:label><ns0:figDesc>[RQ2] -Comparison of the SLMs and LLMs using various tuning techniques ( blue : best-performing tuning method per model, orange : best overall performing model).Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models • 15</ns0:figDesc><ns0:table><ns0:row><ns0:cell>Conala</ns0:cell><ns0:cell>CodeAlpacaPy</ns0:cell></ns0:row></ns0:table><ns0:note><ns0:p>, Vol. 1, No. 1, Article . Publication date: December 2024.</ns0:p></ns0:note></ns0:figure>\n",
      "\t\t\t<ns0:note place=\"foot\" xml:id=\"foot_0\"><ns0:p>, Vol. 1, No. 1, Article . Publication date: December 2024.</ns0:p></ns0:note>\n",
      "\t\t\t<ns0:note place=\"foot\" xml:id=\"foot_1\"><ns0:p>, Vol. 1, No. 1, Article . Publication date: December 2024. Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models • 17</ns0:p></ns0:note>\n",
      "\t\t</ns0:body>\n",
      "\t\t \n",
      "\n",
      "References:\n",
      " <ns0:listBibl xmlns:ns0=\"http://www.tei-c.org/ns/1.0\">\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b0\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">CodeT5+-220M CodeT5+-770M CodeGen-350M-mono CodeGen2-1B CodeGen2-.7B CodeGen2-7B CodeLlama-7B CodeLlama-7B-Instruct CodeLlama-7B-Python REFERENCES</ns0:title>\n",
      "\t\t<ns0:imprint />\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b1\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Structural language models of code</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Uri</ns0:forename><ns0:surname>Alon</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Roy</ns0:forename><ns0:surname>Sadaka</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Omer</ns0:forename><ns0:surname>Levy</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Eran</ns0:forename><ns0:surname>Yahav</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">International conference on machine learning</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>PMLR</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2020\">2020</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"245\" to=\"256\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b2\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Multi-lingual evaluation of code generation models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ben</ns0:forename><ns0:surname>Athiwaratkun</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Krishna</ns0:forename><ns0:surname>Sanjay</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zijian</ns0:forename><ns0:surname>Gouda</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiaopeng</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yuchen</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ming</ns0:forename><ns0:surname>Tian</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Tan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Uddin</ns0:forename><ns0:surname>Wasi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shiqi</ns0:forename><ns0:surname>Ahmad</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qing</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mingyue</ns0:forename><ns0:surname>Sun</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Shang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2210.14868</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b3\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Program synthesis with large language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jacob</ns0:forename><ns0:surname>Austin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Augustus</ns0:forename><ns0:surname>Odena</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Maxwell</ns0:forename><ns0:surname>Nye</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Maarten</ns0:forename><ns0:surname>Bosma</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Henryk</ns0:forename><ns0:surname>Michalewski</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">David</ns0:forename><ns0:surname>Dohan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ellen</ns0:forename><ns0:surname>Jiang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Carrie</ns0:forename><ns0:surname>Cai</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Michael</ns0:forename><ns0:surname>Terry</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Quoc</ns0:forename><ns0:surname>Le</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2108.07732</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b4\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Deepcoder: Learning to write programs</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Matej</ns0:forename><ns0:surname>Balog</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Alexander</ns0:forename><ns0:forename type=\"middle\">L</ns0:forename><ns0:surname>Gaunt</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Marc</ns0:forename><ns0:surname>Brockschmidt</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sebastian</ns0:forename><ns0:surname>Nowozin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daniel</ns0:forename><ns0:surname>Tarlow</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:1611.01989</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2016\">2016. 2016</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b5\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Language models are few-shot learners</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tom</ns0:forename><ns0:surname>Brown</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Benjamin</ns0:forename><ns0:surname>Mann</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Nick</ns0:forename><ns0:surname>Ryder</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Melanie</ns0:forename><ns0:surname>Subbiah</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jared</ns0:forename><ns0:forename type=\"middle\">D</ns0:forename><ns0:surname>Kaplan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Prafulla</ns0:forename><ns0:surname>Dhariwal</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Arvind</ns0:forename><ns0:surname>Neelakantan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pranav</ns0:forename><ns0:surname>Shyam</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Girish</ns0:forename><ns0:surname>Sastry</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Amanda</ns0:forename><ns0:surname>Askell</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Advances in neural information processing systems</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">33</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"1877\" to=\"1901\" />\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2020\">2020. 2020</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b6\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Federico</ns0:forename><ns0:surname>Cassano</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">John</ns0:forename><ns0:surname>Gouwar</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daniel</ns0:forename><ns0:surname>Nguyen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sydney</ns0:forename><ns0:surname>Nguyen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Luna</ns0:forename><ns0:surname>Phipps-Costin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Donald</ns0:forename><ns0:surname>Pinckney</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ming-Ho</ns0:forename><ns0:surname>Yee</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yangtian</ns0:forename><ns0:surname>Zi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Carolyn</ns0:forename><ns0:forename type=\"middle\">Jane</ns0:forename><ns0:surname>Anderson</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Molly</ns0:forename><ns0:forename type=\"middle\">Q</ns0:forename><ns0:surname>Feldman</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">IEEE Transactions on Software Engineering</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b7\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Prompt-RSVQA: Prompting visual context to a language model for remote sensing visual question answering</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Christel</ns0:forename><ns0:surname>Chappuis</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Valérie</ns0:forename><ns0:surname>Zermatten</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sylvain</ns0:forename><ns0:surname>Lobry</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bertrand</ns0:forename><ns0:forename type=\"middle\">Le</ns0:forename><ns0:surname>Saux</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Devis</ns0:forename><ns0:surname>Tuia</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Models • 23 Conference on Computer Vision and Pattern Recognition</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022-12\">2022. December 2024</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">1</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"1372\" to=\"1381\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note>Proceedings of the IEEE/CVF</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b8\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Code Alpaca: An Instruction-following LLaMA model for code generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sahil</ns0:forename><ns0:surname>Chaudhary</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:ptr target=\"https://github.com/sahil280114/codealpaca\" />\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b9\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Evaluating large language models trained on code</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mark</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jerry</ns0:forename><ns0:surname>Tworek</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Heewoo</ns0:forename><ns0:surname>Jun</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qiming</ns0:forename><ns0:surname>Yuan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Henrique</ns0:forename><ns0:surname>Ponde De Oliveira Pinto</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jared</ns0:forename><ns0:surname>Kaplan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Harri</ns0:forename><ns0:surname>Edwards</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yuri</ns0:forename><ns0:surname>Burda</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Nicholas</ns0:forename><ns0:surname>Joseph</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Greg</ns0:forename><ns0:surname>Brockman</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2107.03374</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b10\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yunseok</ns0:forename><ns0:surname>Choi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jee-Hyong</ns0:forename><ns0:surname>Lee</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Findings of the Association for Computational Linguistics: ACL 2023</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"5282\" to=\"5297\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b11\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Palm: Scaling language modeling with pathways</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Aakanksha</ns0:forename><ns0:surname>Chowdhery</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sharan</ns0:forename><ns0:surname>Narang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jacob</ns0:forename><ns0:surname>Devlin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Maarten</ns0:forename><ns0:surname>Bosma</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Gaurav</ns0:forename><ns0:surname>Mishra</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Adam</ns0:forename><ns0:surname>Roberts</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Paul</ns0:forename><ns0:surname>Barham</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hyung</ns0:forename><ns0:forename type=\"middle\">Won</ns0:forename><ns0:surname>Chung</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Charles</ns0:forename><ns0:surname>Sutton</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sebastian</ns0:forename><ns0:surname>Gehrmann</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2204.02311</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b12\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tim</ns0:forename><ns0:surname>Dettmers</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mike</ns0:forename><ns0:surname>Lewis</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Younes</ns0:forename><ns0:surname>Belkada</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Luke</ns0:forename><ns0:surname>Zettlemoyer</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2208.07339</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b13\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Qlora: Efficient finetuning of quantized llms</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tim</ns0:forename><ns0:surname>Dettmers</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Artidoro</ns0:forename><ns0:surname>Pagnoni</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ari</ns0:forename><ns0:surname>Holtzman</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Luke</ns0:forename><ns0:surname>Zettlemoyer</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2305.14314</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b14\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ning</ns0:forename><ns0:surname>Ding</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yujia</ns0:forename><ns0:surname>Qin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Guang</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fuchao</ns0:forename><ns0:surname>Wei</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zonghan</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yusheng</ns0:forename><ns0:surname>Su</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shengding</ns0:forename><ns0:surname>Hu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yulin</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chi-Min</ns0:forename><ns0:surname>Chan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Weize</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2203.06904</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b15\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Parameter-efficient fine-tuning of large-scale pre-trained language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ning</ns0:forename><ns0:surname>Ding</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yujia</ns0:forename><ns0:surname>Qin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Guang</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fuchao</ns0:forename><ns0:surname>Wei</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zonghan</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yusheng</ns0:forename><ns0:surname>Su</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shengding</ns0:forename><ns0:surname>Hu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yulin</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chi-Min</ns0:forename><ns0:surname>Chan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Weize</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Nature Machine Intelligence</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">5</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"issue\">3</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"220\" to=\"235\" />\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b16\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Codebert: A pre-trained model for programming and natural languages</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhangyin</ns0:forename><ns0:surname>Feng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daya</ns0:forename><ns0:surname>Guo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Duyu</ns0:forename><ns0:surname>Tang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Nan</ns0:forename><ns0:surname>Duan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiaocheng</ns0:forename><ns0:surname>Feng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ming</ns0:forename><ns0:surname>Gong</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Linjun</ns0:forename><ns0:surname>Shou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bing</ns0:forename><ns0:surname>Qin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ting</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daxin</ns0:forename><ns0:surname>Jiang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2002.08155</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2020\">2020. 2020</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b17\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuzheng</ns0:forename><ns0:surname>Gao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin-Cheng</ns0:forename><ns0:surname>Wen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Cuiyun</ns0:forename><ns0:surname>Gao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Wenxuan</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Michael R Lyu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2304.07575</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b18\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuzheng</ns0:forename><ns0:surname>Gao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hongyu</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Cuiyun</ns0:forename><ns0:surname>Gao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chaozheng</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2302.03482</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b19\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mingyang</ns0:forename><ns0:surname>Geng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shangwen</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dezun</ns0:forename><ns0:surname>Dong</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Haotian</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ge</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhi</ns0:forename><ns0:surname>Jin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiaoguang</ns0:forename><ns0:surname>Mao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiangke</ns0:forename><ns0:surname>Liao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2024\">2024. 2024</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b20\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">On the cross-modal transfer from natural language to code through adapter modules</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Divyam</ns0:forename><ns0:surname>Goel</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ramansh</ns0:forename><ns0:surname>Grover</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fatemeh</ns0:forename><ns0:forename type=\"middle\">H</ns0:forename><ns0:surname>Fard</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension</ns0:title>\n",
      "\t\t<ns0:meeting>the 30th IEEE/ACM International Conference on Program Comprehension</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"71\" to=\"81\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b21\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Retrieval-based neural code generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Anugrah</ns0:forename><ns0:surname>Shirley</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Raphael</ns0:forename><ns0:surname>Hayati</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pravalika</ns0:forename><ns0:surname>Olivier</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pengcheng</ns0:forename><ns0:surname>Avvaru</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Anthony</ns0:forename><ns0:surname>Yin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Graham</ns0:forename><ns0:surname>Tomasic</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Neubig</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:1808.10025</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2018\">2018. 2018</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b22\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Measuring Coding Challenge Competence With APPS</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dan</ns0:forename><ns0:surname>Hendrycks</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Steven</ns0:forename><ns0:surname>Basart</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Saurav</ns0:forename><ns0:surname>Kadavath</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mantas</ns0:forename><ns0:surname>Mazeika</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Akul</ns0:forename><ns0:surname>Arora</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ethan</ns0:forename><ns0:surname>Guo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Collin</ns0:forename><ns0:surname>Burns</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Samir</ns0:forename><ns0:surname>Puranik</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Horace</ns0:forename><ns0:surname>He</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dawn</ns0:forename><ns0:surname>Song</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jacob</ns0:forename><ns0:surname>Steinhardt</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t\t<ns0:publisher>NeurIPS</ns0:publisher>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b23\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Parameter-efficient transfer learning for NLP</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Neil</ns0:forename><ns0:surname>Houlsby</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Andrei</ns0:forename><ns0:surname>Giurgiu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Stanislaw</ns0:forename><ns0:surname>Jastrzebski</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bruna</ns0:forename><ns0:surname>Morrone</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Quentin</ns0:forename><ns0:surname>De Laroussilhe</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Andrea</ns0:forename><ns0:surname>Gesmundo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mona</ns0:forename><ns0:surname>Attariyan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sylvain</ns0:forename><ns0:surname>Gelly</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">International Conference on Machine Learning</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>PMLR</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2019\">2019. 2790-2799</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b24\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Lora: Low-rank adaptation of large language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">J</ns0:forename><ns0:surname>Edward</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yelong</ns0:forename><ns0:surname>Hu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Phillip</ns0:forename><ns0:surname>Shen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zeyuan</ns0:forename><ns0:surname>Wallis</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yuanzhi</ns0:forename><ns0:surname>Allen-Zhu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shean</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lu</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Weizhu</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2106.09685</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b25\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhiqiang</ns0:forename><ns0:surname>Hu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yihuai</ns0:forename><ns0:surname>Lan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lei</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Wanyu</ns0:forename><ns0:surname>Xu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ee-Peng</ns0:forename><ns0:surname>Lim</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Roy</ns0:forename></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ka-Wei</ns0:forename><ns0:surname>Lee</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lidong</ns0:forename><ns0:surname>Bing</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Soujanya</ns0:forename><ns0:surname>Poria</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2304.01933</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b26\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Lorahub: Efficient cross-task generalization via dynamic lora composition</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chengsong</ns0:forename><ns0:surname>Huang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qian</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bill</ns0:forename><ns0:surname>Yuchen Lin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tianyu</ns0:forename><ns0:surname>Pang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chao</ns0:forename><ns0:surname>Du</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Min</ns0:forename><ns0:surname>Lin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2307.13269</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b27\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Repair is nearly generation: Multilingual program repair with llms</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Harshit</ns0:forename><ns0:surname>Joshi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">José Cambronero</ns0:forename><ns0:surname>Sanchez</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sumit</ns0:forename><ns0:surname>Gulwani</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Gust</ns0:forename><ns0:surname>Vu Le</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ivan</ns0:forename><ns0:surname>Verbruggen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Radiček</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the AAAI Conference on Artificial Intelligence</ns0:title>\n",
      "\t\t<ns0:meeting>the AAAI Conference on Artificial Intelligence</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">37</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"5131\" to=\"5140\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b28\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Denis</ns0:forename><ns0:surname>Kocetkov</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Raymond</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Loubna</ns0:forename><ns0:surname>Ben Allal</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jia</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chenghao</ns0:forename><ns0:surname>Mou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Carlos</ns0:forename><ns0:surname>Muñoz Ferrandis</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yacine</ns0:forename><ns0:surname>Jernite</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Margaret</ns0:forename><ns0:surname>Mitchell</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sean</ns0:forename><ns0:surname>Hughes</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Thomas</ns0:forename><ns0:surname>Wolf</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dzmitry</ns0:forename><ns0:surname>Bahdanau</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Leandro</ns0:forename><ns0:surname>Von Werra</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Harm</ns0:forename><ns0:surname>De Vries</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:title level=\"m\">The Stack: 3 TB of permissively licensed source code</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">Preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b29\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Large language models are zero-shot reasoners</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Takeshi</ns0:forename><ns0:surname>Kojima</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shane</ns0:forename><ns0:surname>Shixiang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Machel</ns0:forename><ns0:surname>Gu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yutaka</ns0:forename><ns0:surname>Reid</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yusuke</ns0:forename><ns0:surname>Matsuo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Iwasawa</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Advances in neural information processing systems</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">35</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"22199\" to=\"22213\" />\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b30\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">The Power of Scale for Parameter-Efficient Prompt Tuning</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Brian</ns0:forename><ns0:surname>Lester</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Rami</ns0:forename><ns0:surname>Al-Rfou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Noah</ns0:forename><ns0:surname>Constant</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</ns0:title>\n",
      "\t\t<ns0:meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"3045\" to=\"3059\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b31\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Retrieval-augmented generation for knowledge-intensive nlp tasks</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Patrick</ns0:forename><ns0:surname>Lewis</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ethan</ns0:forename><ns0:surname>Perez</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Aleksandra</ns0:forename><ns0:surname>Piktus</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fabio</ns0:forename><ns0:surname>Petroni</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Vladimir</ns0:forename><ns0:surname>Karpukhin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Naman</ns0:forename><ns0:surname>Goyal</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Heinrich</ns0:forename><ns0:surname>Küttler</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mike</ns0:forename><ns0:surname>Lewis</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Wen-Tau</ns0:forename><ns0:surname>Yih</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tim</ns0:forename><ns0:surname>Rocktäschel</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Advances in Neural Information Processing Systems</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">33</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"9459\" to=\"9474\" />\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2020\">2020. 2020</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b32\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Skcoder: A sketch-based approach for automatic code generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jia</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yongmin</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ge</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhi</ns0:forename><ns0:surname>Jin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yiyang</ns0:forename><ns0:surname>Hao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xing</ns0:forename><ns0:surname>Hu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2302.06144</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b33\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Prefix-tuning: Optimizing continuous prompts for generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lisa</ns0:forename><ns0:surname>Xiang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Percy</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Liang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2101.00190</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b34\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Towards General Text Embeddings with Multi-stage Contrastive Learning</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zehan</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yanzhao</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dingkun</ns0:forename><ns0:surname>Long</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pengjun</ns0:forename><ns0:surname>Xie</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Meishan</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2308.03281</ns0:idno>\n",
      "\t\t<ns0:ptr target=\"https://arxiv.org/abs/2308.03281\" />\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b35\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Holistic evaluation of language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Percy</ns0:forename><ns0:surname>Liang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Rishi</ns0:forename><ns0:surname>Bommasani</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tony</ns0:forename><ns0:surname>Lee</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dimitris</ns0:forename><ns0:surname>Tsipras</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dilara</ns0:forename><ns0:surname>Soylu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Michihiro</ns0:forename><ns0:surname>Yasunaga</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yian</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Deepak</ns0:forename><ns0:surname>Narayanan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yuhuai</ns0:forename><ns0:surname>Wu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ananya</ns0:forename><ns0:surname>Kumar</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2211.09110</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b36\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">M</ns0:forename><ns0:surname>Liang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Y</ns0:forename><ns0:surname>Yuksekgonul</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">E</ns0:forename><ns0:surname>Mao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Wu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Zou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2304.02819</ns0:idno>\n",
      "\t\t<ns0:title level=\"m\">GPT detectors are biased against non-native English writers</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b37\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Haokun</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Derek</ns0:forename><ns0:surname>Tam</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mohammed</ns0:forename><ns0:surname>Muqeeth</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jay</ns0:forename><ns0:surname>Mohta</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tenghao</ns0:forename><ns0:surname>Huang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mohit</ns0:forename><ns0:surname>Bansal</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Colin</ns0:forename><ns0:forename type=\"middle\">A</ns0:forename><ns0:surname>Raffel</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Advances in Neural Information Processing Systems</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">35</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"1950\" to=\"1965\" />\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b38\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jiaxing</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chaofeng</ns0:forename><ns0:surname>Sha</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin</ns0:forename><ns0:surname>Peng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">38th IEEE/ACM International Conference on Automated Software Engineering (ASE)</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>IEEE</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"397\" to=\"408\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b39\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Retrieval-augmented generation for code summarization via hybrid gnn</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shangqing</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yu</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiaofei</ns0:forename><ns0:surname>Xie</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jingkai</ns0:forename><ns0:surname>Siow</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yang</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2006.05405</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2020\">2020. 2020</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b40\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Delving into Parameter-Efficient Fine-Tuning in Code Change Learning: An Empirical Study</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuo</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jacky</ns0:forename><ns0:surname>Keung</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhen</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fang</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qilin</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yihan</ns0:forename><ns0:surname>Liao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2402.06247</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2024\">2024. 2024</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b41\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Reacc: A retrievalaugmented code completion framework</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuai</ns0:forename><ns0:surname>Lu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Nan</ns0:forename><ns0:surname>Duan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hojae</ns0:forename><ns0:surname>Han</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daya</ns0:forename><ns0:surname>Guo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Seung-Won</ns0:forename><ns0:surname>Hwang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Alexey</ns0:forename><ns0:surname>Svyatkovskiy</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2203.07722</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b42\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Codexglue: A machine learning benchmark dataset for code understanding and generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuai</ns0:forename><ns0:surname>Lu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daya</ns0:forename><ns0:surname>Guo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuo</ns0:forename><ns0:surname>Ren</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Junjie</ns0:forename><ns0:surname>Huang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Alexey</ns0:forename><ns0:surname>Svyatkovskiy</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ambrosio</ns0:forename><ns0:surname>Blanco</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Colin</ns0:forename><ns0:surname>Clement</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dawn</ns0:forename><ns0:surname>Drain</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daxin</ns0:forename><ns0:surname>Jiang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Duyu</ns0:forename><ns0:surname>Tang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2102.04664</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b43\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sourab</ns0:forename><ns0:surname>Mangrulkar</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sylvain</ns0:forename><ns0:surname>Gugger</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lysandre</ns0:forename><ns0:surname>Debut</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Younes</ns0:forename><ns0:surname>Belkada</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sayak</ns0:forename><ns0:surname>Paul</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Benjamin</ns0:forename><ns0:surname>Bossan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:ptr target=\"https://github.com/huggingface/peft\" />\n",
      "\t\t<ns0:title level=\"m\">PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b44\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Recent advances in natural language processing via large pre-trained language models: A survey</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bonan</ns0:forename><ns0:surname>Min</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hayley</ns0:forename><ns0:surname>Ross</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Elior</ns0:forename><ns0:surname>Sulem</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Amir</ns0:forename><ns0:surname>Pouran</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ben</ns0:forename><ns0:surname>Veyseh</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Thien</ns0:forename><ns0:surname>Huu Nguyen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Oscar</ns0:forename><ns0:surname>Sainz</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Eneko</ns0:forename><ns0:surname>Agirre</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ilana</ns0:forename><ns0:surname>Heintz</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dan</ns0:forename><ns0:surname>Roth</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Comput. Surveys</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b45\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Metaicl: Learning to learn in context</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sewon</ns0:forename><ns0:surname>Min</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mike</ns0:forename><ns0:surname>Lewis</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Luke</ns0:forename><ns0:surname>Zettlemoyer</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hannaneh</ns0:forename><ns0:surname>Hajishirzi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2110.15943</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b46\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Retrieval-based prompt selection for code-related few-shot learning</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Noor</ns0:forename><ns0:surname>Nashid</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mifta</ns0:forename><ns0:surname>Sintaha</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ali</ns0:forename><ns0:surname>Mesbah</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 45th International Conference on Software Engineering (ICSE'23)</ns0:title>\n",
      "\t\t<ns0:meeting>the 45th International Conference on Software Engineering (ICSE'23)</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b47\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Codegen2: Lessons for training llms on programming and natural languages</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Erik</ns0:forename><ns0:surname>Nijkamp</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hiroaki</ns0:forename><ns0:surname>Hayashi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Caiming</ns0:forename><ns0:surname>Xiong</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Silvio</ns0:forename><ns0:surname>Savarese</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yingbo</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2305.02309</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b48\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Erik</ns0:forename><ns0:surname>Nijkamp</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bo</ns0:forename><ns0:surname>Pang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hiroaki</ns0:forename><ns0:surname>Hayashi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lifu</ns0:forename><ns0:surname>Tu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Huan</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yingbo</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Silvio</ns0:forename><ns0:surname>Savarese</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Caiming</ns0:forename><ns0:surname>Xiong</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2203.13474[cs.LG</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023-12\">2023. December 2024</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">1</ns0:biblScope>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note>Publication date</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b49\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Code generation from natural language with less prior knowledge and more monolingual data</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sajad</ns0:forename><ns0:surname>Norouzi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Keyi</ns0:forename><ns0:surname>Tang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yanshuai</ns0:forename><ns0:surname>Cao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</ns0:title>\n",
      "\t\t<ns0:meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">2</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"776\" to=\"785\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note>Short Papers</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b50\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">GPT-4 technical report</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Openai</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno>arXiv</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"2303\" to=\"08774\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b51\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Training language models to follow instructions with human feedback</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Long</ns0:forename><ns0:surname>Ouyang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jeffrey</ns0:forename><ns0:surname>Wu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xu</ns0:forename><ns0:surname>Jiang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Diogo</ns0:forename><ns0:surname>Almeida</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Carroll</ns0:forename><ns0:surname>Wainwright</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pamela</ns0:forename><ns0:surname>Mishkin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chong</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sandhini</ns0:forename><ns0:surname>Agarwal</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Katarina</ns0:forename><ns0:surname>Slama</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Alex</ns0:forename><ns0:surname>Ray</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Advances in Neural Information Processing Systems</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">35</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"27730\" to=\"27744\" />\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b52\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Retrieval augmented code generation and summarization</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Md Rizwan Parvez</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Uddin</ns0:forename><ns0:surname>Wasi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Saikat</ns0:forename><ns0:surname>Ahmad</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Baishakhi</ns0:forename><ns0:surname>Chakraborty</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Kai-Wei</ns0:forename><ns0:surname>Ray</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Chang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2108.11601</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b53\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Can OpenAI's codex fix bugs? an evaluation on QuixBugs</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Aron</ns0:forename><ns0:surname>Julian</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hlib</ns0:forename><ns0:surname>Prenner</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Romain</ns0:forename><ns0:surname>Babii</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Robbes</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the Third International Workshop on Automated Program Repair</ns0:title>\n",
      "\t\t<ns0:meeting>the Third International Workshop on Automated Program Repair</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"69\" to=\"75\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b54\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Language models are unsupervised multitask learners</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Alec</ns0:forename><ns0:surname>Radford</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jeffrey</ns0:forename><ns0:surname>Wu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Rewon</ns0:forename><ns0:surname>Child</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">David</ns0:forename><ns0:surname>Luan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dario</ns0:forename><ns0:surname>Amodei</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ilya</ns0:forename><ns0:surname>Sutskever</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">OpenAI blog</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">1</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"issue\">8</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\">9</ns0:biblScope>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2019\">2019. 2019</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b55\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Codebleu: a method for automatic evaluation of code synthesis</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daya</ns0:forename><ns0:surname>Shuo Ren</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuai</ns0:forename><ns0:surname>Guo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Long</ns0:forename><ns0:surname>Lu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shujie</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Duyu</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Neel</ns0:forename><ns0:surname>Tang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ming</ns0:forename><ns0:surname>Sundaresan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ambrosio</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuai</ns0:forename><ns0:surname>Blanco</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Ma</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2009.10297</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2020\">2020. 2020</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b56\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Code llama: Open foundation models for code</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jonas</ns0:forename><ns0:surname>Baptiste Roziere</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fabian</ns0:forename><ns0:surname>Gehring</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sten</ns0:forename><ns0:surname>Gloeckle</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Itai</ns0:forename><ns0:surname>Sootla</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Gat</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ellen</ns0:forename><ns0:surname>Xiaoqing</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yossi</ns0:forename><ns0:surname>Tan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jingyu</ns0:forename><ns0:surname>Adi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tal</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jérémy</ns0:forename><ns0:surname>Remez</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Rapin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2308.12950</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b57\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Utilization of pre-trained language models for adapter-based knowledge transfer in software engineering</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Iman</ns0:forename><ns0:surname>Saberi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fatemeh</ns0:forename><ns0:surname>Fard</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Fuxiang</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Empirical Software Engineering</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">29</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"issue\">4</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\">94</ns0:biblScope>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2024\">2024. 2024</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b58\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Iman</ns0:forename><ns0:surname>Saberi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">H</ns0:forename><ns0:surname>Fatemeh</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Fard</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2303.06233</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b59\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Prompting large language models with answer heuristics for knowledge-based visual question answering</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhenwei</ns0:forename><ns0:surname>Shao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhou</ns0:forename><ns0:surname>Yu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Meng</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jun</ns0:forename><ns0:surname>Yu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</ns0:title>\n",
      "\t\t<ns0:meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"14974\" to=\"14983\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b60\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Adafactor: Adaptive learning rates with sublinear memory cost</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Noam</ns0:forename><ns0:surname>Shazeer</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mitchell</ns0:forename><ns0:surname>Stern</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">International Conference on Machine Learning</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>PMLR</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2018\">2018</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"4596\" to=\"4604\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b61\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Repository-level prompt generation for large language models of code</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Disha</ns0:forename><ns0:surname>Shrivastava</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hugo</ns0:forename><ns0:surname>Larochelle</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daniel</ns0:forename><ns0:surname>Tarlow</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">International Conference on Machine Learning</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>PMLR</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"31693\" to=\"31715\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b62\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Energy and policy considerations for deep learning in NLP</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Emma</ns0:forename><ns0:surname>Strubell</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ananya</ns0:forename><ns0:surname>Ganesh</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Andrew</ns0:forename><ns0:surname>Mccallum</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:1906.02243</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2019\">2019. 2019</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b63\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Treegen: A tree-based transformer architecture for code generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zeyu</ns0:forename><ns0:surname>Sun</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qihao</ns0:forename><ns0:surname>Zhu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yingfei</ns0:forename><ns0:surname>Xiong</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yican</ns0:forename><ns0:surname>Sun</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lili</ns0:forename><ns0:surname>Mou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lu</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the AAAI Conference on Artificial Intelligence</ns0:title>\n",
      "\t\t<ns0:meeting>the AAAI Conference on Artificial Intelligence</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2020\">2020</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">34</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"8984\" to=\"8991\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b64\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Llama: Open and efficient foundation language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hugo</ns0:forename><ns0:surname>Touvron</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Thibaut</ns0:forename><ns0:surname>Lavril</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Gautier</ns0:forename><ns0:surname>Izacard</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xavier</ns0:forename><ns0:surname>Martinet</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Marie-Anne</ns0:forename><ns0:surname>Lachaux</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Timothée</ns0:forename><ns0:surname>Lacroix</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Baptiste</ns0:forename><ns0:surname>Rozière</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Naman</ns0:forename><ns0:surname>Goyal</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Eric</ns0:forename><ns0:surname>Hambro</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Faisal</ns0:forename><ns0:surname>Azhar</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2302.13971</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b65\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Efficient methods for natural language processing: A survey</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Marcos</ns0:forename><ns0:surname>Treviso</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ji-Ung</ns0:forename><ns0:surname>Lee</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tianchu</ns0:forename><ns0:surname>Ji</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Betty</ns0:forename><ns0:surname>Van Aken</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qingqing</ns0:forename><ns0:surname>Cao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Michael</ns0:forename><ns0:surname>Manuel R Ciosici</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Kenneth</ns0:forename><ns0:surname>Hassid</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sara</ns0:forename><ns0:surname>Heafield</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Colin</ns0:forename><ns0:surname>Hooker</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Raffel</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Transactions of the Association for Computational Linguistics</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">11</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"826\" to=\"860\" />\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b66\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Priyan</ns0:forename><ns0:surname>Vaithilingam</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tianyi</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Elena</ns0:forename><ns0:forename type=\"middle\">L</ns0:forename><ns0:surname>Glassman</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Chi conference on human factors in computing systems extended abstracts</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"1\" to=\"7\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b67\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Attention is all you need</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ashish</ns0:forename><ns0:surname>Vaswani</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Noam</ns0:forename><ns0:surname>Shazeer</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Niki</ns0:forename><ns0:surname>Parmar</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jakob</ns0:forename><ns0:surname>Uszkoreit</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Llion</ns0:forename><ns0:surname>Jones</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Aidan</ns0:forename><ns0:forename type=\"middle\">N</ns0:forename><ns0:surname>Gomez</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Łukasz</ns0:forename><ns0:surname>Kaiser</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Illia</ns0:forename><ns0:surname>Polosukhin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Advances in neural information processing systems</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">30</ns0:biblScope>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2017\">2017. 2017</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b68\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">No more finetuning? an experimental evaluation of prompt tuning in code intelligence</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chaozheng</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yuanhang</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Cuiyun</ns0:forename><ns0:surname>Gao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yun</ns0:forename><ns0:surname>Peng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hongyu</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Michael R Lyu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</ns0:title>\n",
      "\t\t<ns0:meeting>the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"382\" to=\"394\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b69\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Deze</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Boxing</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shanshan</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Wei</ns0:forename><ns0:surname>Luo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shaoliang</ns0:forename><ns0:surname>Peng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Wei</ns0:forename><ns0:surname>Dong</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiangke</ns0:forename><ns0:surname>Liao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2303.15822</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b70\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Weishi</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yue</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shafiq</ns0:forename><ns0:surname>Joty</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Steven Ch</ns0:forename><ns0:surname>Hoi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</ns0:title>\n",
      "\t\t<ns0:meeting>the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"146\" to=\"158\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b71\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Codet5+: Open code large language models for code understanding and generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yue</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hung</ns0:forename><ns0:surname>Le</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Akhilesh</ns0:forename><ns0:surname>Deepak Gotmare</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">D</ns0:forename><ns0:forename type=\"middle\">Q</ns0:forename><ns0:surname>Nghi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Junnan</ns0:forename><ns0:surname>Bui</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Steven Ch</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Hoi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2305.07922</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b72\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Codet5: Identifier-aware unified pre-trained encoderdecoder models for code understanding and generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yue</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Weishi</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shafiq</ns0:forename><ns0:surname>Joty</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Steven Ch</ns0:forename><ns0:surname>Hoi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2109.00859</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b73\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Execution-Based Evaluation for Open-Domain Code Generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhiruo</ns0:forename><ns0:surname>Wang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuyan</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daniel</ns0:forename><ns0:surname>Fried</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Graham</ns0:forename><ns0:surname>Neubig</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2212.10481</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b74\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Albert</ns0:forename><ns0:surname>Webson</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ellie</ns0:forename><ns0:surname>Pavlick</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2109.01247</ns0:idno>\n",
      "\t\t<ns0:title level=\"m\">Do prompt-based models really understand the meaning of their prompts?</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b75\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Finetuned language models are zero-shot learners</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jason</ns0:forename><ns0:surname>Wei</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Maarten</ns0:forename><ns0:surname>Bosma</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Y</ns0:forename><ns0:surname>Vincent</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Kelvin</ns0:forename><ns0:surname>Zhao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Adams</ns0:forename><ns0:forename type=\"middle\">Wei</ns0:forename><ns0:surname>Guu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Brian</ns0:forename><ns0:surname>Yu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Nan</ns0:forename><ns0:surname>Lester</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Andrew</ns0:forename><ns0:forename type=\"middle\">M</ns0:forename><ns0:surname>Du</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Quoc V</ns0:forename><ns0:surname>Dai</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Le</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2109.01652</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021. 2021</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b76\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Emergent abilities of large language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Jason</ns0:forename><ns0:surname>Wei</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yi</ns0:forename><ns0:surname>Tay</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Rishi</ns0:forename><ns0:surname>Bommasani</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Colin</ns0:forename><ns0:surname>Raffel</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Barret</ns0:forename><ns0:surname>Zoph</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sebastian</ns0:forename><ns0:surname>Borgeaud</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dani</ns0:forename><ns0:surname>Yogatama</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Maarten</ns0:forename><ns0:surname>Bosma</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Denny</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Donald</ns0:forename><ns0:surname>Metzler</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2206.07682</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022. 2022</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b77\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Better modeling the programming world with code concept graphs-augmented multi-modal learning</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Martin</ns0:forename><ns0:surname>Weyssow</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Houari</ns0:forename><ns0:surname>Sahraoui</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bang</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results</ns0:title>\n",
      "\t\t<ns0:meeting>the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"21\" to=\"25\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b78\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Martin</ns0:forename><ns0:surname>Weyssow</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Kisub</ns0:forename><ns0:surname>Kim</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">David</ns0:forename><ns0:surname>Lo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Houari</ns0:forename><ns0:surname>Sahraoui</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2305.04106</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b79\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Huggingface's transformers: State-of-the-art natural language processing</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Thomas</ns0:forename><ns0:surname>Wolf</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lysandre</ns0:forename><ns0:surname>Debut</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Victor</ns0:forename><ns0:surname>Sanh</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Julien</ns0:forename><ns0:surname>Chaumond</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Clement</ns0:forename><ns0:surname>Delangue</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Anthony</ns0:forename><ns0:surname>Moi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pierric</ns0:forename><ns0:surname>Cistac</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tim</ns0:forename><ns0:surname>Rault</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Rémi</ns0:forename><ns0:surname>Louf</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Morgan</ns0:forename><ns0:surname>Funtowicz</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:1910.03771</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2019\">2019. 2019</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b80\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Automated program repair in the era of large pre-trained language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chunqiu</ns0:forename><ns0:surname>Steven Xia</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yuxiang</ns0:forename><ns0:surname>Wei</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lingming</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 45th International Conference on Software Engineering (ICSE 2023)</ns0:title>\n",
      "\t\t<ns0:meeting>the 45th International Conference on Software Engineering (ICSE 2023)</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>Association for Computing Machinery</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b81\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Less training, more repairing please: revisiting automated program repair via zero-shot learning</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chunqiu</ns0:forename><ns0:surname>Steven</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xia</ns0:forename></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Lingming</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</ns0:title>\n",
      "\t\t<ns0:meeting>the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"959\" to=\"971\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b82\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">A Systematic Evaluation of Large Language Models of Code (MAPS 2022)</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">F</ns0:forename><ns0:surname>Frank</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Uri</ns0:forename><ns0:surname>Xu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Graham</ns0:forename><ns0:surname>Alon</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Vincent</ns0:forename><ns0:surname>Neubig</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:surname>Josua Hellendoorn</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"DOI\">10.1145/3520312.3534862</ns0:idno>\n",
      "\t\t<ns0:ptr target=\"https://doi.org/10.1145/3520312.3534862\" />\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2022\">2022</ns0:date>\n",
      "\t\t\t<ns0:publisher>Association for Computing Machinery</ns0:publisher>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"1\" to=\"10\" />\n",
      "\t\t\t<ns0:pubPlace>New York, NY, USA</ns0:pubPlace>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b83\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Exploring Continual Learning for Code Generation Models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Prateek</ns0:forename><ns0:surname>Yadav</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qing</ns0:forename><ns0:surname>Sun</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hantian</ns0:forename><ns0:surname>Ding</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiaopeng</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dejiao</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ming</ns0:forename><ns0:surname>Tan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiaofei</ns0:forename><ns0:surname>Ma</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Parminder</ns0:forename><ns0:surname>Bhatia</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Ramesh</ns0:forename><ns0:surname>Nallapati</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Murali</ns0:forename><ns0:surname>Krishna Ramanathan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2307.02435</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b84\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">ExploitGen: Templateaugmented exploit code generation based on CodeBERT</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Guang</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yu</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiang</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xiangyu</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Tingting</ns0:forename><ns0:surname>Han</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Taolue</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"j\">Journal of Systems and Software</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">197</ns0:biblScope>\n",
      "\t\t\t<ns0:biblScope unit=\"page\">111577</ns0:biblScope>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b85\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Language in a bottle: Language model guided concept bottlenecks for interpretable image classification</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yue</ns0:forename><ns0:surname>Yang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Artemis</ns0:forename><ns0:surname>Panagopoulou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shenghao</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daniel</ns0:forename><ns0:surname>Jin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Chris</ns0:forename><ns0:surname>Callison-Burch</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mark</ns0:forename><ns0:surname>Yatskar</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</ns0:title>\n",
      "\t\t<ns0:meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"19187\" to=\"19197\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b86\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pengcheng</ns0:forename><ns0:surname>Yin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bowen</ns0:forename><ns0:surname>Deng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Edgar</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bogdan</ns0:forename><ns0:surname>Vasilescu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Graham</ns0:forename><ns0:surname>Neubig</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"DOI\">10.1145/3196398.3196408</ns0:idno>\n",
      "\t\t<ns0:ptr target=\"https://doi.org/10.1145/3196398.3196408\" />\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">Proceedings of the 15th International Conference on Mining Software Repositories</ns0:title>\n",
      "\t\t<ns0:meeting>the 15th International Conference on Mining Software Repositories<ns0:address><ns0:addrLine>Gothenburg, Sweden; New York, NY, USA</ns0:addrLine></ns0:address></ns0:meeting>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>Association for Computing Machinery</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2018\">2018</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"476\" to=\"486\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b87\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Learning to mine aligned code and natural language pairs from stack overflow</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Pengcheng</ns0:forename><ns0:surname>Yin</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bowen</ns0:forename><ns0:surname>Deng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Edgar</ns0:forename><ns0:surname>Chen</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bogdan</ns0:forename><ns0:surname>Vasilescu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Graham</ns0:forename><ns0:surname>Neubig</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">IEEE/ACM 15th international conference on mining software repositories (MSR)</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>IEEE</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2018\">2018. 2018</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"476\" to=\"486\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b88\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Evaluating instruction-tuned large language models on code comprehension and generation</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhiqiang</ns0:forename><ns0:surname>Yuan</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Junwei</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Qiancheng</ns0:forename><ns0:surname>Zi</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Mingwei</ns0:forename><ns0:surname>Liu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin</ns0:forename><ns0:surname>Peng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yiling</ns0:forename><ns0:surname>Lou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2308.01240</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023-12\">2023. 2023. December 2024</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"volume\">1</ns0:biblScope>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "\t<ns0:note>Publication date</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b89\">\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\" type=\"main\">Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Bowen</ns0:forename><ns0:surname>Zhang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xianghua</ns0:forename><ns0:surname>Fu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Daijun</ns0:forename><ns0:surname>Ding</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Hu</ns0:forename><ns0:surname>Huang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Yangyang</ns0:forename><ns0:surname>Li</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Liwen</ns0:forename><ns0:surname>Jing</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:idno type=\"arXiv\">arXiv:2304.03087</ns0:idno>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023. 2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "\t<ns0:note type=\"report_type\">arXiv preprint</ns0:note>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b90\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Calibrate before use: Improving few-shot performance of language models</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zihao</ns0:forename><ns0:surname>Zhao</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Eric</ns0:forename><ns0:surname>Wallace</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shi</ns0:forename><ns0:surname>Feng</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Dan</ns0:forename><ns0:surname>Klein</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Sameer</ns0:forename><ns0:surname>Singh</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">International Conference on Machine Learning</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>PMLR</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"12697\" to=\"12706\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b91\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Docprompting: Generating code by retrieving the docs</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Shuyan</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Uri</ns0:forename><ns0:surname>Alon</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Frank</ns0:forename><ns0:forename type=\"middle\">F</ns0:forename><ns0:surname>Xu</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Zhengbao</ns0:forename><ns0:surname>Jiang</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Graham</ns0:forename><ns0:surname>Neubig</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">The Eleventh International Conference on Learning Representations</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2023\">2023</ns0:date>\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "<ns0:biblStruct xml:id=\"b92\">\n",
      "\t<ns0:analytic>\n",
      "\t\t<ns0:title level=\"a\" type=\"main\">Assessing generalizability of codebert</ns0:title>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">Donggyun</ns0:forename><ns0:surname>Han</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t\t<ns0:author>\n",
      "\t\t\t<ns0:persName><ns0:forename type=\"first\">David</ns0:forename><ns0:surname>Lo</ns0:surname></ns0:persName>\n",
      "\t\t</ns0:author>\n",
      "\t</ns0:analytic>\n",
      "\t<ns0:monogr>\n",
      "\t\t<ns0:title level=\"m\">2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)</ns0:title>\n",
      "\t\t<ns0:imprint>\n",
      "\t\t\t<ns0:publisher>IEEE</ns0:publisher>\n",
      "\t\t\t<ns0:date type=\"published\" when=\"2021\">2021</ns0:date>\n",
      "\t\t\t<ns0:biblScope unit=\"page\" from=\"425\" to=\"436\" />\n",
      "\t\t</ns0:imprint>\n",
      "\t</ns0:monogr>\n",
      "</ns0:biblStruct>\n",
      "\n",
      "\t\t\t\t</ns0:listBibl>\n",
      "\t\t\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_grobid_output(tei_xml):\n",
    "    # GROBID TEI has namespaces, so we define them\n",
    "    ns = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "    root = ET.fromstring(tei_xml)\n",
    "\n",
    "    # Extract metadata (teiHeader)\n",
    "    metadata_elem = root.find(\"tei:teiHeader\", ns)\n",
    "    metadata = ET.tostring(metadata_elem, encoding=\"unicode\") if metadata_elem is not None else None\n",
    "\n",
    "    # Extract main body text\n",
    "    body_elem = root.find(\".//tei:body\", ns)\n",
    "    body = ET.tostring(body_elem, encoding=\"unicode\") if body_elem is not None else None\n",
    "\n",
    "    # Extract references (listBibl in back section)\n",
    "    refs_elem = root.find(\".//tei:listBibl\", ns)\n",
    "    references = ET.tostring(refs_elem, encoding=\"unicode\") if refs_elem is not None else None\n",
    "\n",
    "    return metadata, body, references\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tei_output = \"\"\"<TEI xmlns=\"http://www.tei-c.org/ns/1.0\">\n",
    "<teiHeader> ... metadata ... </teiHeader>\n",
    "<text>\n",
    "  <body> ... main content ... </body>\n",
    "  <back>\n",
    "    <listBibl> ... references ... </listBibl>\n",
    "  </back>\n",
    "</text>\n",
    "</TEI>\"\"\"\n",
    "\n",
    "metadata, body, references = parse_grobid_output(xml_metadata)\n",
    "\n",
    "print(\"Metadata:\\n\", metadata, \"\\n\")\n",
    "print(\"Body:\\n\", body, \"\\n\")\n",
    "print(\"References:\\n\", references, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ns0:teiHeader xmlns:ns0=\"http://www.tei-c.org/ns/1.0\" xml:lang=\"en\">\n",
      "\t\t<ns0:fileDesc>\n",
      "\t\t\t<ns0:titleStmt>\n",
      "\t\t\t\t<ns0:title level=\"a\" type=\"main\">Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</ns0:title>\n",
      "\t\t\t</ns0:titleStmt>\n",
      "\t\t\t<ns0:publicationStmt>\n",
      "\t\t\t\t<ns0:publisher />\n",
      "\t\t\t\t<ns0:availability status=\"unknown\">\n",
      "\t\t\t\t\t<ns0:licence />\n",
      "\t\t\t\t</ns0:availability>\n",
      "\t\t\t\t<ns0:date type=\"published\" when=\"2024-12-27\">27 Dec 2024</ns0:date>\n",
      "\t\t\t</ns0:publicationStmt>\n",
      "\t\t\t<ns0:sourceDesc>\n",
      "\t\t\t\t<ns0:biblStruct>\n",
      "\t\t\t\t\t<ns0:analytic>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">Martin</ns0:forename><ns0:surname>Weyssow</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t\t<ns0:email>martin.weyssow@umontreal.ca</ns0:email>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:surname>Diro</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">Xin</ns0:forename><ns0:surname>Zhou</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">Kisub</ns0:forename><ns0:surname>Kim</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t\t<ns0:email>kisubkim@gmail.com</ns0:email>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:persName><ns0:forename type=\"first\">David</ns0:forename><ns0:surname>Lo</ns0:surname></ns0:persName>\n",
      "\t\t\t\t\t\t\t<ns0:email>davidlo@smu.edu.sg</ns0:email>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff0\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff1\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"SG\">Singapore</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff2\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"SG\">Singapore</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff3\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"department\" key=\"dep1\">Singapore HOUARI SAHRAOUI</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"department\" key=\"dep2\">DIRO</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\" key=\"instit1\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\" key=\"instit2\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff4\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"department\">DIRO</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Xin Zhou</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff5\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Singapore</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff6\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Singapore</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff7\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">Singapore Management University</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:settlement>Singapore</ns0:settlement>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:author>\n",
      "\t\t\t\t\t\t\t<ns0:affiliation key=\"aff8\">\n",
      "\t\t\t\t\t\t\t\t<ns0:orgName type=\"institution\">University of Montreal</ns0:orgName>\n",
      "\t\t\t\t\t\t\t\t<ns0:address>\n",
      "\t\t\t\t\t\t\t\t\t<ns0:country key=\"CA\">Canada</ns0:country>\n",
      "\t\t\t\t\t\t\t\t</ns0:address>\n",
      "\t\t\t\t\t\t\t</ns0:affiliation>\n",
      "\t\t\t\t\t\t</ns0:author>\n",
      "\t\t\t\t\t\t<ns0:title level=\"a\" type=\"main\">Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</ns0:title>\n",
      "\t\t\t\t\t</ns0:analytic>\n",
      "\t\t\t\t\t<ns0:monogr>\n",
      "\t\t\t\t\t\t<ns0:imprint>\n",
      "\t\t\t\t\t\t\t<ns0:date type=\"published\" when=\"2024-12-27\">27 Dec 2024</ns0:date>\n",
      "\t\t\t\t\t\t</ns0:imprint>\n",
      "\t\t\t\t\t</ns0:monogr>\n",
      "\t\t\t\t\t<ns0:idno type=\"MD5\">ED08C5420997939C148460D2D743B3D0</ns0:idno>\n",
      "\t\t\t\t\t<ns0:idno type=\"DOI\">10.1145/nnnnnnn.nnnnnnn</ns0:idno>\n",
      "\t\t\t\t\t<ns0:idno type=\"arXiv\">arXiv:2308.10462v3[cs.SE]</ns0:idno>\n",
      "\t\t\t\t</ns0:biblStruct>\n",
      "\t\t\t</ns0:sourceDesc>\n",
      "\t\t</ns0:fileDesc>\n",
      "\t\t<ns0:encodingDesc>\n",
      "\t\t\t<ns0:appInfo>\n",
      "\t\t\t\t<ns0:application version=\"0.8.1\" ident=\"GROBID\" when=\"2025-08-17T09:27+0000\">\n",
      "\t\t\t\t\t<ns0:desc>GROBID - A machine learning software for extracting information from scholarly documents</ns0:desc>\n",
      "\t\t\t\t\t<ns0:ref target=\"https://github.com/kermitt2/grobid\" />\n",
      "\t\t\t\t</ns0:application>\n",
      "\t\t\t</ns0:appInfo>\n",
      "\t\t</ns0:encodingDesc>\n",
      "\t\t<ns0:profileDesc>\n",
      "\t\t\t<ns0:textClass>\n",
      "\t\t\t\t<ns0:keywords>\n",
      "\t\t\t\t\t<ns0:term>CCS Concepts:</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>Software and its engineering → Software creation and management; Software development techniques code generation</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>large language models</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>parameter-efficient fine-tuning</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>quantization</ns0:term>\n",
      "\t\t\t\t\t<ns0:term>empirical study 29.47</ns0:term>\n",
      "\t\t\t\t</ns0:keywords>\n",
      "\t\t\t</ns0:textClass>\n",
      "\t\t\t<ns0:abstract>\n",
      "<ns0:div><ns0:p>Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.</ns0:p></ns0:div>\n",
      "\t\t\t</ns0:abstract>\n",
      "\t\t</ns0:profileDesc>\n",
      "\t</ns0:teiHeader>\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "def generate_bibtex_from_grobid(metadata_xml, body_xml, references_xml, gemini_api_key):\n",
    "    client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a research paper parser. \n",
    "    Given the TEI XML segments extracted from GROBID, convert them into a structured JSON object with fields:\n",
    "    - citation_key\n",
    "    - authors (list of \"Last, First\")\n",
    "    - title\n",
    "    - date, year, month, day\n",
    "    - doi\n",
    "    - eprint (arXiv or other)\n",
    "    - abstract\n",
    "    - high level overview of the paper\n",
    "    - specialities\n",
    "    - distinguing features\n",
    "    - publisher\n",
    "    - keywords\n",
    "    - bibtex string\n",
    "\n",
    "    Metadata XML:\n",
    "    {metadata_xml}\n",
    "    Body XML:\n",
    "    {body_xml}\n",
    "    References XML:\n",
    "    {references_xml}\n",
    "    If a field is missing in the XML, leave it as an empty string.\n",
    "    The final output should be JSON containing all fields above, and include a complete BibTeX entry.\n",
    "    \"\"\"\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    return resp.text  # Could be parsed as JSON if well-formed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"citation_key\": \"weyssow2024exploring\",\n",
      "  \"authors\": [\n",
      "    \"Weyssow, Martin\",\n",
      "    \"Diro,\",\n",
      "    \"Zhou, Xin\",\n",
      "    \"Kim, Kisub\",\n",
      "    \"Lo, David\"\n",
      "  ],\n",
      "  \"title\": \"Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models\",\n",
      "  \"date\": \"2024-12-27\",\n",
      "  \"year\": \"2024\",\n",
      "  \"month\": \"12\",\n",
      "  \"day\": \"27\",\n",
      "  \"doi\": \"10.1145/nnnnnnn.nnnnnnn\",\n",
      "  \"eprint\": \"arXiv:2308.10462v3[cs.SE]\",\n",
      "  \"abstract\": \"Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.\",\n",
      "  \"high_level_overview\": \"This paper presents a comprehensive empirical study of parameter-efficient fine-tuning (PEFT) techniques for large language models (LLMs) in the context of automated code generation.  The authors compare PEFT techniques (LoRA, IA3, Prompt tuning, Prefix tuning, QLoRA) against in-context learning (ICL) and retrieval-augmented generation (RAG), evaluating their performance on three Python code generation datasets (Conala, CodeAlpacaPy, and APPS).  The study highlights the superiority of PEFT, especially LoRA and QLoRA, in terms of effectiveness and resource efficiency, enabling the fine-tuning of even very large LLMs with limited computational resources.\",\n",
      "  \"specialities\": [\n",
      "    \"Software engineering\",\n",
      "    \"Code generation\",\n",
      "    \"Large language models\",\n",
      "    \"Parameter-efficient fine-tuning\"\n",
      "  ],\n",
      "  \"distinguishing_features\": [\n",
      "    \"Comprehensive study of PEFT techniques for LLMs in code generation\",\n",
      "    \"Comparison of PEFT with ICL and RAG\",\n",
      "    \"Evaluation on three datasets with varying complexity\",\n",
      "    \"Demonstration of practicality under resource-constrained scenarios\",\n",
      "    \"Exploration of quantization with PEFT\"\n",
      "  ],\n",
      "  \"publisher\": \"\",\n",
      "  \"keywords\": [\n",
      "    \"CCS Concepts:\",\n",
      "    \"Software and its engineering → Software creation and management; Software development techniques code generation\",\n",
      "    \"large language models\",\n",
      "    \"parameter-efficient fine-tuning\",\n",
      "    \"quantization\",\n",
      "    \"empirical study 29.47\"\n",
      "  ],\n",
      "  \"bibtex_string\": \"@article{weyssow2024exploring,\\n  title={Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models},\\n  author={Weyssow, Martin and Diro, and Zhou, Xin and Kim, Kisub and Lo, David},\\n  journal={},\\n  year={2024},\\n  month={dec},\\n  doi={10.1145/nnnnnnn.nnnnnnn},\\n  eprint={arXiv:2308.10462v3[cs.SE]}\\n}\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tei_xml = processFulltextDocument(\"/Users/arjunbhndary/SEI_accelerate/rrbZq8-2308.10462v3.pdf\")  # Your GROBID function\n",
    "metadata_xml, body_xml, refs_xml = parse_grobid_output(tei_xml)\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "structured_output = generate_bibtex_from_grobid(metadata_xml, body_xml, refs_xml, gemini_api_key)\n",
    "\n",
    "print(structured_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'citation_key': 'weyssow2024exploring', 'authors': ['Weyssow, Martin', 'Diro,', 'Zhou, Xin', 'Kim, Kisub', 'Lo, David'], 'title': 'Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models', 'date': '2024-12-27', 'year': '2024', 'month': '12', 'day': '27', 'doi': '10.1145/nnnnnnn.nnnnnnn', 'eprint': 'arXiv:2308.10462v3[cs.SE]', 'abstract': 'Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.', 'high_level_overview': 'This paper presents a comprehensive empirical study of parameter-efficient fine-tuning (PEFT) techniques for large language models (LLMs) in the context of automated code generation.  The authors compare PEFT techniques (LoRA, IA3, Prompt tuning, Prefix tuning, QLoRA) against in-context learning (ICL) and retrieval-augmented generation (RAG), evaluating their performance on three Python code generation datasets (Conala, CodeAlpacaPy, and APPS).  The study highlights the superiority of PEFT, especially LoRA and QLoRA, in terms of effectiveness and resource efficiency, enabling the fine-tuning of even very large LLMs with limited computational resources.', 'specialities': ['Software engineering', 'Code generation', 'Large language models', 'Parameter-efficient fine-tuning'], 'distinguishing_features': ['Comprehensive study of PEFT techniques for LLMs in code generation', 'Comparison of PEFT with ICL and RAG', 'Evaluation on three datasets with varying complexity', 'Demonstration of practicality under resource-constrained scenarios', 'Exploration of quantization with PEFT'], 'publisher': '', 'keywords': ['CCS Concepts:', 'Software and its engineering → Software creation and management; Software development techniques code generation', 'large language models', 'parameter-efficient fine-tuning', 'quantization', 'empirical study 29.47'], 'bibtex_string': '@article{weyssow2024exploring,\\n  title={Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models},\\n  author={Weyssow, Martin and Diro, and Zhou, Xin and Kim, Kisub and Lo, David},\\n  journal={},\\n  year={2024},\\n  month={dec},\\n  doi={10.1145/nnnnnnn.nnnnnnn},\\n  eprint={arXiv:2308.10462v3[cs.SE]}\\n}'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "# remove backticks and optional \"json\" label\n",
    "cleaned_str = re.sub(r\"^```json|```$\", \"\", structured_output.strip(), flags=re.MULTILINE).strip()\n",
    "\n",
    "# convert to dict\n",
    "structured_output_dict = json.loads(cleaned_str)\n",
    "\n",
    "print(structured_output_dict)\n",
    "print(type(structured_output_dict))  # should be <class 'dict'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"meta_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(structured_output_dict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import pandas as pd\n",
    "import re,json,os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "gemini_api = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=gemini_api)\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    A class to handle dataset operations like metadata extraction, \n",
    "    metadata generation, and summarization using a generative AI model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_column_metadata(csv_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Reads a CSV file and extracts metadata about its columns.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): The file path to the CSV.\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted string describing each column's name and inferred type.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        def classify_dtype(series):\n",
    "            if pd.api.types.is_datetime64_any_dtype(series):\n",
    "                return \"datetime\"\n",
    "            elif pd.api.types.is_numeric_dtype(series):\n",
    "                return \"numeric\"\n",
    "            elif pd.api.types.is_categorical_dtype(series):\n",
    "                return \"categorical\"\n",
    "            else:\n",
    "                return \"string\"\n",
    "\n",
    "        lines = []\n",
    "        for col in df.columns:\n",
    "            dtype = classify_dtype(df[col])\n",
    "            lines.append(f\"Column: {col} | Type: {dtype}\")\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def generate_metadata(self, user_input: str, data_path: str) -> dict:\n",
    "        \"\"\"\n",
    "        Generates structured metadata for a dataset in JSON format.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): A natural language description of the dataset.\n",
    "            data_path (str): The file path to the CSV data.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the structured metadata.\n",
    "        \"\"\"\n",
    "        column_data = self.extract_column_metadata(data_path)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a dataset metadata generator. \n",
    "        Given the dataset description and column details, generate complete metadata in a valid JSON format only.\n",
    "\n",
    "        Requirements:\n",
    "        - The JSON must include: title, description, columns (with name + type), source, license, update_frequency, and limitations.\n",
    "        - Do not include any extra explanation or text outside of the JSON.\n",
    "\n",
    "        dataset_information: \"{user_input}\"\n",
    "        feature_information:\n",
    "        {column_data}\n",
    "        \"\"\"\n",
    "        \n",
    "        resp = client.models.generate_content(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            contents=prompt\n",
    "        )\n",
    "        resp_text = resp.text\n",
    "        cleaned_str = re.sub(r\"^```json|```$\", \"\", resp_text.strip(), flags=re.MULTILINE).strip()\n",
    "        \n",
    "        return json.loads(cleaned_str)\n",
    "\n",
    "    def generate_summary(self, user_input: str, data_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a natural language summary for a dataset.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): A natural language description of the dataset.\n",
    "            data_path (str): The file path to the CSV data.\n",
    "\n",
    "        Returns:\n",
    "            str: A text summary of the dataset.\n",
    "        \"\"\"\n",
    "        column_data = self.extract_column_metadata(data_path)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a data science assistant. \n",
    "        Given the following description and column information for a dataset, please write a concise, easy-to-understand summary. \n",
    "        Describe what the dataset is about, what kind of information it contains, and what it might be used for.\n",
    "        \n",
    "        Make sure the output is in JSON format and it should include the concise summary, information like use cases, what is the field it can be used in, where and how the dataset was synthesised. \n",
    "        Dataset Description: \"{user_input}\"\n",
    "        \n",
    "        Column Information:\n",
    "        {column_data}\n",
    "        \"\"\"\n",
    "        \n",
    "        resp = client.models.generate_content(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            contents=prompt\n",
    "        )\n",
    "        resp_text = resp.text\n",
    "        cleaned_str = re.sub(r\"^```json|```$\", \"\", resp_text.strip(), flags=re.MULTILINE).strip()\n",
    "        \n",
    "        return json.loads(cleaned_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'College Student Well-being Survey',\n",
       " 'description': 'This dataset captures survey responses from 843 college students aged 18–21 regarding their experiences with stress, health, relationships, academics, and emotional well-being. The responses were collected via Google Forms using a five-point Likert scale (\"Not at all\" to \"Extremely\") and anonymized to protect privacy. It enables nuanced analysis of emotional and physical stress indicators and their correlations with academic performance and lifestyle factors.',\n",
       " 'columns': [{'name': 'anxiety_level', 'type': 'numeric'},\n",
       "  {'name': 'self_esteem', 'type': 'numeric'},\n",
       "  {'name': 'mental_health_history', 'type': 'numeric'},\n",
       "  {'name': 'depression', 'type': 'numeric'},\n",
       "  {'name': 'headache', 'type': 'numeric'},\n",
       "  {'name': 'blood_pressure', 'type': 'numeric'},\n",
       "  {'name': 'sleep_quality', 'type': 'numeric'},\n",
       "  {'name': 'breathing_problem', 'type': 'numeric'},\n",
       "  {'name': 'noise_level', 'type': 'numeric'},\n",
       "  {'name': 'living_conditions', 'type': 'numeric'},\n",
       "  {'name': 'safety', 'type': 'numeric'},\n",
       "  {'name': 'basic_needs', 'type': 'numeric'},\n",
       "  {'name': 'academic_performance', 'type': 'numeric'},\n",
       "  {'name': 'study_load', 'type': 'numeric'},\n",
       "  {'name': 'teacher_student_relationship', 'type': 'numeric'},\n",
       "  {'name': 'future_career_concerns', 'type': 'numeric'},\n",
       "  {'name': 'social_support', 'type': 'numeric'},\n",
       "  {'name': 'peer_pressure', 'type': 'numeric'},\n",
       "  {'name': 'extracurricular_activities', 'type': 'numeric'},\n",
       "  {'name': 'bullying', 'type': 'numeric'},\n",
       "  {'name': 'stress_level', 'type': 'numeric'}],\n",
       " 'source': 'Google Forms Survey',\n",
       " 'license': 'Unspecified',\n",
       " 'update_frequency': 'One-time',\n",
       " 'limitations': 'Data collected from a specific population (college students aged 18-21) and may not be generalizable to other populations.  Responses rely on self-reporting and may be subject to bias.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset()\n",
    "path = \"/Users/arjunbhndary/SEI_accelerate/StressLevelDataset.csv\"\n",
    "user_info  = \"\"\"This dataset captures survey responses from 843 college students aged 18–21 regarding their experiences with stress, health, relationships, academics, and emotional well-being. The responses were collected via Google Forms using a five-point Likert scale (\"Not at all\" to \"Extremely\") and anonymized to protect privacy.\n",
    "\n",
    "It enables nuanced analysis of emotional and physical stress indicators and their correlations with academic performance and lifestyle factors.\"\"\"\n",
    "data.generate_metadata(user_info,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_summary': 'This dataset contains anonymized survey responses from 843 college students (aged 18-21) about their stress, health, relationships, academics, and emotional well-being.  Responses are scored on a 5-point Likert scale.  It provides quantitative data on various aspects of student life.',\n",
       " 'information': {'what_it_contains': 'Numeric data on anxiety levels, self-esteem, mental health history, depression, physical health (headaches, blood pressure, sleep, breathing), living conditions, safety, basic needs, academic performance, study load, teacher-student relationships, career concerns, social support, peer pressure, extracurricular activities, bullying, and overall stress levels.',\n",
       "  'use_cases': ['Analyzing correlations between stress indicators and academic performance.',\n",
       "   'Identifying risk factors for mental health issues in college students.',\n",
       "   'Assessing the impact of lifestyle factors (social support, living conditions) on well-being.',\n",
       "   'Developing targeted interventions to improve student mental and physical health.',\n",
       "   'Studying the relationship between academic pressure and mental health.'],\n",
       "  'field_of_use': 'Educational psychology, public health, student affairs, social sciences',\n",
       "  'synthesis_method': 'Survey data collected via Google Forms using a 5-point Likert scale. Data was anonymized to protect privacy.',\n",
       "  'data_source': 'Survey responses from 843 college students aged 18-21.'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.generate_summary(user_info,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'title': \"Dijkstra's Algorithm\", 'description': \"This image depicts the pseudocode for Dijkstra's algorithm, a classic single-source shortest path algorithm. It calculates the shortest paths from a specified source vertex 's' to all other vertices in a given graph 'G', where 'w' represents the edge weights.\", 'field_of_study': ['Computer Science', 'Algorithms', 'Graph Theory', 'Data Structures'], 'application': ['Shortest Path Finding', 'Network Routing (e.g., OSPF, IS-IS)', 'Geographic Information Systems (GIS)', 'Transportation Planning', 'Telecommunication Networks', 'Game AI (pathfinding)'], 'algorithm_details': {'name': \"Dijkstra's Algorithm\", 'type': 'Single-Source Shortest Path (SSSP)', 'input': {'G': 'Graph (V, E)', 'w': 'Edge weight function (non-negative)', 's': 'Source vertex'}, 'output': \"Shortest path distances from 's' to all other vertices, and predecessor pointers to reconstruct paths.\", 'constraints': 'Requires non-negative edge weights. Does not work correctly with negative edge weights (Bellman-Ford or SPFA would be used instead).', 'key_steps': ['Initialization: Sets all distances to infinity (except source to 0) and predecessors to null.', \"Maintain two sets of vertices: 'S' (vertices whose shortest path from 's' is finalized) and 'Q' (a priority queue of vertices not yet in 'S').\", \"Iteratively extract the vertex 'u' with the minimum estimated distance from 'Q'.\", \"Add 'u' to 'S'.\", \"Relax all edges (u, v) originating from 'u': update 'v's distance if a shorter path through 'u' is found, and update 'v's position in the priority queue if its distance changes.\"], 'data_structures_used': ['Adjacency List/Matrix (for graph G.Adj[u])', 'Priority Queue (for Q, to efficiently find EXTRACT-MIN)', 'Set (for S)'], 'time_complexity': 'Typically O(E log V) or O(E + V log V) with a Fibonacci heap, or O(E log V) with a binary heap, where V is the number of vertices and E is the number of edges.'}, 'keywords': ['Dijkstra', 'Shortest Path', 'Graph Algorithm', 'Weighted Graph', 'Non-negative Weights', 'Greedy Algorithm', 'Priority Queue']}}\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "gemini_api = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "from google import genai\n",
    "import os\n",
    "\n",
    "from google import genai\n",
    "import pandas as pd\n",
    "import re,json,os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "gemini_api = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "class formula:\n",
    "  def __init__(self):\n",
    "    self.client = genai.Client(api_key=gemini_api)\n",
    "  \n",
    "  def extract_metadata(self, user_info: str, image_path: str):\n",
    "    my_file = self.client.files.upload(file=image_path)\n",
    "\n",
    "    prompt = f\"\"\"Given the image and the description of the information in the image, \n",
    "    your job is to make a metadata of the information and return it in JSON format. \n",
    "    Make sure to include information like the field of study, application, and what it is about.\n",
    "    \n",
    "    description: {user_info}\n",
    "    \"\"\"\n",
    "\n",
    "    response = self.client.models.generate_content(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      contents=[my_file, prompt],\n",
    "    )\n",
    "    text = response.candidates[0].content.parts[0].text\n",
    "    cleaned_str = re.sub(r\"^```json|```$\", \"\", text.strip(), flags=re.MULTILINE).strip()\n",
    "        \n",
    "    return json.loads(cleaned_str)\n",
    "  \n",
    "  def extract_summary(self, user_info: str, image_path: str):\n",
    "    my_file = self.client.files.upload(file=image_path)\n",
    "\n",
    "    prompt = f\"\"\"Given the image and the description of the information in the image, \n",
    "    your job is to make a SUMMARY of the information and return it in JSON format. \n",
    "    Inlcude everything important such that just by reading the JSON i can understand information about the paper.\n",
    "    \n",
    "    description: {user_info}\n",
    "    \"\"\"\n",
    "\n",
    "    response = self.client.models.generate_content(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      contents=[my_file, prompt],\n",
    "    )\n",
    "    text = response.candidates[0].content.parts[0].text\n",
    "    cleaned_str = re.sub(r\"^```json|```$\", \"\", text.strip(), flags=re.MULTILINE).strip()\n",
    "        \n",
    "    return json.loads(cleaned_str)\n",
    "\n",
    "\n",
    "f = formula()\n",
    "user_info = \"This is a shortest path algorithm which gives us the shortest path from a source node to any given node when the edges have non negative edge weights.\"\n",
    "image_path = \"/Users/arjunbhndary/SEI_accelerate/Screenshot 2025-08-22 at 5.01.58 PM.png\"\n",
    "\n",
    "metadata = f.extract_metadata(user_info, image_path)\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "formula.extract_metadata() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m user_info = \u001b[33m\"\u001b[39m\u001b[33mThis is a shorted path algorithm which gives us the shortest path from a source node to any given node when the edges have non negetive edge weights\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33m/Users/arjunbhndary/SEI_accelerate/Screenshot 2025-08-22 at 5.01.58 PM.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: formula.extract_metadata() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
