{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyCb5WPiZVfPjG6Zbn8ikKuaWTn9_yke6dk'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"abstract\": \"Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.\", \"keywords\": [\"CCS Concepts:\", \"Software and its engineering \\u2192 Software creation and management; Software development techniques code generation\", \"large language models\", \"parameter-efficient fine-tuning\", \"quantization\", \"empirical study 29.47\"], \"problem_statement\": \"Fine-tuning large language models (LLMs) for code generation is computationally expensive, especially for models with billions of parameters.  In-context learning (ICL) and retrieval-augmented generation (RAG) offer alternatives, but they lack the ability to learn task-specific parameters, limiting performance.  There's a need for parameter-efficient fine-tuning (PEFT) techniques applicable to LLMs in resource-constrained environments.\", \"methodology_summary\": \"The study empirically evaluates six PEFT techniques (LoRA, IA3, Prompt tuning, Prefix tuning, QLoRA-8bit, QLoRA-4bit) for code generation using eight LLMs and three SLMs across three datasets (Conala, CodeAlpacaPy, APPS).  It compares PEFT against ICL and RAG, focusing on resource-constrained scenarios (single 24GB GPU).  Metrics include Exact Match (EM), CodeBLEU, average passed tests, and Pass@k.\", \"results_summary\": \"ICL significantly improves model performance compared to zero-shot.  PEFT consistently outperforms fully fine-tuned SLMs and ICL.  LoRA achieves the highest effectiveness. QLoRA significantly reduces memory usage while maintaining or improving effectiveness, enabling fine-tuning of 34B parameter LLMs with limited resources. LoRA outperforms ICL and RAG. LoRA and QLoRA improve CodeLlama-7B-Instruct's performance on APPS.\", \"contributions\": [\"Comprehensive empirical study of six PEFT techniques for Python code generation on a range of SLMs and LLMs.\", \"Comparison of PEFT techniques against ICL and RAG for LLMs in code generation.\", \"Demonstration of PEFT's practicality for efficient LLM fine-tuning and reduced computational burden.\"], \"field_of_study\": \"Software Engineering\", \"subfields\": [\"Automated Code Generation\", \"Program Repair\", \"Code Intelligence\"], \"tasks\": [\"Python Code Generation\"], \"datasets_used\": [\"Conala\", \"CodeAlpacaPy\", \"APPS\"], \"code_link\": \"https://github.com/martin-wey/peft-llm-code\", \"application_domains\": [\"Software Development\"], \"bibtex\": \"@article{weyssow2024exploring,\\n  title={Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models},\\n  author={Weyssow, Martin and Diro, and Zhou, Xin and Kim, Kisub and Lo, David},\\n  journal={},\\n  year={2024},\\n  month={Dec},\\n  day={27},\\n  doi={10.1145/nnnnnnn.nnnnnnn},\\n  arxiv={2308.10462v3}\\n}\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your file\n",
    "file_path = \"/Users/arjunbhndary/SEI_Rolyalties/summary copy.json\"\n",
    "\n",
    "# Load JSON from file\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert JSON object (dict/list) into a string\n",
    "summary_str = json.dumps(data)\n",
    "\n",
    "print(summary_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "def sovle(gemini_api_key,summary_str):\n",
    "    client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "    prompt = f\"\"\"SYSTEM:\n",
    "You are a pragmatic license-term generator for research artifacts (paper text, code, model weights, datasets). Your task: read the provided research paper summary and generate exactly **3 distinct** license templates tailored to that paper. Each license must be focused on the **royalties** field and must be consistent in structure and types across all three objects. Output **only** a JSON array (no explanation, no markdown, no extra text).\n",
    "\n",
    "USER:\n",
    "INPUT_SUMMARY:\n",
    "{summary_str}\n",
    "\n",
    "OUTPUT REQUIREMENTS (strict — must follow exactly):\n",
    "\n",
    "1. Top-level output: a JSON array with exactly 3 objects.\n",
    "\n",
    "2. Each object must include the following fields (names and types must match exactly):\n",
    "\n",
    "- \"license_id\": string — unique short id (e.g., \"REVSHARE-001\").\n",
    "- \"license_name\": string — human-friendly name.\n",
    "- \"license_type\": string — one of: \"open-attribution\", \"permissive-patent\", \"commercial-revenue-share\", \"saas-api\", \"dual-license\", \"contributor-revenue-split\" (choose the best-fit label).\n",
    "- \"royalties\": object containing:\n",
    "    - \"model\": string — one of: \"none\", \"percentage\", \"per_call_or_subscription\", \"flat_fee\".\n",
    "    - \"value\": number — if model is \"percentage\" use integer (0-100). If \"flat_fee\" use USD amount. If \"per_call_or_subscription\" use numeric per-call USD (e.g., 0.005) or 0 if variable.\n",
    "    - \"split\": object or null — if present, keys are stakeholders and values are integers that sum to 100. If not applicable, set null.\n",
    "    - \"payment_interval_days\": integer or null — days between payouts (e.g., 30, 90) or null.\n",
    "    - \"min_fee_usd\": number or null — minimum payable fee in USD, or null.\n",
    "    - \"notes\": string — short note about how royalties apply (1-2 sentences).\n",
    "\n",
    "- \"restrictions\": array of strings — key restrictions/obligations (e.g., attribution, no redistribution of weights, reporting, telemetry).\n",
    "\n",
    "3. Additional rules & constraints:\n",
    "- All 3 licenses must be distinct (different \"license_type\" or different royalty models).\n",
    "- Tailor each license to the input summary: reference relevant artifact types present in the summary (code repo, model weights, datasets, etc.) in \"restrictions\" or \"notes\" where appropriate.\n",
    "- The \"royalties\" field must be prominent and realistic (e.g., common revenue-share values 5–30% or per-call pricing like 0.001–0.05 USD).\n",
    "- If \"split\" is provided, the integer parts must sum to 100.\n",
    "- Use \"PERCENT\" as currency when model is \"percentage\"; use \"USD_PER_CALL\" for per-call values.\n",
    "- Do not output any additional fields beyond the specified schema.\n",
    "- Output must be valid JSON (no trailing commas).\n",
    "\n",
    "4. Ordering:\n",
    "- Order the array by \"popularity_for_deployment\" inferred from the summary: most broadly adoptable license first, most restrictive/commercial last.\n",
    "\n",
    "\n",
    "END.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    return resp.text  # Could be parsed as JSON if well-formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyCb5WPiZVfPjG6Zbn8ikKuaWTn9_yke6dk\n"
     ]
    }
   ],
   "source": [
    "print(gemini_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = sovle(gemini_api,summary_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n  {\\n    \"license_id\": \"OPEN-001\",\\n    \"license_name\": \"Open Attribution License\",\\n    \"license_type\": \"open-attribution\",\\n    \"royalties\": {\\n      \"model\": \"none\",\\n      \"value\": 0,\\n      \"split\": null,\\n      \"payment_interval_days\": null,\\n      \"min_fee_usd\": null,\\n      \"notes\": \"No royalties; attribution required in any work using the code, models, or datasets.\"\\n    },\\n    \"restrictions\": [\\n      \"Attribution required\",\\n      \"No redistribution of model weights without explicit permission\",\\n      \"Datasets may be used for non-commercial purposes only\"\\n    ]\\n  },\\n  {\\n    \"license_id\": \"REVSHARE-001\",\\n    \"license_name\": \"Commercial Revenue Share\",\\n    \"license_type\": \"commercial-revenue-share\",\\n    \"royalties\": {\\n      \"model\": \"percentage\",\\n      \"value\": 10,\\n      \"split\": {\\n        \"Authors\": 70,\\n        \"Contributors\": 30\\n      },\\n      \"payment_interval_days\": 90,\\n      \"min_fee_usd\": 1000,\\n      \"notes\": \"10 PERCENT revenue share on commercial applications using the code, model weights or datasets; split as indicated. Minimum annual payout is USD 1000.\"\\n    },\\n    \"restrictions\": [\\n      \"Attribution required\",\\n      \"Commercial use requires royalty payment\",\\n      \"Revenue reporting required annually\",\\n      \"Code modifications must be open-sourced under the same license\"\\n    ]\\n  },\\n  {\\n    \"license_id\": \"COMMERCIAL-001\",\\n    \"license_name\": \"Commercial License\",\\n    \"license_type\": \"commercial-revenue-share\",\\n    \"royalties\": {\\n      \"model\": \"per_call_or_subscription\",\\n      \"value\": 0.01,\\n      \"split\": null,\\n      \"payment_interval_days\": 30,\\n      \"min_fee_usd\": null,\\n      \"notes\": \"Commercial use requires a per-call/subscription fee of 0.01 USD_PER_CALL for API access and usage of the model weights.  Contact authors for details.\"\\n    },\\n    \"restrictions\": [\\n      \"Attribution required\",\\n      \"Commercial use requires license agreement\",\\n      \"No redistribution of the model or code without explicit permission\",\\n      \"Datasets usage limited to permitted domains\"\\n    ]\\n  }\\n]\\n```\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'license_id': 'OPEN-001', 'license_name': 'Open Attribution License', 'license_type': 'open-attribution', 'royalties': {'model': 'none', 'value': 0, 'split': None, 'payment_interval_days': None, 'min_fee_usd': None, 'notes': 'No royalties; attribution required in any work using the code, models, or datasets.'}, 'restrictions': ['Attribution required', 'No redistribution of model weights without explicit permission', 'Datasets may be used for non-commercial purposes only']}, {'license_id': 'REVSHARE-001', 'license_name': 'Commercial Revenue Share', 'license_type': 'commercial-revenue-share', 'royalties': {'model': 'percentage', 'value': 10, 'split': {'Authors': 70, 'Contributors': 30}, 'payment_interval_days': 90, 'min_fee_usd': 1000, 'notes': '10 PERCENT revenue share on commercial applications using the code, model weights or datasets; split as indicated. Minimum annual payout is USD 1000.'}, 'restrictions': ['Attribution required', 'Commercial use requires royalty payment', 'Revenue reporting required annually', 'Code modifications must be open-sourced under the same license']}, {'license_id': 'COMMERCIAL-001', 'license_name': 'Commercial License', 'license_type': 'commercial-revenue-share', 'royalties': {'model': 'per_call_or_subscription', 'value': 0.01, 'split': None, 'payment_interval_days': 30, 'min_fee_usd': None, 'notes': 'Commercial use requires a per-call/subscription fee of 0.01 USD_PER_CALL for API access and usage of the model weights.  Contact authors for details.'}, 'restrictions': ['Attribution required', 'Commercial use requires license agreement', 'No redistribution of the model or code without explicit permission', 'Datasets usage limited to permitted domains']}]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "# remove backticks and optional \"json\" label\n",
    "cleaned_str = re.sub(r\"^```json|```$\", \"\", ans.strip(), flags=re.MULTILINE).strip()\n",
    "\n",
    "# convert to dict\n",
    "structured_output_dict = json.loads(cleaned_str)\n",
    "\n",
    "print(structured_output_dict)\n",
    "print(type(structured_output_dict))  # should be <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"royalties.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(structured_output_dict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_api = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, file_path):\n",
    "    self.file_path = file_path\n",
    "    self.summary = self.make_summary()\n",
    "    \n",
    "  def make_summary(self):\n",
    "    with open(self.file_path, \"r\") as f:\n",
    "      data = json.load(f)\n",
    "    summary_str = json.dumps(data)\n",
    "    return summary_str\n",
    "\n",
    "  def solve(self):\n",
    "      client = genai.Client(api_key=gemini_api)\n",
    "      prompt = f\"\"\"SYSTEM:\n",
    "You are a pragmatic license-term generator for research artifacts (paper text, code, model weights, datasets). Your task: read the provided research paper summary and generate exactly **3 distinct** license templates tailored to that paper. Each license must be focused on the **royalties** field and must be consistent in structure and types across all three objects. Output **only** a JSON array (no explanation, no markdown, no extra text).\n",
    "\n",
    "USER:\n",
    "INPUT_SUMMARY:\n",
    "{self.summary}\n",
    "\n",
    "OUTPUT REQUIREMENTS (strict — must follow exactly):\n",
    "\n",
    "1. Top-level output: a JSON array with exactly 3 objects.\n",
    "\n",
    "2. Each object must include the following fields (names and types must match exactly):\n",
    "\n",
    "- \"license_id\": string — unique short id (e.g., \"REVSHARE-001\").\n",
    "- \"license_name\": string — human-friendly name.\n",
    "- \"license_type\": string — one of: \"open-attribution\", \"permissive-patent\", \"commercial-revenue-share\", \"saas-api\", \"dual-license\", \"contributor-revenue-split\" (choose the best-fit label).\n",
    "- \"royalties\": object containing:\n",
    "    - \"model\": string — one of: \"none\", \"percentage\", \"per_call_or_subscription\", \"flat_fee\".\n",
    "    - \"value\": number — if model is \"percentage\" use integer (0-100). If \"flat_fee\" use USD amount. If \"per_call_or_subscription\" use numeric per-call USD (e.g., 0.005) or 0 if variable.\n",
    "    - \"split\": object or null — if present, keys are stakeholders and values are integers that sum to 100. If not applicable, set null.\n",
    "    - \"payment_interval_days\": integer or null — days between payouts (e.g., 30, 90) or null.\n",
    "    - \"min_fee_usd\": number or null — minimum payable fee in USD, or null.\n",
    "    - \"notes\": string — short note about how royalties apply (1-2 sentences).\n",
    "\n",
    "- \"restrictions\": array of strings — key restrictions/obligations (e.g., attribution, no redistribution of weights, reporting, telemetry).\n",
    "\n",
    "3. Additional rules & constraints:\n",
    "- All 3 licenses must be distinct (different \"license_type\" or different royalty models).\n",
    "- Tailor each license to the input summary: reference relevant artifact types present in the summary (code repo, model weights, datasets, etc.) in \"restrictions\" or \"notes\" where appropriate.\n",
    "- The \"royalties\" field must be prominent and realistic (e.g., common revenue-share values 5–30% or per-call pricing like 0.001–0.05 USD).\n",
    "- If \"split\" is provided, the integer parts must sum to 100.\n",
    "- Use \"PERCENT\" as currency when model is \"percentage\"; use \"USD_PER_CALL\" for per-call values.\n",
    "- Do not output any additional fields beyond the specified schema.\n",
    "- Output must be valid JSON (no trailing commas).\n",
    "\n",
    "4. Ordering:\n",
    "- Order the array by \"popularity_for_deployment\" inferred from the summary: most broadly adoptable license first, most restrictive/commercial last.\n",
    "\n",
    "\n",
    "END.\n",
    "\"\"\"\n",
    "\n",
    "      resp = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt\n",
    "      )\n",
    "      \n",
    "      cleaned_str = re.sub(r\"^\\s*```json\\s*|\\s*```\\s*$\", \"\", resp.text.strip(), flags=re.MULTILINE).strip()\n",
    "      structured_output_dict = json.loads(cleaned_str)  \n",
    "      \n",
    "      return structured_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'license_id': 'OPEN-001', 'license_name': 'Open Attribution License', 'license_type': 'open-attribution', 'royalties': {'model': 'none', 'value': 0, 'split': None, 'payment_interval_days': None, 'min_fee_usd': None, 'notes': 'Free to use for research and commercial purposes; attribution required in publications and code using the model weights or datasets.'}, 'restrictions': ['Attribution required', 'No redistribution of model weights without permission', 'Datasets must be cited']}, {'license_id': 'REVSHARE-001', 'license_name': 'Commercial Revenue Share', 'license_type': 'commercial-revenue-share', 'royalties': {'model': 'percentage', 'value': 10, 'split': {'Researchers': 70, 'Commercial User': 30}, 'payment_interval_days': 90, 'min_fee_usd': 1000, 'notes': '10% revenue share on commercial applications using the model weights, code, or datasets. Revenue split detailed in the split object.'}, 'restrictions': ['Revenue sharing required for commercial applications', 'Regular reporting of revenue required', 'Code modifications must be open-sourced']}, {'license_id': 'PERCALL-001', 'license_name': 'Per-Call Commercial License', 'license_type': 'commercial-revenue-share', 'royalties': {'model': 'per_call_or_subscription', 'value': 0.01, 'split': {'Researchers': 80, 'Commercial User': 20}, 'payment_interval_days': 30, 'min_fee_usd': 100, 'notes': '0.01 USD per API call for commercial use of the model weights or code; payment split detailed in the split object. Minimum fee per month applies.'}, 'restrictions': ['Per-call fee required for commercial applications', 'Usage data reporting required', 'Model weights can not be reverse-engineered']}]\n"
     ]
    }
   ],
   "source": [
    "# 1. Create an instance of the Agent, passing the file path to the constructor.\n",
    "agent_instance = Agent(file_path=\"/Users/arjunbhndary/SEI_Rolyalties/summary copy.json\")\n",
    "\n",
    "# 2. Now, call the solve() method on the instance.\n",
    "licenses = agent_instance.solve()\n",
    "\n",
    "# You can now work with the result\n",
    "print(licenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
